[
  {
    "sha": "97d8e8e52cb8ab3d7675880a92626d9a4332f7a6",
    "message": "netfs: Fix ref leak on inserted extra subreq in write retry\n\nThe write-retry algorithm will insert extra subrequests into the list if it\ncan't get sufficient capacity to split the range that needs to be retried\ninto the sequence of subrequests it currently has (for instance, if the\ncifs credit pool has fewer credits available than it did when the range was\noriginally divided).\n\nHowever, the allocator furnishes each new subreq with 2 refs and then\nanother is added for resubmission, causing one to be leaked.\n\nFix this by replacing the ref-getting line with a neutral trace line.\n\nFixes: 288ace2f57c9 (\"netfs: New writeback implementation\")\nSigned-off-by: David Howells <dhowells@redhat.com>\nLink: https://lore.kernel.org/20250701163852.2171681-6-dhowells@redhat.com\nTested-by: Steve French <sfrench@samba.org>\nReviewed-by: Paulo Alcantara <pc@manguebit.org>\ncc: netfs@lists.linux.dev\ncc: linux-fsdevel@vger.kernel.org\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
    "author": "David Howells",
    "date": "2025-07-01T22:37:13+02:00",
    "files_changed": [
      "fs/netfs/write_retry.c"
    ],
    "diff": "diff --git a/fs/netfs/write_retry.c b/fs/netfs/write_retry.c\nindex 9d1d8a8bab72..7158657061e9 100644\n--- a/fs/netfs/write_retry.c\n+++ b/fs/netfs/write_retry.c\n@@ -153,7 +153,7 @@ static void netfs_retry_write_stream(struct netfs_io_request *wreq,\n \t\t\ttrace_netfs_sreq_ref(wreq->debug_id, subreq->debug_index,\n \t\t\t\t\t     refcount_read(&subreq->ref),\n \t\t\t\t\t     netfs_sreq_trace_new);\n-\t\t\tnetfs_get_subrequest(subreq, netfs_sreq_trace_get_resubmit);\n+\t\t\ttrace_netfs_sreq(subreq, netfs_sreq_trace_split);\n \n \t\t\tlist_add(&subreq->rreq_link, &to->rreq_link);\n \t\t\tto = list_next_entry(to, rreq_link);",
    "stats": {
      "insertions": 1,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "8c44dac8add7503c345c0f6c7962e4863b88ba42",
    "message": "eventpoll: Fix priority inversion problem\n\nThe ready event list of an epoll object is protected by read-write\nsemaphore:\n\n  - The consumer (waiter) acquires the write lock and takes items.\n  - the producer (waker) takes the read lock and adds items.\n\nThe point of this design is enabling epoll to scale well with large number\nof producers, as multiple producers can hold the read lock at the same\ntime.\n\nUnfortunately, this implementation may cause scheduling priority inversion\nproblem. Suppose the consumer has higher scheduling priority than the\nproducer. The consumer needs to acquire the write lock, but may be blocked\nby the producer holding the read lock. Since read-write semaphore does not\nsupport priority-boosting for the readers (even with CONFIG_PREEMPT_RT=y),\nwe have a case of priority inversion: a higher priority consumer is blocked\nby a lower priority producer. This problem was reported in [1].\n\nFurthermore, this could also cause stall problem, as described in [2].\n\nTo fix this problem, make the event list half-lockless:\n\n  - The consumer acquires a mutex (ep->mtx) and takes items.\n  - The producer locklessly adds items to the list.\n\nPerformance is not the main goal of this patch, but as the producer now can\nadd items without waiting for consumer to release the lock, performance\nimprovement is observed using the stress test from\nhttps://github.com/rouming/test-tools/blob/master/stress-epoll.c. This is\nthe same test that justified using read-write semaphore in the past.\n\nTesting using 12 x86_64 CPUs:\n\n          Before     After        Diff\nthreads  events/ms  events/ms\n      8       6932      19753    +185%\n     16       7820      27923    +257%\n     32       7648      35164    +360%\n     64       9677      37780    +290%\n    128      11166      38174    +242%\n\nTesting using 1 riscv64 CPU (averaged over 10 runs, as the numbers are\nnoisy):\n\n          Before     After        Diff\nthreads  events/ms  events/ms\n      1         73        129     +77%\n      2        151        216     +43%\n      4        216        364     +69%\n      8        234        382     +63%\n     16        251        392     +56%\n\nReported-by: Frederic Weisbecker <frederic@kernel.org>\nCloses: https://lore.kernel.org/linux-rt-users/20210825132754.GA895675@lothringen/ [1]\nReported-by: Valentin Schneider <vschneid@redhat.com>\nCloses: https://lore.kernel.org/linux-rt-users/xhsmhttqvnall.mognet@vschneid.remote.csb/ [2]\nSigned-off-by: Nam Cao <namcao@linutronix.de>\nLink: https://lore.kernel.org/20250527090836.1290532-1-namcao@linutronix.de\nTested-by: K Prateek Nayak <kprateek.nayak@amd.com>\nAcked-by: Frederic Weisbecker <frederic@kernel.org>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
    "author": "Nam Cao",
    "date": "2025-07-01T22:31:51+02:00",
    "files_changed": [
      "fs/eventpoll.c"
    ],
    "diff": "diff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex d4dbffdedd08..a97a771a459c 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -137,13 +137,7 @@ struct epitem {\n \t};\n \n \t/* List header used to link this structure to the eventpoll ready list */\n-\tstruct list_head rdllink;\n-\n-\t/*\n-\t * Works together \"struct eventpoll\"->ovflist in keeping the\n-\t * single linked chain of items.\n-\t */\n-\tstruct epitem *next;\n+\tstruct llist_node rdllink;\n \n \t/* The file descriptor information this item refers to */\n \tstruct epoll_filefd ffd;\n@@ -191,22 +185,15 @@ struct eventpoll {\n \t/* Wait queue used by file->poll() */\n \twait_queue_head_t poll_wait;\n \n-\t/* List of ready file descriptors */\n-\tstruct list_head rdllist;\n-\n-\t/* Lock which protects rdllist and ovflist */\n-\trwlock_t lock;\n+\t/*\n+\t * List of ready file descriptors. Adding to this list is lockless. Items can be removed\n+\t * only with eventpoll::mtx\n+\t */\n+\tstruct llist_head rdllist;\n \n \t/* RB tree root used to store monitored fd structs */\n \tstruct rb_root_cached rbr;\n \n-\t/*\n-\t * This is a single linked list that chains all the \"struct epitem\" that\n-\t * happened while transferring ready events to userspace w/out\n-\t * holding ->lock.\n-\t */\n-\tstruct epitem *ovflist;\n-\n \t/* wakeup_source used when ep_send_events or __ep_eventpoll_poll is running */\n \tstruct wakeup_source *ws;\n \n@@ -361,10 +348,14 @@ static inline int ep_cmp_ffd(struct epoll_filefd *p1,\n \t        (p1->file < p2->file ? -1 : p1->fd - p2->fd));\n }\n \n-/* Tells us if the item is currently linked */\n-static inline int ep_is_linked(struct epitem *epi)\n+/*\n+ * Add the item to its container eventpoll's rdllist; do nothing if the item is already on rdllist.\n+ */\n+static void epitem_ready(struct epitem *epi)\n {\n-\treturn !list_empty(&epi->rdllink);\n+\tif (&epi->rdllink == cmpxchg(&epi->rdllink.next, &epi->rdllink, NULL))\n+\t\tllist_add(&epi->rdllink, &epi->ep->rdllist);\n+\n }\n \n static inline struct eppoll_entry *ep_pwq_from_wait(wait_queue_entry_t *p)\n@@ -383,13 +374,26 @@ static inline struct epitem *ep_item_from_wait(wait_queue_entry_t *p)\n  *\n  * @ep: Pointer to the eventpoll context.\n  *\n- * Return: a value different than %zero if ready events are available,\n- *          or %zero otherwise.\n+ * Return: true if ready events might be available, false otherwise.\n  */\n-static inline int ep_events_available(struct eventpoll *ep)\n+static inline bool ep_events_available(struct eventpoll *ep)\n {\n-\treturn !list_empty_careful(&ep->rdllist) ||\n-\t\tREAD_ONCE(ep->ovflist) != EP_UNACTIVE_PTR;\n+\tbool available;\n+\tint locked;\n+\n+\tlocked = mutex_trylock(&ep->mtx);\n+\tif (!locked) {\n+\t\t/*\n+\t\t * The lock held and someone might have removed all items while inspecting it. The\n+\t\t * llist_empty() check in this case is futile. Assume that something is enqueued and\n+\t\t * let ep_try_send_events() figure it out.\n+\t\t */\n+\t\treturn true;\n+\t}\n+\n+\tavailable = !llist_empty(&ep->rdllist);\n+\tmutex_unlock(&ep->mtx);\n+\treturn available;\n }\n \n #ifdef CONFIG_NET_RX_BUSY_POLL\n@@ -724,77 +728,6 @@ static inline void ep_pm_stay_awake_rcu(struct epitem *epi)\n \trcu_read_unlock();\n }\n \n-\n-/*\n- * ep->mutex needs to be held because we could be hit by\n- * eventpoll_release_file() and epoll_ctl().\n- */\n-static void ep_start_scan(struct eventpoll *ep, struct list_head *txlist)\n-{\n-\t/*\n-\t * Steal the ready list, and re-init the original one to the\n-\t * empty list. Also, set ep->ovflist to NULL so that events\n-\t * happening while looping w/out locks, are not lost. We cannot\n-\t * have the poll callback to queue directly on ep->rdllist,\n-\t * because we want the \"sproc\" callback to be able to do it\n-\t * in a lockless way.\n-\t */\n-\tlockdep_assert_irqs_enabled();\n-\twrite_lock_irq(&ep->lock);\n-\tlist_splice_init(&ep->rdllist, txlist);\n-\tWRITE_ONCE(ep->ovflist, NULL);\n-\twrite_unlock_irq(&ep->lock);\n-}\n-\n-static void ep_done_scan(struct eventpoll *ep,\n-\t\t\t struct list_head *txlist)\n-{\n-\tstruct epitem *epi, *nepi;\n-\n-\twrite_lock_irq(&ep->lock);\n-\t/*\n-\t * During the time we spent inside the \"sproc\" callback, some\n-\t * other events might have been queued by the poll callback.\n-\t * We re-insert them inside the main ready-list here.\n-\t */\n-\tfor (nepi = READ_ONCE(ep->ovflist); (epi = nepi) != NULL;\n-\t     nepi = epi->next, epi->next = EP_UNACTIVE_PTR) {\n-\t\t/*\n-\t\t * We need to check if the item is already in the list.\n-\t\t * During the \"sproc\" callback execution time, items are\n-\t\t * queued into ->ovflist but the \"txlist\" might already\n-\t\t * contain them, and the list_splice() below takes care of them.\n-\t\t */\n-\t\tif (!ep_is_linked(epi)) {\n-\t\t\t/*\n-\t\t\t * ->ovflist is LIFO, so we have to reverse it in order\n-\t\t\t * to keep in FIFO.\n-\t\t\t */\n-\t\t\tlist_add(&epi->rdllink, &ep->rdllist);\n-\t\t\tep_pm_stay_awake(epi);\n-\t\t}\n-\t}\n-\t/*\n-\t * We need to set back ep->ovflist to EP_UNACTIVE_PTR, so that after\n-\t * releasing the lock, events will be queued in the normal way inside\n-\t * ep->rdllist.\n-\t */\n-\tWRITE_ONCE(ep->ovflist, EP_UNACTIVE_PTR);\n-\n-\t/*\n-\t * Quickly re-inject items left on \"txlist\".\n-\t */\n-\tlist_splice(txlist, &ep->rdllist);\n-\t__pm_relax(ep->ws);\n-\n-\tif (!list_empty(&ep->rdllist)) {\n-\t\tif (waitqueue_active(&ep->wq))\n-\t\t\twake_up(&ep->wq);\n-\t}\n-\n-\twrite_unlock_irq(&ep->lock);\n-}\n-\n static void ep_get(struct eventpoll *ep)\n {\n \trefcount_inc(&ep->refcount);\n@@ -832,10 +765,12 @@ static void ep_free(struct eventpoll *ep)\n static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n {\n \tstruct file *file = epi->ffd.file;\n+\tstruct llist_node *put_back_last;\n \tstruct epitems_head *to_free;\n \tstruct hlist_head *head;\n+\tLLIST_HEAD(put_back);\n \n-\tlockdep_assert_irqs_enabled();\n+\tlockdep_assert_held(&ep->mtx);\n \n \t/*\n \t * Removes poll wait queue hooks.\n@@ -867,10 +802,20 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n \n \trb_erase_cached(&epi->rbn, &ep->rbr);\n \n-\twrite_lock_irq(&ep->lock);\n-\tif (ep_is_linked(epi))\n-\t\tlist_del_init(&epi->rdllink);\n-\twrite_unlock_irq(&ep->lock);\n+\tif (llist_on_list(&epi->rdllink)) {\n+\t\tput_back_last = NULL;\n+\t\twhile (true) {\n+\t\t\tstruct llist_node *n = llist_del_first(&ep->rdllist);\n+\n+\t\t\tif (&epi->rdllink == n || WARN_ON(!n))\n+\t\t\t\tbreak;\n+\t\t\tif (!put_back_last)\n+\t\t\t\tput_back_last = n;\n+\t\t\t__llist_add(n, &put_back);\n+\t\t}\n+\t\tif (put_back_last)\n+\t\t\tllist_add_batch(put_back.first, put_back_last, &ep->rdllist);\n+\t}\n \n \twakeup_source_unregister(ep_wakeup_source(epi));\n \t/*\n@@ -974,8 +919,9 @@ static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt, int depth\n static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int depth)\n {\n \tstruct eventpoll *ep = file->private_data;\n-\tLIST_HEAD(txlist);\n-\tstruct epitem *epi, *tmp;\n+\tstruct wakeup_source *ws;\n+\tstruct llist_node *n;\n+\tstruct epitem *epi;\n \tpoll_table pt;\n \t__poll_t res = 0;\n \n@@ -989,22 +935,39 @@ static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int dep\n \t * the ready list.\n \t */\n \tmutex_lock_nested(&ep->mtx, depth);\n-\tep_start_scan(ep, &txlist);\n-\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n+\twhile (true) {\n+\t\tn = llist_del_first_init(&ep->rdllist);\n+\t\tif (!n)\n+\t\t\tbreak;\n+\n+\t\tepi = llist_entry(n, struct epitem, rdllink);\n+\n \t\tif (ep_item_poll(epi, &pt, depth + 1)) {\n \t\t\tres = EPOLLIN | EPOLLRDNORM;\n+\t\t\tepitem_ready(epi);\n \t\t\tbreak;\n \t\t} else {\n \t\t\t/*\n-\t\t\t * Item has been dropped into the ready list by the poll\n-\t\t\t * callback, but it's not actually ready, as far as\n-\t\t\t * caller requested events goes. We can remove it here.\n+\t\t\t * We need to activate ep before deactivating epi, to prevent autosuspend\n+\t\t\t * just in case epi becomes active after ep_item_poll() above.\n+\t\t\t *\n+\t\t\t * This is similar to ep_send_events().\n \t\t\t */\n+\t\t\tws = ep_wakeup_source(epi);\n+\t\t\tif (ws) {\n+\t\t\t\tif (ws->active)\n+\t\t\t\t\t__pm_stay_awake(ep->ws);\n+\t\t\t\t__pm_relax(ws);\n+\t\t\t}\n \t\t\t__pm_relax(ep_wakeup_source(epi));\n-\t\t\tlist_del_init(&epi->rdllink);\n+\n+\t\t\t/* Just in case epi becomes active right before __pm_relax() */\n+\t\t\tif (unlikely(ep_item_poll(epi, &pt, depth + 1)))\n+\t\t\t\tep_pm_stay_awake(epi);\n+\n+\t\t\t__pm_relax(ep->ws);\n \t\t}\n \t}\n-\tep_done_scan(ep, &txlist);\n \tmutex_unlock(&ep->mtx);\n \treturn res;\n }\n@@ -1153,12 +1116,10 @@ static int ep_alloc(struct eventpoll **pep)\n \t\treturn -ENOMEM;\n \n \tmutex_init(&ep->mtx);\n-\trwlock_init(&ep->lock);\n \tinit_waitqueue_head(&ep->wq);\n \tinit_waitqueue_head(&ep->poll_wait);\n-\tINIT_LIST_HEAD(&ep->rdllist);\n+\tinit_llist_head(&ep->rdllist);\n \tep->rbr = RB_ROOT_CACHED;\n-\tep->ovflist = EP_UNACTIVE_PTR;\n \tep->user = get_current_user();\n \trefcount_set(&ep->refcount, 1);\n \n@@ -1240,94 +1201,11 @@ struct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,\n }\n #endif /* CONFIG_KCMP */\n \n-/*\n- * Adds a new entry to the tail of the list in a lockless way, i.e.\n- * multiple CPUs are allowed to call this function concurrently.\n- *\n- * Beware: it is necessary to prevent any other modifications of the\n- *         existing list until all changes are completed, in other words\n- *         concurrent list_add_tail_lockless() calls should be protected\n- *         with a read lock, where write lock acts as a barrier which\n- *         makes sure all list_add_tail_lockless() calls are fully\n- *         completed.\n- *\n- *        Also an element can be locklessly added to the list only in one\n- *        direction i.e. either to the tail or to the head, otherwise\n- *        concurrent access will corrupt the list.\n- *\n- * Return: %false if element has been already added to the list, %true\n- * otherwise.\n- */\n-static inline bool list_add_tail_lockless(struct list_head *new,\n-\t\t\t\t\t  struct list_head *head)\n-{\n-\tstruct list_head *prev;\n-\n-\t/*\n-\t * This is simple 'new->next = head' operation, but cmpxchg()\n-\t * is used in order to detect that same element has been just\n-\t * added to the list from another CPU: the winner observes\n-\t * new->next == new.\n-\t */\n-\tif (!try_cmpxchg(&new->next, &new, head))\n-\t\treturn false;\n-\n-\t/*\n-\t * Initially ->next of a new element must be updated with the head\n-\t * (we are inserting to the tail) and only then pointers are atomically\n-\t * exchanged.  XCHG guarantees memory ordering, thus ->next should be\n-\t * updated before pointers are actually swapped and pointers are\n-\t * swapped before prev->next is updated.\n-\t */\n-\n-\tprev = xchg(&head->prev, new);\n-\n-\t/*\n-\t * It is safe to modify prev->next and new->prev, because a new element\n-\t * is added only to the tail and new->next is updated before XCHG.\n-\t */\n-\n-\tprev->next = new;\n-\tnew->prev = prev;\n-\n-\treturn true;\n-}\n-\n-/*\n- * Chains a new epi entry to the tail of the ep->ovflist in a lockless way,\n- * i.e. multiple CPUs are allowed to call this function concurrently.\n- *\n- * Return: %false if epi element has been already chained, %true otherwise.\n- */\n-static inline bool chain_epi_lockless(struct epitem *epi)\n-{\n-\tstruct eventpoll *ep = epi->ep;\n-\n-\t/* Fast preliminary check */\n-\tif (epi->next != EP_UNACTIVE_PTR)\n-\t\treturn false;\n-\n-\t/* Check that the same epi has not been just chained from another CPU */\n-\tif (cmpxchg(&epi->next, EP_UNACTIVE_PTR, NULL) != EP_UNACTIVE_PTR)\n-\t\treturn false;\n-\n-\t/* Atomically exchange tail */\n-\tepi->next = xchg(&ep->ovflist, epi);\n-\n-\treturn true;\n-}\n-\n /*\n  * This is the callback that is passed to the wait queue wakeup\n  * mechanism. It is called by the stored file descriptors when they\n  * have events to report.\n  *\n- * This callback takes a read lock in order not to contend with concurrent\n- * events from another file descriptor, thus all modifications to ->rdllist\n- * or ->ovflist are lockless.  Read lock is paired with the write lock from\n- * ep_start/done_scan(), which stops all list modifications and guarantees\n- * that lists state is seen correctly.\n- *\n  * Another thing worth to mention is that ep_poll_callback() can be called\n  * concurrently for the same @epi from different CPUs if poll table was inited\n  * with several wait queues entries.  Plural wakeup from different CPUs of a\n@@ -1337,15 +1215,11 @@ static inline bool chain_epi_lockless(struct epitem *epi)\n  */\n static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n {\n-\tint pwake = 0;\n \tstruct epitem *epi = ep_item_from_wait(wait);\n \tstruct eventpoll *ep = epi->ep;\n \t__poll_t pollflags = key_to_poll(key);\n-\tunsigned long flags;\n \tint ewake = 0;\n \n-\tread_lock_irqsave(&ep->lock, flags);\n-\n \tep_set_busy_poll_napi_id(epi);\n \n \t/*\n@@ -1355,7 +1229,7 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * until the next EPOLL_CTL_MOD will be issued.\n \t */\n \tif (!(epi->event.events & ~EP_PRIVATE_BITS))\n-\t\tgoto out_unlock;\n+\t\tgoto out;\n \n \t/*\n \t * Check the events coming with the callback. At this stage, not\n@@ -1364,22 +1238,10 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * test for \"key\" != NULL before the event match test.\n \t */\n \tif (pollflags && !(pollflags & epi->event.events))\n-\t\tgoto out_unlock;\n+\t\tgoto out;\n \n-\t/*\n-\t * If we are transferring events to userspace, we can hold no locks\n-\t * (because we're accessing user memory, and because of linux f_op->poll()\n-\t * semantics). All the events that happen during that period of time are\n-\t * chained in ep->ovflist and requeued later on.\n-\t */\n-\tif (READ_ONCE(ep->ovflist) != EP_UNACTIVE_PTR) {\n-\t\tif (chain_epi_lockless(epi))\n-\t\t\tep_pm_stay_awake_rcu(epi);\n-\t} else if (!ep_is_linked(epi)) {\n-\t\t/* In the usual case, add event to ready list. */\n-\t\tif (list_add_tail_lockless(&epi->rdllink, &ep->rdllist))\n-\t\t\tep_pm_stay_awake_rcu(epi);\n-\t}\n+\tep_pm_stay_awake_rcu(epi);\n+\tepitem_ready(epi);\n \n \t/*\n \t * Wake up ( if active ) both the eventpoll wait list and the ->poll()\n@@ -1408,15 +1270,9 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t\t\twake_up(&ep->wq);\n \t}\n \tif (waitqueue_active(&ep->poll_wait))\n-\t\tpwake++;\n-\n-out_unlock:\n-\tread_unlock_irqrestore(&ep->lock, flags);\n-\n-\t/* We have to call this outside the lock */\n-\tif (pwake)\n \t\tep_poll_safewake(ep, epi, pollflags & EPOLL_URING_WAKE);\n \n+out:\n \tif (!(epi->event.events & EPOLLEXCLUSIVE))\n \t\tewake = 1;\n \n@@ -1661,8 +1517,6 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \tif (is_file_epoll(tfile))\n \t\ttep = tfile->private_data;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tif (unlikely(percpu_counter_compare(&ep->user->epoll_watches,\n \t\t\t\t\t    max_user_watches) >= 0))\n \t\treturn -ENOSPC;\n@@ -1674,11 +1528,10 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t}\n \n \t/* Item initialization follow here ... */\n-\tINIT_LIST_HEAD(&epi->rdllink);\n+\tinit_llist_node(&epi->rdllink);\n \tepi->ep = ep;\n \tep_set_ffd(&epi->ffd, tfile, fd);\n \tepi->event = *event;\n-\tepi->next = EP_UNACTIVE_PTR;\n \n \tif (tep)\n \t\tmutex_lock_nested(&tep->mtx, 1);\n@@ -1745,16 +1598,13 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\treturn -ENOMEM;\n \t}\n \n-\t/* We have to drop the new item inside our item list to keep track of it */\n-\twrite_lock_irq(&ep->lock);\n-\n \t/* record NAPI ID of new item if present */\n \tep_set_busy_poll_napi_id(epi);\n \n \t/* If the file is already \"ready\" we drop it inside the ready list */\n-\tif (revents && !ep_is_linked(epi)) {\n-\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n+\tif (revents) {\n \t\tep_pm_stay_awake(epi);\n+\t\tepitem_ready(epi);\n \n \t\t/* Notify waiting tasks that events are available */\n \t\tif (waitqueue_active(&ep->wq))\n@@ -1763,8 +1613,6 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\t\tpwake++;\n \t}\n \n-\twrite_unlock_irq(&ep->lock);\n-\n \t/* We have to call this outside the lock */\n \tif (pwake)\n \t\tep_poll_safewake(ep, NULL, 0);\n@@ -1779,11 +1627,8 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t\t     const struct epoll_event *event)\n {\n-\tint pwake = 0;\n \tpoll_table pt;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tinit_poll_funcptr(&pt, NULL);\n \n \t/*\n@@ -1827,24 +1672,16 @@ static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t * list, push it inside.\n \t */\n \tif (ep_item_poll(epi, &pt, 1)) {\n-\t\twrite_lock_irq(&ep->lock);\n-\t\tif (!ep_is_linked(epi)) {\n-\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n-\t\t\tep_pm_stay_awake(epi);\n+\t\tep_pm_stay_awake(epi);\n+\t\tepitem_ready(epi);\n \n-\t\t\t/* Notify waiting tasks that events are available */\n-\t\t\tif (waitqueue_active(&ep->wq))\n-\t\t\t\twake_up(&ep->wq);\n-\t\t\tif (waitqueue_active(&ep->poll_wait))\n-\t\t\t\tpwake++;\n-\t\t}\n-\t\twrite_unlock_irq(&ep->lock);\n+\t\t/* Notify waiting tasks that events are available */\n+\t\tif (waitqueue_active(&ep->wq))\n+\t\t\twake_up(&ep->wq);\n+\t\tif (waitqueue_active(&ep->poll_wait))\n+\t\t\tep_poll_safewake(ep, NULL, 0);\n \t}\n \n-\t/* We have to call this outside the lock */\n-\tif (pwake)\n-\t\tep_poll_safewake(ep, NULL, 0);\n-\n \treturn 0;\n }\n \n@@ -1852,7 +1689,7 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t  struct epoll_event __user *events, int maxevents)\n {\n \tstruct epitem *epi, *tmp;\n-\tLIST_HEAD(txlist);\n+\tLLIST_HEAD(txlist);\n \tpoll_table pt;\n \tint res = 0;\n \n@@ -1867,19 +1704,18 @@ static int ep_send_events(struct eventpoll *ep,\n \tinit_poll_funcptr(&pt, NULL);\n \n \tmutex_lock(&ep->mtx);\n-\tep_start_scan(ep, &txlist);\n \n-\t/*\n-\t * We can loop without lock because we are passed a task private list.\n-\t * Items cannot vanish during the loop we are holding ep->mtx.\n-\t */\n-\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n+\twhile (res < maxevents) {\n \t\tstruct wakeup_source *ws;\n+\t\tstruct llist_node *n;\n \t\t__poll_t revents;\n \n-\t\tif (res >= maxevents)\n+\t\tn = llist_del_first(&ep->rdllist);\n+\t\tif (!n)\n \t\t\tbreak;\n \n+\t\tepi = llist_entry(n, struct epitem, rdllink);\n+\n \t\t/*\n \t\t * Activate ep->ws before deactivating epi->ws to prevent\n \t\t * triggering auto-suspend here (in case we reactive epi->ws\n@@ -1896,21 +1732,30 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t__pm_relax(ws);\n \t\t}\n \n-\t\tlist_del_init(&epi->rdllink);\n-\n \t\t/*\n \t\t * If the event mask intersect the caller-requested one,\n \t\t * deliver the event to userspace. Again, we are holding ep->mtx,\n \t\t * so no operations coming from userspace can change the item.\n \t\t */\n \t\trevents = ep_item_poll(epi, &pt, 1);\n-\t\tif (!revents)\n+\t\tif (!revents) {\n+\t\t\tinit_llist_node(n);\n+\n+\t\t\t/*\n+\t\t\t * Just in case epi becomes ready after ep_item_poll() above, but before\n+\t\t\t * init_llist_node(). Make sure to add it to the ready list, otherwise an\n+\t\t\t * event may be lost.\n+\t\t\t */\n+\t\t\tif (unlikely(ep_item_poll(epi, &pt, 1))) {\n+\t\t\t\tep_pm_stay_awake(epi);\n+\t\t\t\tepitem_ready(epi);\n+\t\t\t}\n \t\t\tcontinue;\n+\t\t}\n \n \t\tevents = epoll_put_uevent(revents, epi->event.data, events);\n \t\tif (!events) {\n-\t\t\tlist_add(&epi->rdllink, &txlist);\n-\t\t\tep_pm_stay_awake(epi);\n+\t\t\tllist_add(&epi->rdllink, &ep->rdllist);\n \t\t\tif (!res)\n \t\t\t\tres = -EFAULT;\n \t\t\tbreak;\n@@ -1918,25 +1763,31 @@ static int ep_send_events(struct eventpoll *ep,\n \t\tres++;\n \t\tif (epi->event.events & EPOLLONESHOT)\n \t\t\tepi->event.events &= EP_PRIVATE_BITS;\n-\t\telse if (!(epi->event.events & EPOLLET)) {\n+\t\t__llist_add(n, &txlist);\n+\t}\n+\n+\tllist_for_each_entry_safe(epi, tmp, txlist.first, rdllink) {\n+\t\tinit_llist_node(&epi->rdllink);\n+\n+\t\tif (!(epi->event.events & EPOLLET)) {\n \t\t\t/*\n-\t\t\t * If this file has been added with Level\n-\t\t\t * Trigger mode, we need to insert back inside\n-\t\t\t * the ready list, so that the next call to\n-\t\t\t * epoll_wait() will check again the events\n-\t\t\t * availability. At this point, no one can insert\n-\t\t\t * into ep->rdllist besides us. The epoll_ctl()\n-\t\t\t * callers are locked out by\n-\t\t\t * ep_send_events() holding \"mtx\" and the\n-\t\t\t * poll callback will queue them in ep->ovflist.\n+\t\t\t * If this file has been added with Level Trigger mode, we need to insert\n+\t\t\t * back inside the ready list, so that the next call to epoll_wait() will\n+\t\t\t * check again the events availability.\n \t\t\t */\n-\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n \t\t\tep_pm_stay_awake(epi);\n+\t\t\tepitem_ready(epi);\n \t\t}\n \t}\n-\tep_done_scan(ep, &txlist);\n+\n+\t__pm_relax(ep->ws);\n \tmutex_unlock(&ep->mtx);\n \n+\tif (!llist_empty(&ep->rdllist)) {\n+\t\tif (waitqueue_active(&ep->wq))\n+\t\t\twake_up(&ep->wq);\n+\t}\n+\n \treturn res;\n }\n \n@@ -2029,8 +1880,6 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \twait_queue_entry_t wait;\n \tktime_t expires, *to = NULL;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tif (timeout && (timeout->tv_sec | timeout->tv_nsec)) {\n \t\tslack = select_estimate_accuracy(timeout);\n \t\tto = &expires;\n@@ -2090,54 +1939,15 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \t\tinit_wait(&wait);\n \t\twait.func = ep_autoremove_wake_function;\n \n-\t\twrite_lock_irq(&ep->lock);\n-\t\t/*\n-\t\t * Barrierless variant, waitqueue_active() is called under\n-\t\t * the same lock on wakeup ep_poll_callback() side, so it\n-\t\t * is safe to avoid an explicit barrier.\n-\t\t */\n-\t\t__set_current_state(TASK_INTERRUPTIBLE);\n+\t\tprepare_to_wait_exclusive(&ep->wq, &wait, TASK_INTERRUPTIBLE);\n \n-\t\t/*\n-\t\t * Do the final check under the lock. ep_start/done_scan()\n-\t\t * plays with two lists (->rdllist and ->ovflist) and there\n-\t\t * is always a race when both lists are empty for short\n-\t\t * period of time although events are pending, so lock is\n-\t\t * important.\n-\t\t */\n-\t\teavail = ep_events_available(ep);\n-\t\tif (!eavail)\n-\t\t\t__add_wait_queue_exclusive(&ep->wq, &wait);\n-\n-\t\twrite_unlock_irq(&ep->lock);\n-\n-\t\tif (!eavail)\n+\t\tif (!ep_events_available(ep))\n \t\t\ttimed_out = !ep_schedule_timeout(to) ||\n \t\t\t\t!schedule_hrtimeout_range(to, slack,\n \t\t\t\t\t\t\t  HRTIMER_MODE_ABS);\n-\t\t__set_current_state(TASK_RUNNING);\n-\n-\t\t/*\n-\t\t * We were woken up, thus go and try to harvest some events.\n-\t\t * If timed out and still on the wait queue, recheck eavail\n-\t\t * carefully under lock, below.\n-\t\t */\n-\t\teavail = 1;\n \n-\t\tif (!list_empty_careful(&wait.entry)) {\n-\t\t\twrite_lock_irq(&ep->lock);\n-\t\t\t/*\n-\t\t\t * If the thread timed out and is not on the wait queue,\n-\t\t\t * it means that the thread was woken up after its\n-\t\t\t * timeout expired before it could reacquire the lock.\n-\t\t\t * Thus, when wait.entry is empty, it needs to harvest\n-\t\t\t * events.\n-\t\t\t */\n-\t\t\tif (timed_out)\n-\t\t\t\teavail = list_empty(&wait.entry);\n-\t\t\t__remove_wait_queue(&ep->wq, &wait);\n-\t\t\twrite_unlock_irq(&ep->lock);\n-\t\t}\n+\t\tfinish_wait(&ep->wq, &wait);\n+\t\teavail = ep_events_available(ep);\n \t}\n }\n ",
    "stats": {
      "insertions": 134,
      "deletions": 324,
      "files": 1
    }
  },
  {
    "sha": "0d519bb0de3bf0ac9e6f401d4910fc119062d7be",
    "message": "brd: fix sleeping function called from invalid context in brd_insert_page()\n\n__xa_cmpxchg() is called with rcu_read_lock(), and it will allocate\nmemory if necessary.\n\nFix the problem by moving rcu_read_lock() after __xa_cmpxchg(), meanwhile,\nit still should be held before xa_unlock(), prevent returned page to be\nfreed by concurrent discard.\n\nFixes: bbcacab2e8ee (\"brd: avoid extra xarray lookups on first write\")\nReported-by: syzbot+ea4c8fd177a47338881a@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/all/685ec4c9.a00a0220.129264.000c.GAE@google.com/\nSigned-off-by: Yu Kuai <yukuai3@huawei.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20250630112828.421219-1-yukuai1@huaweicloud.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
    "author": "Yu Kuai",
    "date": "2025-07-01T08:14:01-06:00",
    "files_changed": [
      "drivers/block/brd.c"
    ],
    "diff": "diff --git a/drivers/block/brd.c b/drivers/block/brd.c\nindex b1be6c510372..0c2eabe14af3 100644\n--- a/drivers/block/brd.c\n+++ b/drivers/block/brd.c\n@@ -64,13 +64,15 @@ static struct page *brd_insert_page(struct brd_device *brd, sector_t sector,\n \n \trcu_read_unlock();\n \tpage = alloc_page(gfp | __GFP_ZERO | __GFP_HIGHMEM);\n-\trcu_read_lock();\n-\tif (!page)\n+\tif (!page) {\n+\t\trcu_read_lock();\n \t\treturn ERR_PTR(-ENOMEM);\n+\t}\n \n \txa_lock(&brd->brd_pages);\n \tret = __xa_cmpxchg(&brd->brd_pages, sector >> PAGE_SECTORS_SHIFT, NULL,\n \t\t\tpage, gfp);\n+\trcu_read_lock();\n \tif (ret) {\n \t\txa_unlock(&brd->brd_pages);\n \t\t__free_page(page);",
    "stats": {
      "insertions": 4,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "009836b4fa52f92cba33618e773b1094affa8cd2",
    "message": "sched/core: Fix migrate_swap() vs. hotplug\n\nOn Mon, Jun 02, 2025 at 03:22:13PM +0800, Kuyo Chang wrote:\n\n> So, the potential race scenario is:\n>\n> \tCPU0\t\t\t\t\t\t\tCPU1\n> \t// doing migrate_swap(cpu0/cpu1)\n> \tstop_two_cpus()\n> \t\t\t\t\t\t\t  ...\n> \t\t\t\t\t\t\t // doing _cpu_down()\n> \t\t\t\t\t\t\t      sched_cpu_deactivate()\n> \t\t\t\t\t\t\t\tset_cpu_active(cpu, false);\n> \t\t\t\t\t\t\t\tbalance_push_set(cpu, true);\n> \tcpu_stop_queue_two_works\n> \t    __cpu_stop_queue_work(stopper1,...);\n> \t    __cpu_stop_queue_work(stopper2,..);\n> \tstop_cpus_in_progress -> true\n> \t\tpreempt_enable();\n> \t\t\t\t\t\t\t\t...\n> \t\t\t\t\t\t\t1st balance_push\n> \t\t\t\t\t\t\tstop_one_cpu_nowait\n> \t\t\t\t\t\t\tcpu_stop_queue_work\n> \t\t\t\t\t\t\t__cpu_stop_queue_work\n> \t\t\t\t\t\t\tlist_add_tail  -> 1st add push_work\n> \t\t\t\t\t\t\twake_up_q(&wakeq);  -> \"wakeq is empty.\n> \t\t\t\t\t\t\t\t\t\tThis implies that the stopper is at wakeq@migrate_swap.\"\n> \tpreempt_disable\n> \twake_up_q(&wakeq);\n> \t        wake_up_process // wakeup migrate/0\n> \t\t    try_to_wake_up\n> \t\t        ttwu_queue\n> \t\t            ttwu_queue_cond ->meet below case\n> \t\t                if (cpu == smp_processor_id())\n> \t\t\t         return false;\n> \t\t\tttwu_do_activate\n> \t\t\t//migrate/0 wakeup done\n> \t\twake_up_process // wakeup migrate/1\n> \t           try_to_wake_up\n> \t\t    ttwu_queue\n> \t\t\tttwu_queue_cond\n> \t\t        ttwu_queue_wakelist\n> \t\t\t__ttwu_queue_wakelist\n> \t\t\t__smp_call_single_queue\n> \tpreempt_enable();\n>\n> \t\t\t\t\t\t\t2nd balance_push\n> \t\t\t\t\t\t\tstop_one_cpu_nowait\n> \t\t\t\t\t\t\tcpu_stop_queue_work\n> \t\t\t\t\t\t\t__cpu_stop_queue_work\n> \t\t\t\t\t\t\tlist_add_tail  -> 2nd add push_work, so the double list add is detected\n> \t\t\t\t\t\t\t...\n> \t\t\t\t\t\t\t...\n> \t\t\t\t\t\t\tcpu1 get ipi, do sched_ttwu_pending, wakeup migrate/1\n>\n\nSo this balance_push() is part of schedule(), and schedule() is supposed\nto switch to stopper task, but because of this race condition, stopper\ntask is stuck in WAKING state and not actually visible to be picked.\n\nTherefore CPU1 can do another schedule() and end up doing another\nbalance_push() even though the last one hasn't been done yet.\n\nThis is a confluence of fail, where both wake_q and ttwu_wakelist can\ncause crucial wakeups to be delayed, resulting in the malfunction of\nbalance_push.\n\nSince there is only a single stopper thread to be woken, the wake_q\ndoesn't really add anything here, and can be removed in favour of\ndirect wakeups of the stopper thread.\n\nThen add a clause to ttwu_queue_cond() to ensure the stopper threads\nare never queued / delayed.\n\nOf all 3 moving parts, the last addition was the balance_push()\nmachinery, so pick that as the point the bug was introduced.\n\nFixes: 2558aacff858 (\"sched/hotplug: Ensure only per-cpu kthreads run during hotplug\")\nReported-by: Kuyo Chang <kuyo.chang@mediatek.com>\nSigned-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nTested-by: Kuyo Chang <kuyo.chang@mediatek.com>\nLink: https://lkml.kernel.org/r/20250605100009.GO39944@noisy.programming.kicks-ass.net",
    "author": "Peter Zijlstra",
    "date": "2025-07-01T15:02:03+02:00",
    "files_changed": [
      "kernel/sched/core.c",
      "kernel/stop_machine.c"
    ],
    "diff": "diff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex cd80b66a960a..ec68fc686bd7 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -3943,6 +3943,11 @@ static inline bool ttwu_queue_cond(struct task_struct *p, int cpu)\n \tif (!scx_allow_ttwu_queue(p))\n \t\treturn false;\n \n+#ifdef CONFIG_SMP\n+\tif (p->sched_class == &stop_sched_class)\n+\t\treturn false;\n+#endif\n+\n \t/*\n \t * Do not complicate things with the async wake_list while the CPU is\n \t * in hotplug state.\ndiff --git a/kernel/stop_machine.c b/kernel/stop_machine.c\nindex 5d2d0562115b..3fe6b0c99f3d 100644\n--- a/kernel/stop_machine.c\n+++ b/kernel/stop_machine.c\n@@ -82,18 +82,15 @@ static void cpu_stop_signal_done(struct cpu_stop_done *done)\n }\n \n static void __cpu_stop_queue_work(struct cpu_stopper *stopper,\n-\t\t\t\t\tstruct cpu_stop_work *work,\n-\t\t\t\t\tstruct wake_q_head *wakeq)\n+\t\t\t\t  struct cpu_stop_work *work)\n {\n \tlist_add_tail(&work->list, &stopper->works);\n-\twake_q_add(wakeq, stopper->thread);\n }\n \n /* queue @work to @stopper.  if offline, @work is completed immediately */\n static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\n {\n \tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n-\tDEFINE_WAKE_Q(wakeq);\n \tunsigned long flags;\n \tbool enabled;\n \n@@ -101,12 +98,13 @@ static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\n \traw_spin_lock_irqsave(&stopper->lock, flags);\n \tenabled = stopper->enabled;\n \tif (enabled)\n-\t\t__cpu_stop_queue_work(stopper, work, &wakeq);\n+\t\t__cpu_stop_queue_work(stopper, work);\n \telse if (work->done)\n \t\tcpu_stop_signal_done(work->done);\n \traw_spin_unlock_irqrestore(&stopper->lock, flags);\n \n-\twake_up_q(&wakeq);\n+\tif (enabled)\n+\t\twake_up_process(stopper->thread);\n \tpreempt_enable();\n \n \treturn enabled;\n@@ -264,7 +262,6 @@ static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\n {\n \tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);\n \tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);\n-\tDEFINE_WAKE_Q(wakeq);\n \tint err;\n \n retry:\n@@ -300,8 +297,8 @@ static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\n \t}\n \n \terr = 0;\n-\t__cpu_stop_queue_work(stopper1, work1, &wakeq);\n-\t__cpu_stop_queue_work(stopper2, work2, &wakeq);\n+\t__cpu_stop_queue_work(stopper1, work1);\n+\t__cpu_stop_queue_work(stopper2, work2);\n \n unlock:\n \traw_spin_unlock(&stopper2->lock);\n@@ -316,7 +313,10 @@ static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\n \t\tgoto retry;\n \t}\n \n-\twake_up_q(&wakeq);\n+\tif (!err) {\n+\t\twake_up_process(stopper1->thread);\n+\t\twake_up_process(stopper2->thread);\n+\t}\n \tpreempt_enable();\n \n \treturn err;",
    "stats": {
      "insertions": 15,
      "deletions": 10,
      "files": 2
    }
  },
  {
    "sha": "d6811074203b13f715ce2480ac64c5b1c773f2a5",
    "message": "nvme-multipath: fix suspicious RCU usage warning\n\nWhen I run the NVME over TCP test in virtme-ng, I get the following\n\"suspicious RCU usage\" warning in nvme_mpath_add_sysfs_link():\n\n'''\n[    5.024557][   T44] nvmet: Created nvm controller 1 for subsystem nqn.2025-06.org.nvmexpress.mptcp for NQN nqn.2014-08.org.nvmexpress:uuid:f7f6b5e0-ff97-4894-98ac-c85309e0bc77.\n[    5.027401][  T183] nvme nvme0: creating 2 I/O queues.\n[    5.029017][  T183] nvme nvme0: mapped 2/0/0 default/read/poll queues.\n[    5.032587][  T183] nvme nvme0: new ctrl: NQN \"nqn.2025-06.org.nvmexpress.mptcp\", addr 127.0.0.1:4420, hostnqn: nqn.2014-08.org.nvmexpress:uuid:f7f6b5e0-ff97-4894-98ac-c85309e0bc77\n[    5.042214][   T25]\n[    5.042440][   T25] =============================\n[    5.042579][   T25] WARNING: suspicious RCU usage\n[    5.042705][   T25] 6.16.0-rc3+ #23 Not tainted\n[    5.042812][   T25] -----------------------------\n[    5.042934][   T25] drivers/nvme/host/multipath.c:1203 RCU-list traversed in non-reader section!!\n[    5.043111][   T25]\n[    5.043111][   T25] other info that might help us debug this:\n[    5.043111][   T25]\n[    5.043341][   T25]\n[    5.043341][   T25] rcu_scheduler_active = 2, debug_locks = 1\n[    5.043502][   T25] 3 locks held by kworker/u9:0/25:\n[    5.043615][   T25]  #0: ffff888008730948 ((wq_completion)async){+.+.}-{0:0}, at: process_one_work+0x7ed/0x1350\n[    5.043830][   T25]  #1: ffffc900001afd40 ((work_completion)(&entry->work)){+.+.}-{0:0}, at: process_one_work+0xcf3/0x1350\n[    5.044084][   T25]  #2: ffff888013ee0020 (&head->srcu){.+.+}-{0:0}, at: nvme_mpath_add_sysfs_link.part.0+0xb4/0x3a0\n[    5.044300][   T25]\n[    5.044300][   T25] stack backtrace:\n[    5.044439][   T25] CPU: 0 UID: 0 PID: 25 Comm: kworker/u9:0 Not tainted 6.16.0-rc3+ #23 PREEMPT(full)\n[    5.044441][   T25] Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011\n[    5.044442][   T25] Workqueue: async async_run_entry_fn\n[    5.044445][   T25] Call Trace:\n[    5.044446][   T25]  <TASK>\n[    5.044449][   T25]  dump_stack_lvl+0x6f/0xb0\n[    5.044453][   T25]  lockdep_rcu_suspicious.cold+0x4f/0xb1\n[    5.044457][   T25]  nvme_mpath_add_sysfs_link.part.0+0x2fb/0x3a0\n[    5.044459][   T25]  ? queue_work_on+0x90/0xf0\n[    5.044461][   T25]  ? lockdep_hardirqs_on+0x78/0x110\n[    5.044466][   T25]  nvme_mpath_set_live+0x1e9/0x4f0\n[    5.044470][   T25]  nvme_mpath_add_disk+0x240/0x2f0\n[    5.044472][   T25]  ? __pfx_nvme_mpath_add_disk+0x10/0x10\n[    5.044475][   T25]  ? add_disk_fwnode+0x361/0x580\n[    5.044480][   T25]  nvme_alloc_ns+0x81c/0x17c0\n[    5.044483][   T25]  ? kasan_quarantine_put+0x104/0x240\n[    5.044487][   T25]  ? __pfx_nvme_alloc_ns+0x10/0x10\n[    5.044495][   T25]  ? __pfx_nvme_find_get_ns+0x10/0x10\n[    5.044496][   T25]  ? rcu_read_lock_any_held+0x45/0xa0\n[    5.044498][   T25]  ? validate_chain+0x232/0x4f0\n[    5.044503][   T25]  nvme_scan_ns+0x4c8/0x810\n[    5.044506][   T25]  ? __pfx_nvme_scan_ns+0x10/0x10\n[    5.044508][   T25]  ? find_held_lock+0x2b/0x80\n[    5.044512][   T25]  ? ktime_get+0x16d/0x220\n[    5.044517][   T25]  ? kvm_clock_get_cycles+0x18/0x30\n[    5.044520][   T25]  ? __pfx_nvme_scan_ns_async+0x10/0x10\n[    5.044522][   T25]  async_run_entry_fn+0x97/0x560\n[    5.044523][   T25]  ? rcu_is_watching+0x12/0xc0\n[    5.044526][   T25]  process_one_work+0xd3c/0x1350\n[    5.044532][   T25]  ? __pfx_process_one_work+0x10/0x10\n[    5.044536][   T25]  ? assign_work+0x16c/0x240\n[    5.044539][   T25]  worker_thread+0x4da/0xd50\n[    5.044545][   T25]  ? __pfx_worker_thread+0x10/0x10\n[    5.044546][   T25]  kthread+0x356/0x5c0\n[    5.044548][   T25]  ? __pfx_kthread+0x10/0x10\n[    5.044549][   T25]  ? ret_from_fork+0x1b/0x2e0\n[    5.044552][   T25]  ? __lock_release.isra.0+0x5d/0x180\n[    5.044553][   T25]  ? ret_from_fork+0x1b/0x2e0\n[    5.044555][   T25]  ? rcu_is_watching+0x12/0xc0\n[    5.044557][   T25]  ? __pfx_kthread+0x10/0x10\n[    5.044559][   T25]  ret_from_fork+0x218/0x2e0\n[    5.044561][   T25]  ? __pfx_kthread+0x10/0x10\n[    5.044562][   T25]  ret_from_fork_asm+0x1a/0x30\n[    5.044570][   T25]  </TASK>\n'''\n\nThis patch uses sleepable RCU version of helper list_for_each_entry_srcu()\ninstead of list_for_each_entry_rcu() to fix it.\n\nFixes: 4dbd2b2ebe4c (\"nvme-multipath: Add visibility for round-robin io-policy\")\nSigned-off-by: Geliang Tang <tanggeliang@kylinos.cn>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Hannes Reinecke <hare@suse.de>\nReviewed-by: Nilay Shroff <nilay@linux.ibm.com>\nSigned-off-by: Christoph Hellwig <hch@lst.de>",
    "author": "Geliang Tang",
    "date": "2025-07-01T08:17:02+02:00",
    "files_changed": [
      "drivers/nvme/host/multipath.c"
    ],
    "diff": "diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c\nindex e03f3a29bd75..5d1ad28986e9 100644\n--- a/drivers/nvme/host/multipath.c\n+++ b/drivers/nvme/host/multipath.c\n@@ -1200,7 +1200,8 @@ void nvme_mpath_add_sysfs_link(struct nvme_ns_head *head)\n \t */\n \tsrcu_idx = srcu_read_lock(&head->srcu);\n \n-\tlist_for_each_entry_rcu(ns, &head->list, siblings) {\n+\tlist_for_each_entry_srcu(ns, &head->list, siblings,\n+\t\t\t\t srcu_read_lock_held(&head->srcu)) {\n \t\t/*\n \t\t * Ensure that ns path disk node is already added otherwise we\n \t\t * may get invalid kobj name for target",
    "stats": {
      "insertions": 2,
      "deletions": 1,
      "files": 1
    }
  }
]
[
  {
    "sha": "cbe4134ea4bc493239786220bd69cb8a13493190",
    "message": "fs: export anon_inode_make_secure_inode() and fix secretmem LSM bypass\n\nExport anon_inode_make_secure_inode() to allow KVM guest_memfd to create\nanonymous inodes with proper security context. This replaces the current\npattern of calling alloc_anon_inode() followed by\ninode_init_security_anon() for creating security context manually.\n\nThis change also fixes a security regression in secretmem where the\nS_PRIVATE flag was not cleared after alloc_anon_inode(), causing\nLSM/SELinux checks to be bypassed for secretmem file descriptors.\n\nAs guest_memfd currently resides in the KVM module, we need to export this\nsymbol for use outside the core kernel. In the future, guest_memfd might be\nmoved to core-mm, at which point the symbols no longer would have to be\nexported. When/if that happens is still unclear.\n\nFixes: 2bfe15c52612 (\"mm: create security context for memfd_secret inodes\")\nSuggested-by: David Hildenbrand <david@redhat.com>\nSuggested-by: Mike Rapoport <rppt@kernel.org>\nSigned-off-by: Shivank Garg <shivankg@amd.com>\nLink: https://lore.kernel.org/20250620070328.803704-3-shivankg@amd.com\nAcked-by: \"Mike Rapoport (Microsoft)\" <rppt@kernel.org>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
    "author": "Shivank Garg",
    "date": "2025-06-23T12:41:17+02:00",
    "files_changed": [
      "fs/anon_inodes.c",
      "include/linux/fs.h",
      "mm/secretmem.c"
    ],
    "diff": "diff --git a/fs/anon_inodes.c b/fs/anon_inodes.c\nindex e51e7d88980a..1d847a939f29 100644\n--- a/fs/anon_inodes.c\n+++ b/fs/anon_inodes.c\n@@ -98,14 +98,25 @@ static struct file_system_type anon_inode_fs_type = {\n \t.kill_sb\t= kill_anon_super,\n };\n \n-static struct inode *anon_inode_make_secure_inode(\n-\tconst char *name,\n-\tconst struct inode *context_inode)\n+/**\n+ * anon_inode_make_secure_inode - allocate an anonymous inode with security context\n+ * @sb:\t\t[in]\tSuperblock to allocate from\n+ * @name:\t[in]\tName of the class of the newfile (e.g., \"secretmem\")\n+ * @context_inode:\n+ *\t\t[in]\tOptional parent inode for security inheritance\n+ *\n+ * The function ensures proper security initialization through the LSM hook\n+ * security_inode_init_security_anon().\n+ *\n+ * Return:\tPointer to new inode on success, ERR_PTR on failure.\n+ */\n+struct inode *anon_inode_make_secure_inode(struct super_block *sb, const char *name,\n+\t\t\t\t\t   const struct inode *context_inode)\n {\n \tstruct inode *inode;\n \tint error;\n \n-\tinode = alloc_anon_inode(anon_inode_mnt->mnt_sb);\n+\tinode = alloc_anon_inode(sb);\n \tif (IS_ERR(inode))\n \t\treturn inode;\n \tinode->i_flags &= ~S_PRIVATE;\n@@ -118,6 +129,7 @@ static struct inode *anon_inode_make_secure_inode(\n \t}\n \treturn inode;\n }\n+EXPORT_SYMBOL_GPL_FOR_MODULES(anon_inode_make_secure_inode, \"kvm\");\n \n static struct file *__anon_inode_getfile(const char *name,\n \t\t\t\t\t const struct file_operations *fops,\n@@ -132,7 +144,8 @@ static struct file *__anon_inode_getfile(const char *name,\n \t\treturn ERR_PTR(-ENOENT);\n \n \tif (make_inode) {\n-\t\tinode =\tanon_inode_make_secure_inode(name, context_inode);\n+\t\tinode =\tanon_inode_make_secure_inode(anon_inode_mnt->mnt_sb,\n+\t\t\t\t\t\t     name, context_inode);\n \t\tif (IS_ERR(inode)) {\n \t\t\tfile = ERR_CAST(inode);\n \t\t\tgoto err;\ndiff --git a/include/linux/fs.h b/include/linux/fs.h\nindex 4ec77da65f14..3a8e643c4279 100644\n--- a/include/linux/fs.h\n+++ b/include/linux/fs.h\n@@ -3606,6 +3606,8 @@ extern int simple_write_begin(struct file *file, struct address_space *mapping,\n extern const struct address_space_operations ram_aops;\n extern int always_delete_dentry(const struct dentry *);\n extern struct inode *alloc_anon_inode(struct super_block *);\n+struct inode *anon_inode_make_secure_inode(struct super_block *sb, const char *name,\n+\t\t\t\t\t   const struct inode *context_inode);\n extern int simple_nosetlease(struct file *, int, struct file_lease **, void **);\n extern const struct dentry_operations simple_dentry_operations;\n \ndiff --git a/mm/secretmem.c b/mm/secretmem.c\nindex 589b26c2d553..9a11a38a6770 100644\n--- a/mm/secretmem.c\n+++ b/mm/secretmem.c\n@@ -195,18 +195,11 @@ static struct file *secretmem_file_create(unsigned long flags)\n \tstruct file *file;\n \tstruct inode *inode;\n \tconst char *anon_name = \"[secretmem]\";\n-\tint err;\n \n-\tinode = alloc_anon_inode(secretmem_mnt->mnt_sb);\n+\tinode = anon_inode_make_secure_inode(secretmem_mnt->mnt_sb, anon_name, NULL);\n \tif (IS_ERR(inode))\n \t\treturn ERR_CAST(inode);\n \n-\terr = security_inode_init_security_anon(inode, &QSTR(anon_name), NULL);\n-\tif (err) {\n-\t\tfile = ERR_PTR(err);\n-\t\tgoto err_free_inode;\n-\t}\n-\n \tfile = alloc_file_pseudo(inode, secretmem_mnt, \"secretmem\",\n \t\t\t\t O_RDWR, &secretmem_fops);\n \tif (IS_ERR(file))",
    "stats": {
      "insertions": 21,
      "deletions": 13,
      "files": 3
    }
  },
  {
    "sha": "b993ea46b3b601915ceaaf3c802adf11e7d6bac6",
    "message": "atm: clip: prevent NULL deref in clip_push()\n\nBlamed commit missed that vcc_destroy_socket() calls\nclip_push() with a NULL skb.\n\nIf clip_devs is NULL, clip_push() then crashes when reading\nskb->truesize.\n\nFixes: 93a2014afbac (\"atm: fix a UAF in lec_arp_clear_vccs()\")\nReported-by: syzbot+1316233c4c6803382a8b@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/netdev/68556f59.a00a0220.137b3.004e.GAE@google.com/T/#u\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Cong Wang <xiyou.wangcong@gmail.com>\nCc: Gengming Liu <l.dmxcsnsbh@gmail.com>\nReviewed-by: Simon Horman <horms@kernel.org>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
    "author": "Eric Dumazet",
    "date": "2025-06-22T19:31:14+01:00",
    "files_changed": [
      "net/atm/clip.c"
    ],
    "diff": "diff --git a/net/atm/clip.c b/net/atm/clip.c\nindex 61b5b700817d..b234dc3bcb0d 100644\n--- a/net/atm/clip.c\n+++ b/net/atm/clip.c\n@@ -193,12 +193,6 @@ static void clip_push(struct atm_vcc *vcc, struct sk_buff *skb)\n \n \tpr_debug(\"\\n\");\n \n-\tif (!clip_devs) {\n-\t\tatm_return(vcc, skb->truesize);\n-\t\tkfree_skb(skb);\n-\t\treturn;\n-\t}\n-\n \tif (!skb) {\n \t\tpr_debug(\"removing VCC %p\\n\", clip_vcc);\n \t\tif (clip_vcc->entry)\n@@ -208,6 +202,11 @@ static void clip_push(struct atm_vcc *vcc, struct sk_buff *skb)\n \t\treturn;\n \t}\n \tatm_return(vcc, skb->truesize);\n+\tif (!clip_devs) {\n+\t\tkfree_skb(skb);\n+\t\treturn;\n+\t}\n+\n \tskb->dev = clip_vcc->entry ? clip_vcc->entry->neigh->dev : clip_devs;\n \t/* clip_vcc->entry == NULL if we don't have an IP address yet */\n \tif (!skb->dev) {",
    "stats": {
      "insertions": 5,
      "deletions": 6,
      "files": 1
    }
  },
  {
    "sha": "5c00eca95a9a20e662bd290c3ef3f2e07dfa9baa",
    "message": "Merge tag 'x86_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip\n\nPull x86 fixes from Borislav Petkov:\n\n - Make sure the array tracking which kernel text positions need to be\n   alternatives-patched doesn't get mishandled by out-of-order\n   modifications, leading to it overflowing and causing page faults when\n   patching\n\n - Avoid an infinite loop when early code does a ranged TLB invalidation\n   before the broadcast TLB invalidation count of how many pages it can\n   flush, has been read from CPUID\n\n - Fix a CONFIG_MODULES typo\n\n - Disable broadcast TLB invalidation when PTI is enabled to avoid an\n   overflow of the bitmap tracking dynamic ASIDs which need to be\n   flushed when the kernel switches between the user and kernel address\n   space\n\n - Handle the case of a CPU going offline and thus reporting zeroes when\n   reading top-level events in the resctrl code\n\n* tag 'x86_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:\n  x86/alternatives: Fix int3 handling failure from broken text_poke array\n  x86/mm: Fix early boot use of INVPLGB\n  x86/its: Fix an ifdef typo in its_alloc()\n  x86/mm: Disable INVLPGB when PTI is enabled\n  x86,fs/resctrl: Remove inappropriate references to cacheinfo in the resctrl subsystem",
    "author": "Linus Torvalds",
    "date": "2025-06-22T10:30:44-07:00",
    "files_changed": [
      "arch/x86/kernel/alternative.c",
      "arch/x86/kernel/cpu/amd.c",
      "arch/x86/kernel/cpu/resctrl/core.c",
      "arch/x86/mm/pti.c",
      "fs/resctrl/ctrlmondata.c",
      "fs/resctrl/internal.h",
      "fs/resctrl/monitor.c",
      "fs/resctrl/rdtgroup.c",
      "include/linux/resctrl.h"
    ],
    "diff": "diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c\nindex 6455f7f751b3..ea1d984166cd 100644\n--- a/arch/x86/kernel/alternative.c\n+++ b/arch/x86/kernel/alternative.c\n@@ -228,7 +228,7 @@ static void *its_alloc(void)\n \tstruct its_array *pages = &its_pages;\n \tvoid *page;\n \n-#ifdef CONFIG_MODULE\n+#ifdef CONFIG_MODULES\n \tif (its_mod)\n \t\tpages = &its_mod->arch.its_pages;\n #endif\n@@ -3138,6 +3138,6 @@ void __ref smp_text_poke_batch_add(void *addr, const void *opcode, size_t len, c\n  */\n void __ref smp_text_poke_single(void *addr, const void *opcode, size_t len, const void *emulate)\n {\n-\t__smp_text_poke_batch_add(addr, opcode, len, emulate);\n+\tsmp_text_poke_batch_add(addr, opcode, len, emulate);\n \tsmp_text_poke_batch_finish();\n }\ndiff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c\nindex 93da466dfe2c..b2ad8d13211a 100644\n--- a/arch/x86/kernel/cpu/amd.c\n+++ b/arch/x86/kernel/cpu/amd.c\n@@ -31,7 +31,7 @@\n \n #include \"cpu.h\"\n \n-u16 invlpgb_count_max __ro_after_init;\n+u16 invlpgb_count_max __ro_after_init = 1;\n \n static inline int rdmsrq_amd_safe(unsigned msr, u64 *p)\n {\ndiff --git a/arch/x86/kernel/cpu/resctrl/core.c b/arch/x86/kernel/cpu/resctrl/core.c\nindex 7109cbfcad4f..187d527ef73b 100644\n--- a/arch/x86/kernel/cpu/resctrl/core.c\n+++ b/arch/x86/kernel/cpu/resctrl/core.c\n@@ -498,6 +498,7 @@ static void domain_add_cpu_mon(int cpu, struct rdt_resource *r)\n \tstruct rdt_hw_mon_domain *hw_dom;\n \tstruct rdt_domain_hdr *hdr;\n \tstruct rdt_mon_domain *d;\n+\tstruct cacheinfo *ci;\n \tint err;\n \n \tlockdep_assert_held(&domain_list_lock);\n@@ -525,12 +526,13 @@ static void domain_add_cpu_mon(int cpu, struct rdt_resource *r)\n \td = &hw_dom->d_resctrl;\n \td->hdr.id = id;\n \td->hdr.type = RESCTRL_MON_DOMAIN;\n-\td->ci = get_cpu_cacheinfo_level(cpu, RESCTRL_L3_CACHE);\n-\tif (!d->ci) {\n+\tci = get_cpu_cacheinfo_level(cpu, RESCTRL_L3_CACHE);\n+\tif (!ci) {\n \t\tpr_warn_once(\"Can't find L3 cache for CPU:%d resource %s\\n\", cpu, r->name);\n \t\tmon_domain_free(hw_dom);\n \t\treturn;\n \t}\n+\td->ci_id = ci->id;\n \tcpumask_set_cpu(cpu, &d->hdr.cpu_mask);\n \n \tarch_mon_domain_online(r, d);\ndiff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c\nindex 190299834011..c0c40b67524e 100644\n--- a/arch/x86/mm/pti.c\n+++ b/arch/x86/mm/pti.c\n@@ -98,6 +98,11 @@ void __init pti_check_boottime_disable(void)\n \t\treturn;\n \n \tsetup_force_cpu_cap(X86_FEATURE_PTI);\n+\n+\tif (cpu_feature_enabled(X86_FEATURE_INVLPGB)) {\n+\t\tpr_debug(\"PTI enabled, disabling INVLPGB\\n\");\n+\t\tsetup_clear_cpu_cap(X86_FEATURE_INVLPGB);\n+\t}\n }\n \n static int __init pti_parse_cmdline(char *arg)\ndiff --git a/fs/resctrl/ctrlmondata.c b/fs/resctrl/ctrlmondata.c\nindex 6ed2dfd4dbbd..d98e0d2de09f 100644\n--- a/fs/resctrl/ctrlmondata.c\n+++ b/fs/resctrl/ctrlmondata.c\n@@ -594,9 +594,10 @@ int rdtgroup_mondata_show(struct seq_file *m, void *arg)\n \tstruct rmid_read rr = {0};\n \tstruct rdt_mon_domain *d;\n \tstruct rdtgroup *rdtgrp;\n+\tint domid, cpu, ret = 0;\n \tstruct rdt_resource *r;\n+\tstruct cacheinfo *ci;\n \tstruct mon_data *md;\n-\tint domid, ret = 0;\n \n \trdtgrp = rdtgroup_kn_lock_live(of->kn);\n \tif (!rdtgrp) {\n@@ -623,10 +624,14 @@ int rdtgroup_mondata_show(struct seq_file *m, void *arg)\n \t\t * one that matches this cache id.\n \t\t */\n \t\tlist_for_each_entry(d, &r->mon_domains, hdr.list) {\n-\t\t\tif (d->ci->id == domid) {\n-\t\t\t\trr.ci = d->ci;\n+\t\t\tif (d->ci_id == domid) {\n+\t\t\t\trr.ci_id = d->ci_id;\n+\t\t\t\tcpu = cpumask_any(&d->hdr.cpu_mask);\n+\t\t\t\tci = get_cpu_cacheinfo_level(cpu, RESCTRL_L3_CACHE);\n+\t\t\t\tif (!ci)\n+\t\t\t\t\tcontinue;\n \t\t\t\tmon_event_read(&rr, r, NULL, rdtgrp,\n-\t\t\t\t\t       &d->ci->shared_cpu_map, evtid, false);\n+\t\t\t\t\t       &ci->shared_cpu_map, evtid, false);\n \t\t\t\tgoto checkresult;\n \t\t\t}\n \t\t}\ndiff --git a/fs/resctrl/internal.h b/fs/resctrl/internal.h\nindex 9a8cf6f11151..0a1eedba2b03 100644\n--- a/fs/resctrl/internal.h\n+++ b/fs/resctrl/internal.h\n@@ -98,7 +98,7 @@ struct mon_data {\n  *\t   domains in @r sharing L3 @ci.id\n  * @evtid: Which monitor event to read.\n  * @first: Initialize MBM counter when true.\n- * @ci:    Cacheinfo for L3. Only set when @d is NULL. Used when summing domains.\n+ * @ci_id: Cacheinfo id for L3. Only set when @d is NULL. Used when summing domains.\n  * @err:   Error encountered when reading counter.\n  * @val:   Returned value of event counter. If @rgrp is a parent resource group,\n  *\t   @val includes the sum of event counts from its child resource groups.\n@@ -112,7 +112,7 @@ struct rmid_read {\n \tstruct rdt_mon_domain\t*d;\n \tenum resctrl_event_id\tevtid;\n \tbool\t\t\tfirst;\n-\tstruct cacheinfo\t*ci;\n+\tunsigned int\t\tci_id;\n \tint\t\t\terr;\n \tu64\t\t\tval;\n \tvoid\t\t\t*arch_mon_ctx;\ndiff --git a/fs/resctrl/monitor.c b/fs/resctrl/monitor.c\nindex bde2801289d3..f5637855c3ac 100644\n--- a/fs/resctrl/monitor.c\n+++ b/fs/resctrl/monitor.c\n@@ -361,6 +361,7 @@ static int __mon_event_count(u32 closid, u32 rmid, struct rmid_read *rr)\n {\n \tint cpu = smp_processor_id();\n \tstruct rdt_mon_domain *d;\n+\tstruct cacheinfo *ci;\n \tstruct mbm_state *m;\n \tint err, ret;\n \tu64 tval = 0;\n@@ -388,7 +389,8 @@ static int __mon_event_count(u32 closid, u32 rmid, struct rmid_read *rr)\n \t}\n \n \t/* Summing domains that share a cache, must be on a CPU for that cache. */\n-\tif (!cpumask_test_cpu(cpu, &rr->ci->shared_cpu_map))\n+\tci = get_cpu_cacheinfo_level(cpu, RESCTRL_L3_CACHE);\n+\tif (!ci || ci->id != rr->ci_id)\n \t\treturn -EINVAL;\n \n \t/*\n@@ -400,7 +402,7 @@ static int __mon_event_count(u32 closid, u32 rmid, struct rmid_read *rr)\n \t */\n \tret = -EINVAL;\n \tlist_for_each_entry(d, &rr->r->mon_domains, hdr.list) {\n-\t\tif (d->ci->id != rr->ci->id)\n+\t\tif (d->ci_id != rr->ci_id)\n \t\t\tcontinue;\n \t\terr = resctrl_arch_rmid_read(rr->r, d, closid, rmid,\n \t\t\t\t\t     rr->evtid, &tval, rr->arch_mon_ctx);\ndiff --git a/fs/resctrl/rdtgroup.c b/fs/resctrl/rdtgroup.c\nindex 1beb124e25f6..77d08229d855 100644\n--- a/fs/resctrl/rdtgroup.c\n+++ b/fs/resctrl/rdtgroup.c\n@@ -3036,7 +3036,7 @@ static void rmdir_mondata_subdir_allrdtgrp(struct rdt_resource *r,\n \tchar name[32];\n \n \tsnc_mode = r->mon_scope == RESCTRL_L3_NODE;\n-\tsprintf(name, \"mon_%s_%02d\", r->name, snc_mode ? d->ci->id : d->hdr.id);\n+\tsprintf(name, \"mon_%s_%02d\", r->name, snc_mode ? d->ci_id : d->hdr.id);\n \tif (snc_mode)\n \t\tsprintf(subname, \"mon_sub_%s_%02d\", r->name, d->hdr.id);\n \n@@ -3061,7 +3061,7 @@ static int mon_add_all_files(struct kernfs_node *kn, struct rdt_mon_domain *d,\n \t\treturn -EPERM;\n \n \tlist_for_each_entry(mevt, &r->evt_list, list) {\n-\t\tdomid = do_sum ? d->ci->id : d->hdr.id;\n+\t\tdomid = do_sum ? d->ci_id : d->hdr.id;\n \t\tpriv = mon_get_kn_priv(r->rid, domid, mevt, do_sum);\n \t\tif (WARN_ON_ONCE(!priv))\n \t\t\treturn -EINVAL;\n@@ -3089,7 +3089,7 @@ static int mkdir_mondata_subdir(struct kernfs_node *parent_kn,\n \tlockdep_assert_held(&rdtgroup_mutex);\n \n \tsnc_mode = r->mon_scope == RESCTRL_L3_NODE;\n-\tsprintf(name, \"mon_%s_%02d\", r->name, snc_mode ? d->ci->id : d->hdr.id);\n+\tsprintf(name, \"mon_%s_%02d\", r->name, snc_mode ? d->ci_id : d->hdr.id);\n \tkn = kernfs_find_and_get(parent_kn, name);\n \tif (kn) {\n \t\t/*\ndiff --git a/include/linux/resctrl.h b/include/linux/resctrl.h\nindex 9ba771f2ddea..6fb4894b8cfd 100644\n--- a/include/linux/resctrl.h\n+++ b/include/linux/resctrl.h\n@@ -159,7 +159,7 @@ struct rdt_ctrl_domain {\n /**\n  * struct rdt_mon_domain - group of CPUs sharing a resctrl monitor resource\n  * @hdr:\t\tcommon header for different domain types\n- * @ci:\t\t\tcache info for this domain\n+ * @ci_id:\t\tcache info id for this domain\n  * @rmid_busy_llc:\tbitmap of which limbo RMIDs are above threshold\n  * @mbm_total:\t\tsaved state for MBM total bandwidth\n  * @mbm_local:\t\tsaved state for MBM local bandwidth\n@@ -170,7 +170,7 @@ struct rdt_ctrl_domain {\n  */\n struct rdt_mon_domain {\n \tstruct rdt_domain_hdr\t\thdr;\n-\tstruct cacheinfo\t\t*ci;\n+\tunsigned int\t\t\tci_id;\n \tunsigned long\t\t\t*rmid_busy_llc;\n \tstruct mbm_state\t\t*mbm_total;\n \tstruct mbm_state\t\t*mbm_local;",
    "stats": {
      "insertions": 32,
      "deletions": 18,
      "files": 9
    }
  },
  {
    "sha": "17ef32ae66b1afc9fa6dbea40eb18a13edba9c31",
    "message": "Merge tag 'perf_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip\n\nPull perf fixes from Borislav Petkov:\n\n - Avoid a crash on a heterogeneous machine where not all cores support\n   the same hw events features\n\n - Avoid a deadlock when throttling events\n\n - Document the perf event states more\n\n - Make sure a number of perf paths switching off or rescheduling events\n   call perf_cgroup_event_disable()\n\n - Make sure perf does task sampling before its userspace mapping is\n   torn down, and not after\n\n* tag 'perf_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:\n  perf/x86/intel: Fix crash in icl_update_topdown_event()\n  perf: Fix the throttle error of some clock events\n  perf: Add comment to enum perf_event_state\n  perf/core: Fix WARN in perf_cgroup_switch()\n  perf: Fix dangling cgroup pointer in cpuctx\n  perf: Fix cgroup state vs ERROR\n  perf: Fix sample vs do_exit()",
    "author": "Linus Torvalds",
    "date": "2025-06-22T10:11:45-07:00",
    "files_changed": [
      "arch/x86/events/intel/core.c",
      "include/linux/perf_event.h",
      "kernel/events/core.c",
      "kernel/exit.c"
    ],
    "diff": "diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c\nindex 741b229f0718..c2fb729c270e 100644\n--- a/arch/x86/events/intel/core.c\n+++ b/arch/x86/events/intel/core.c\n@@ -2826,7 +2826,7 @@ static void intel_pmu_read_event(struct perf_event *event)\n \t\t * If the PEBS counters snapshotting is enabled,\n \t\t * the topdown event is available in PEBS records.\n \t\t */\n-\t\tif (is_topdown_event(event) && !is_pebs_counter_event_group(event))\n+\t\tif (is_topdown_count(event) && !is_pebs_counter_event_group(event))\n \t\t\tstatic_call(intel_pmu_update_topdown_event)(event, NULL);\n \t\telse\n \t\t\tintel_pmu_drain_pebs_buffer();\ndiff --git a/include/linux/perf_event.h b/include/linux/perf_event.h\nindex 52dc7cfab0e0..ec9d96025683 100644\n--- a/include/linux/perf_event.h\n+++ b/include/linux/perf_event.h\n@@ -635,8 +635,46 @@ struct perf_addr_filter_range {\n \tunsigned long\t\t\tsize;\n };\n \n-/**\n- * enum perf_event_state - the states of an event:\n+/*\n+ * The normal states are:\n+ *\n+ *            ACTIVE    --.\n+ *               ^        |\n+ *               |        |\n+ *       sched_{in,out}() |\n+ *               |        |\n+ *               v        |\n+ *      ,---> INACTIVE  --+ <-.\n+ *      |                 |   |\n+ *      |                {dis,en}able()\n+ *   sched_in()           |   |\n+ *      |       OFF    <--' --+\n+ *      |                     |\n+ *      `--->  ERROR    ------'\n+ *\n+ * That is:\n+ *\n+ * sched_in:       INACTIVE          -> {ACTIVE,ERROR}\n+ * sched_out:      ACTIVE            -> INACTIVE\n+ * disable:        {ACTIVE,INACTIVE} -> OFF\n+ * enable:         {OFF,ERROR}       -> INACTIVE\n+ *\n+ * Where {OFF,ERROR} are disabled states.\n+ *\n+ * Then we have the {EXIT,REVOKED,DEAD} states which are various shades of\n+ * defunct events:\n+ *\n+ *  - EXIT means task that the even was assigned to died, but child events\n+ *    still live, and further children can still be created. But the event\n+ *    itself will never be active again. It can only transition to\n+ *    {REVOKED,DEAD};\n+ *\n+ *  - REVOKED means the PMU the event was associated with is gone; all\n+ *    functionality is stopped but the event is still alive. Can only\n+ *    transition to DEAD;\n+ *\n+ *  - DEAD event really is DYING tearing down state and freeing bits.\n+ *\n  */\n enum perf_event_state {\n \tPERF_EVENT_STATE_DEAD\t\t= -5,\ndiff --git a/kernel/events/core.c b/kernel/events/core.c\nindex f34c99f8ce8f..1f746469fda5 100644\n--- a/kernel/events/core.c\n+++ b/kernel/events/core.c\n@@ -207,6 +207,19 @@ static void perf_ctx_unlock(struct perf_cpu_context *cpuctx,\n \t__perf_ctx_unlock(&cpuctx->ctx);\n }\n \n+typedef struct {\n+\tstruct perf_cpu_context *cpuctx;\n+\tstruct perf_event_context *ctx;\n+} class_perf_ctx_lock_t;\n+\n+static inline void class_perf_ctx_lock_destructor(class_perf_ctx_lock_t *_T)\n+{ perf_ctx_unlock(_T->cpuctx, _T->ctx); }\n+\n+static inline class_perf_ctx_lock_t\n+class_perf_ctx_lock_constructor(struct perf_cpu_context *cpuctx,\n+\t\t\t\tstruct perf_event_context *ctx)\n+{ perf_ctx_lock(cpuctx, ctx); return (class_perf_ctx_lock_t){ cpuctx, ctx }; }\n+\n #define TASK_TOMBSTONE ((void *)-1L)\n \n static bool is_kernel_event(struct perf_event *event)\n@@ -944,7 +957,13 @@ static void perf_cgroup_switch(struct task_struct *task)\n \tif (READ_ONCE(cpuctx->cgrp) == cgrp)\n \t\treturn;\n \n-\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n+\tguard(perf_ctx_lock)(cpuctx, cpuctx->task_ctx);\n+\t/*\n+\t * Re-check, could've raced vs perf_remove_from_context().\n+\t */\n+\tif (READ_ONCE(cpuctx->cgrp) == NULL)\n+\t\treturn;\n+\n \tperf_ctx_disable(&cpuctx->ctx, true);\n \n \tctx_sched_out(&cpuctx->ctx, NULL, EVENT_ALL|EVENT_CGROUP);\n@@ -962,7 +981,6 @@ static void perf_cgroup_switch(struct task_struct *task)\n \tctx_sched_in(&cpuctx->ctx, NULL, EVENT_ALL|EVENT_CGROUP);\n \n \tperf_ctx_enable(&cpuctx->ctx, true);\n-\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n }\n \n static int perf_cgroup_ensure_storage(struct perf_event *event,\n@@ -2120,18 +2138,6 @@ list_del_event(struct perf_event *event, struct perf_event_context *ctx)\n \tif (event->group_leader == event)\n \t\tdel_event_from_groups(event, ctx);\n \n-\t/*\n-\t * If event was in error state, then keep it\n-\t * that way, otherwise bogus counts will be\n-\t * returned on read(). The only way to get out\n-\t * of error state is by explicit re-enabling\n-\t * of the event\n-\t */\n-\tif (event->state > PERF_EVENT_STATE_OFF) {\n-\t\tperf_cgroup_event_disable(event, ctx);\n-\t\tperf_event_set_state(event, PERF_EVENT_STATE_OFF);\n-\t}\n-\n \tctx->generation++;\n \tevent->pmu_ctx->nr_events--;\n }\n@@ -2149,8 +2155,9 @@ perf_aux_output_match(struct perf_event *event, struct perf_event *aux_event)\n }\n \n static void put_event(struct perf_event *event);\n-static void event_sched_out(struct perf_event *event,\n-\t\t\t    struct perf_event_context *ctx);\n+static void __event_disable(struct perf_event *event,\n+\t\t\t    struct perf_event_context *ctx,\n+\t\t\t    enum perf_event_state state);\n \n static void perf_put_aux_event(struct perf_event *event)\n {\n@@ -2183,8 +2190,7 @@ static void perf_put_aux_event(struct perf_event *event)\n \t\t * state so that we don't try to schedule it again. Note\n \t\t * that perf_event_enable() will clear the ERROR status.\n \t\t */\n-\t\tevent_sched_out(iter, ctx);\n-\t\tperf_event_set_state(event, PERF_EVENT_STATE_ERROR);\n+\t\t__event_disable(iter, ctx, PERF_EVENT_STATE_ERROR);\n \t}\n }\n \n@@ -2242,18 +2248,6 @@ static inline struct list_head *get_event_list(struct perf_event *event)\n \t\t\t\t    &event->pmu_ctx->flexible_active;\n }\n \n-/*\n- * Events that have PERF_EV_CAP_SIBLING require being part of a group and\n- * cannot exist on their own, schedule them out and move them into the ERROR\n- * state. Also see _perf_event_enable(), it will not be able to recover\n- * this ERROR state.\n- */\n-static inline void perf_remove_sibling_event(struct perf_event *event)\n-{\n-\tevent_sched_out(event, event->ctx);\n-\tperf_event_set_state(event, PERF_EVENT_STATE_ERROR);\n-}\n-\n static void perf_group_detach(struct perf_event *event)\n {\n \tstruct perf_event *leader = event->group_leader;\n@@ -2289,8 +2283,15 @@ static void perf_group_detach(struct perf_event *event)\n \t */\n \tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n \n+\t\t/*\n+\t\t * Events that have PERF_EV_CAP_SIBLING require being part of\n+\t\t * a group and cannot exist on their own, schedule them out\n+\t\t * and move them into the ERROR state. Also see\n+\t\t * _perf_event_enable(), it will not be able to recover this\n+\t\t * ERROR state.\n+\t\t */\n \t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n-\t\t\tperf_remove_sibling_event(sibling);\n+\t\t\t__event_disable(sibling, ctx, PERF_EVENT_STATE_ERROR);\n \n \t\tsibling->group_leader = sibling;\n \t\tlist_del_init(&sibling->sibling_list);\n@@ -2493,11 +2494,14 @@ __perf_remove_from_context(struct perf_event *event,\n \t\tstate = PERF_EVENT_STATE_EXIT;\n \tif (flags & DETACH_REVOKE)\n \t\tstate = PERF_EVENT_STATE_REVOKED;\n-\tif (flags & DETACH_DEAD) {\n-\t\tevent->pending_disable = 1;\n+\tif (flags & DETACH_DEAD)\n \t\tstate = PERF_EVENT_STATE_DEAD;\n-\t}\n+\n \tevent_sched_out(event, ctx);\n+\n+\tif (event->state > PERF_EVENT_STATE_OFF)\n+\t\tperf_cgroup_event_disable(event, ctx);\n+\n \tperf_event_set_state(event, min(event->state, state));\n \n \tif (flags & DETACH_GROUP)\n@@ -2562,6 +2566,15 @@ static void perf_remove_from_context(struct perf_event *event, unsigned long fla\n \tevent_function_call(event, __perf_remove_from_context, (void *)flags);\n }\n \n+static void __event_disable(struct perf_event *event,\n+\t\t\t    struct perf_event_context *ctx,\n+\t\t\t    enum perf_event_state state)\n+{\n+\tevent_sched_out(event, ctx);\n+\tperf_cgroup_event_disable(event, ctx);\n+\tperf_event_set_state(event, state);\n+}\n+\n /*\n  * Cross CPU call to disable a performance event\n  */\n@@ -2576,13 +2589,18 @@ static void __perf_event_disable(struct perf_event *event,\n \tperf_pmu_disable(event->pmu_ctx->pmu);\n \tctx_time_update_event(ctx, event);\n \n+\t/*\n+\t * When disabling a group leader, the whole group becomes ineligible\n+\t * to run, so schedule out the full group.\n+\t */\n \tif (event == event->group_leader)\n \t\tgroup_sched_out(event, ctx);\n-\telse\n-\t\tevent_sched_out(event, ctx);\n \n-\tperf_event_set_state(event, PERF_EVENT_STATE_OFF);\n-\tperf_cgroup_event_disable(event, ctx);\n+\t/*\n+\t * But only mark the leader OFF; the siblings will remain\n+\t * INACTIVE.\n+\t */\n+\t__event_disable(event, ctx, PERF_EVENT_STATE_OFF);\n \n \tperf_pmu_enable(event->pmu_ctx->pmu);\n }\n@@ -2656,8 +2674,8 @@ static void perf_event_unthrottle(struct perf_event *event, bool start)\n \n static void perf_event_throttle(struct perf_event *event)\n {\n-\tevent->pmu->stop(event, 0);\n \tevent->hw.interrupts = MAX_INTERRUPTS;\n+\tevent->pmu->stop(event, 0);\n \tif (event == event->group_leader)\n \t\tperf_log_throttle(event, 0);\n }\n@@ -7439,6 +7457,10 @@ perf_sample_ustack_size(u16 stack_size, u16 header_size,\n \tif (!regs)\n \t\treturn 0;\n \n+\t/* No mm, no stack, no dump. */\n+\tif (!current->mm)\n+\t\treturn 0;\n+\n \t/*\n \t * Check if we fit in with the requested stack size into the:\n \t * - TASK_SIZE\n@@ -8150,6 +8172,9 @@ perf_callchain(struct perf_event *event, struct pt_regs *regs)\n \tconst u32 max_stack = event->attr.sample_max_stack;\n \tstruct perf_callchain_entry *callchain;\n \n+\tif (!current->mm)\n+\t\tuser = false;\n+\n \tif (!kernel && !user)\n \t\treturn &__empty_callchain;\n \n@@ -11749,7 +11774,12 @@ static void perf_swevent_cancel_hrtimer(struct perf_event *event)\n {\n \tstruct hw_perf_event *hwc = &event->hw;\n \n-\tif (is_sampling_event(event)) {\n+\t/*\n+\t * The throttle can be triggered in the hrtimer handler.\n+\t * The HRTIMER_NORESTART should be used to stop the timer,\n+\t * rather than hrtimer_cancel(). See perf_swevent_hrtimer()\n+\t */\n+\tif (is_sampling_event(event) && (hwc->interrupts != MAX_INTERRUPTS)) {\n \t\tktime_t remaining = hrtimer_get_remaining(&hwc->hrtimer);\n \t\tlocal64_set(&hwc->period_left, ktime_to_ns(remaining));\n \n@@ -11804,7 +11834,8 @@ static void cpu_clock_event_start(struct perf_event *event, int flags)\n static void cpu_clock_event_stop(struct perf_event *event, int flags)\n {\n \tperf_swevent_cancel_hrtimer(event);\n-\tcpu_clock_event_update(event);\n+\tif (flags & PERF_EF_UPDATE)\n+\t\tcpu_clock_event_update(event);\n }\n \n static int cpu_clock_event_add(struct perf_event *event, int flags)\n@@ -11882,7 +11913,8 @@ static void task_clock_event_start(struct perf_event *event, int flags)\n static void task_clock_event_stop(struct perf_event *event, int flags)\n {\n \tperf_swevent_cancel_hrtimer(event);\n-\ttask_clock_event_update(event, event->ctx->time);\n+\tif (flags & PERF_EF_UPDATE)\n+\t\ttask_clock_event_update(event, event->ctx->time);\n }\n \n static int task_clock_event_add(struct perf_event *event, int flags)\ndiff --git a/kernel/exit.c b/kernel/exit.c\nindex bd743900354c..bb184a67ac73 100644\n--- a/kernel/exit.c\n+++ b/kernel/exit.c\n@@ -940,6 +940,15 @@ void __noreturn do_exit(long code)\n \ttaskstats_exit(tsk, group_dead);\n \ttrace_sched_process_exit(tsk, group_dead);\n \n+\t/*\n+\t * Since sampling can touch ->mm, make sure to stop everything before we\n+\t * tear it down.\n+\t *\n+\t * Also flushes inherited counters to the parent - before the parent\n+\t * gets woken up by child-exit notifications.\n+\t */\n+\tperf_event_exit_task(tsk);\n+\n \texit_mm();\n \n \tif (group_dead)\n@@ -955,14 +964,6 @@ void __noreturn do_exit(long code)\n \texit_task_work(tsk);\n \texit_thread(tsk);\n \n-\t/*\n-\t * Flush inherited counters to the parent - before the parent\n-\t * gets woken up by child-exit notifications.\n-\t *\n-\t * because of cgroup mode, must be called before cgroup_exit()\n-\t */\n-\tperf_event_exit_task(tsk);\n-\n \tsched_autogroup_exit_task(tsk);\n \tcgroup_exit(tsk);\n ",
    "stats": {
      "insertions": 124,
      "deletions": 53,
      "files": 4
    }
  },
  {
    "sha": "73543bad766486c3cdbf6fa9d1faf7d0c4bcc7af",
    "message": "Merge tag 'edac_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/ras/ras\n\nPull EDAC fixes from Borislav Petkov:\n\n - amd64: Correct the number of memory controllers on some AMD Zen\n   clients\n\n - igen6: Handle firmware-disabled memory controllers properly\n\n* tag 'edac_urgent_for_v6.16_rc3' of git://git.kernel.org/pub/scm/linux/kernel/git/ras/ras:\n  EDAC/igen6: Fix NULL pointer dereference\n  EDAC/amd64: Correct number of UMCs for family 19h models 70h-7fh",
    "author": "Linus Torvalds",
    "date": "2025-06-22T10:05:33-07:00",
    "files_changed": [
      "drivers/edac/amd64_edac.c",
      "drivers/edac/igen6_edac.c"
    ],
    "diff": "diff --git a/drivers/edac/amd64_edac.c b/drivers/edac/amd64_edac.c\nindex 58b1482a0fbb..b681c0663203 100644\n--- a/drivers/edac/amd64_edac.c\n+++ b/drivers/edac/amd64_edac.c\n@@ -3879,6 +3879,7 @@ static int per_family_init(struct amd64_pvt *pvt)\n \t\t\tbreak;\n \t\tcase 0x70 ... 0x7f:\n \t\t\tpvt->ctl_name\t\t\t= \"F19h_M70h\";\n+\t\t\tpvt->max_mcs\t\t\t= 4;\n \t\t\tpvt->flags.zn_regs_v2\t\t= 1;\n \t\t\tbreak;\n \t\tcase 0x90 ... 0x9f:\ndiff --git a/drivers/edac/igen6_edac.c b/drivers/edac/igen6_edac.c\nindex 1930dc00c791..1cb5c67e78ae 100644\n--- a/drivers/edac/igen6_edac.c\n+++ b/drivers/edac/igen6_edac.c\n@@ -125,7 +125,7 @@\n #define MEM_SLICE_HASH_MASK(v)\t\t(GET_BITFIELD(v, 6, 19) << 6)\n #define MEM_SLICE_HASH_LSB_MASK_BIT(v)\tGET_BITFIELD(v, 24, 26)\n \n-static const struct res_config {\n+static struct res_config {\n \tbool machine_check;\n \t/* The number of present memory controllers. */\n \tint num_imc;\n@@ -479,7 +479,7 @@ static u64 rpl_p_err_addr(u64 ecclog)\n \treturn ECC_ERROR_LOG_ADDR45(ecclog);\n }\n \n-static const struct res_config ehl_cfg = {\n+static struct res_config ehl_cfg = {\n \t.num_imc\t\t= 1,\n \t.imc_base\t\t= 0x5000,\n \t.ibecc_base\t\t= 0xdc00,\n@@ -489,7 +489,7 @@ static const struct res_config ehl_cfg = {\n \t.err_addr_to_imc_addr\t= ehl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config icl_cfg = {\n+static struct res_config icl_cfg = {\n \t.num_imc\t\t= 1,\n \t.imc_base\t\t= 0x5000,\n \t.ibecc_base\t\t= 0xd800,\n@@ -499,7 +499,7 @@ static const struct res_config icl_cfg = {\n \t.err_addr_to_imc_addr\t= ehl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config tgl_cfg = {\n+static struct res_config tgl_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 2,\n \t.imc_base\t\t= 0x5000,\n@@ -513,7 +513,7 @@ static const struct res_config tgl_cfg = {\n \t.err_addr_to_imc_addr\t= tgl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config adl_cfg = {\n+static struct res_config adl_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 2,\n \t.imc_base\t\t= 0xd800,\n@@ -524,7 +524,7 @@ static const struct res_config adl_cfg = {\n \t.err_addr_to_imc_addr\t= adl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config adl_n_cfg = {\n+static struct res_config adl_n_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 1,\n \t.imc_base\t\t= 0xd800,\n@@ -535,7 +535,7 @@ static const struct res_config adl_n_cfg = {\n \t.err_addr_to_imc_addr\t= adl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config rpl_p_cfg = {\n+static struct res_config rpl_p_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 2,\n \t.imc_base\t\t= 0xd800,\n@@ -547,7 +547,7 @@ static const struct res_config rpl_p_cfg = {\n \t.err_addr_to_imc_addr\t= adl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config mtl_ps_cfg = {\n+static struct res_config mtl_ps_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 2,\n \t.imc_base\t\t= 0xd800,\n@@ -558,7 +558,7 @@ static const struct res_config mtl_ps_cfg = {\n \t.err_addr_to_imc_addr\t= adl_err_addr_to_imc_addr,\n };\n \n-static const struct res_config mtl_p_cfg = {\n+static struct res_config mtl_p_cfg = {\n \t.machine_check\t\t= true,\n \t.num_imc\t\t= 2,\n \t.imc_base\t\t= 0xd800,\n@@ -569,7 +569,7 @@ static const struct res_config mtl_p_cfg = {\n \t.err_addr_to_imc_addr\t= adl_err_addr_to_imc_addr,\n };\n \n-static const struct pci_device_id igen6_pci_tbl[] = {\n+static struct pci_device_id igen6_pci_tbl[] = {\n \t{ PCI_VDEVICE(INTEL, DID_EHL_SKU5), (kernel_ulong_t)&ehl_cfg },\n \t{ PCI_VDEVICE(INTEL, DID_EHL_SKU6), (kernel_ulong_t)&ehl_cfg },\n \t{ PCI_VDEVICE(INTEL, DID_EHL_SKU7), (kernel_ulong_t)&ehl_cfg },\n@@ -1350,9 +1350,11 @@ static int igen6_register_mcis(struct pci_dev *pdev, u64 mchbar)\n \t\treturn -ENODEV;\n \t}\n \n-\tif (lmc < res_cfg->num_imc)\n+\tif (lmc < res_cfg->num_imc) {\n \t\tigen6_printk(KERN_WARNING, \"Expected %d mcs, but only %d detected.\",\n \t\t\t     res_cfg->num_imc, lmc);\n+\t\tres_cfg->num_imc = lmc;\n+\t}\n \n \treturn 0;\n ",
    "stats": {
      "insertions": 14,
      "deletions": 11,
      "files": 2
    }
  }
]
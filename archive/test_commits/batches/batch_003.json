[
  {
    "sha": "73d7cf07109e79b093d1a1fb57a88d4048cd9b4b",
    "message": "Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm\n\nPull KVM fixes from Paolo Bonzini:\n \"Many patches, pretty much all of them small, that accumulated while I\n  was on vacation.\n\n  ARM:\n\n   - Remove the last leftovers of the ill-fated FPSIMD host state\n     mapping at EL2 stage-1\n\n   - Fix unexpected advertisement to the guest of unimplemented S2 base\n     granule sizes\n\n   - Gracefully fail initialising pKVM if the interrupt controller isn't\n     GICv3\n\n   - Also gracefully fail initialising pKVM if the carveout allocation\n     fails\n\n   - Fix the computing of the minimum MMIO range required for the host\n     on stage-2 fault\n\n   - Fix the generation of the GICv3 Maintenance Interrupt in nested\n     mode\n\n  x86:\n\n   - Reject SEV{-ES} intra-host migration if one or more vCPUs are\n     actively being created, so as not to create a non-SEV{-ES} vCPU in\n     an SEV{-ES} VM\n\n   - Use a pre-allocated, per-vCPU buffer for handling de-sparsification\n     of vCPU masks in Hyper-V hypercalls; fixes a \"stack frame too\n     large\" issue\n\n   - Allow out-of-range/invalid Xen event channel ports when configuring\n     IRQ routing, to avoid dictating a specific ioctl() ordering to\n     userspace\n\n   - Conditionally reschedule when setting memory attributes to avoid\n     soft lockups when userspace converts huge swaths of memory to/from\n     private\n\n   - Add back MWAIT as a required feature for the MONITOR/MWAIT selftest\n\n   - Add a missing field in struct sev_data_snp_launch_start that\n     resulted in the guest-visible workarounds field being filled at the\n     wrong offset\n\n   - Skip non-canonical address when processing Hyper-V PV TLB flushes\n     to avoid VM-Fail on INVVPID\n\n   - Advertise supported TDX TDVMCALLs to userspace\n\n   - Pass SetupEventNotifyInterrupt arguments to userspace\n\n   - Fix TSC frequency underflow\"\n\n* tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm:\n  KVM: x86: avoid underflow when scaling TSC frequency\n  KVM: arm64: Remove kvm_arch_vcpu_run_map_fp()\n  KVM: arm64: Fix handling of FEAT_GTG for unimplemented granule sizes\n  KVM: arm64: Don't free hyp pages with pKVM on GICv2\n  KVM: arm64: Fix error path in init_hyp_mode()\n  KVM: arm64: Adjust range correctly during host stage-2 faults\n  KVM: arm64: nv: Fix MI line level calculation in vgic_v3_nested_update_mi()\n  KVM: x86/hyper-v: Skip non-canonical addresses during PV TLB flush\n  KVM: SVM: Add missing member in SNP_LAUNCH_START command structure\n  Documentation: KVM: Fix unexpected unindent warnings\n  KVM: selftests: Add back the missing check of MONITOR/MWAIT availability\n  KVM: Allow CPU to reschedule while setting per-page memory attributes\n  KVM: x86/xen: Allow 'out of range' event channel ports in IRQ routing table.\n  KVM: x86/hyper-v: Use preallocated per-vCPU buffer for de-sparsified vCPU masks\n  KVM: SVM: Initialize vmsa_pa in VMCB to INVALID_PAGE if VMSA page is NULL\n  KVM: SVM: Reject SEV{-ES} intra host migration if vCPU creation is in-flight\n  KVM: TDX: Report supported optional TDVMCALLs in TDX capabilities\n  KVM: TDX: Exit to userspace for SetupEventNotifyInterrupt",
    "author": "Linus Torvalds",
    "date": "2025-07-10T09:06:53-07:00",
    "files_changed": [
      "arch/arm64/include/asm/kvm_host.h",
      "arch/arm64/kvm/arm.c",
      "arch/arm64/kvm/fpsimd.c",
      "arch/arm64/kvm/hyp/nvhe/mem_protect.c",
      "arch/arm64/kvm/nested.c",
      "arch/arm64/kvm/vgic/vgic-v3-nested.c",
      "arch/x86/include/asm/kvm_host.h",
      "arch/x86/include/asm/shared/tdx.h",
      "arch/x86/include/uapi/asm/kvm.h",
      "arch/x86/kvm/hyperv.c",
      "arch/x86/kvm/svm/sev.c",
      "arch/x86/kvm/vmx/tdx.c",
      "arch/x86/kvm/x86.c",
      "arch/x86/kvm/xen.c",
      "include/linux/psp-sev.h",
      "include/uapi/linux/kvm.h",
      "tools/testing/selftests/kvm/x86/monitor_mwait_test.c",
      "virt/kvm/kvm_main.c"
    ],
    "diff": "diff --git a/Documentation/virt/kvm/api.rst b/Documentation/virt/kvm/api.rst\nindex 9abf93ee5f65..43ed57e048a8 100644\n--- a/Documentation/virt/kvm/api.rst\n+++ b/Documentation/virt/kvm/api.rst\n@@ -7196,6 +7196,10 @@ The valid value for 'flags' is:\n \t\t\t\t\tu64 leaf;\n \t\t\t\t\tu64 r11, r12, r13, r14;\n \t\t\t\t} get_tdvmcall_info;\n+\t\t\t\tstruct {\n+\t\t\t\t\tu64 ret;\n+\t\t\t\t\tu64 vector;\n+\t\t\t\t} setup_event_notify;\n \t\t\t};\n \t\t} tdx;\n \n@@ -7210,21 +7214,24 @@ number from register R11.  The remaining field of the union provide the\n inputs and outputs of the TDVMCALL.  Currently the following values of\n ``nr`` are defined:\n \n-* ``TDVMCALL_GET_QUOTE``: the guest has requested to generate a TD-Quote\n-signed by a service hosting TD-Quoting Enclave operating on the host.\n-Parameters and return value are in the ``get_quote`` field of the union.\n-The ``gpa`` field and ``size`` specify the guest physical address\n-(without the shared bit set) and the size of a shared-memory buffer, in\n-which the TDX guest passes a TD Report.  The ``ret`` field represents\n-the return value of the GetQuote request.  When the request has been\n-queued successfully, the TDX guest can poll the status field in the\n-shared-memory area to check whether the Quote generation is completed or\n-not. When completed, the generated Quote is returned via the same buffer.\n-\n-* ``TDVMCALL_GET_TD_VM_CALL_INFO``: the guest has requested the support\n-status of TDVMCALLs.  The output values for the given leaf should be\n-placed in fields from ``r11`` to ``r14`` of the ``get_tdvmcall_info``\n-field of the union.\n+ * ``TDVMCALL_GET_QUOTE``: the guest has requested to generate a TD-Quote\n+   signed by a service hosting TD-Quoting Enclave operating on the host.\n+   Parameters and return value are in the ``get_quote`` field of the union.\n+   The ``gpa`` field and ``size`` specify the guest physical address\n+   (without the shared bit set) and the size of a shared-memory buffer, in\n+   which the TDX guest passes a TD Report.  The ``ret`` field represents\n+   the return value of the GetQuote request.  When the request has been\n+   queued successfully, the TDX guest can poll the status field in the\n+   shared-memory area to check whether the Quote generation is completed or\n+   not. When completed, the generated Quote is returned via the same buffer.\n+\n+ * ``TDVMCALL_GET_TD_VM_CALL_INFO``: the guest has requested the support\n+   status of TDVMCALLs.  The output values for the given leaf should be\n+   placed in fields from ``r11`` to ``r14`` of the ``get_tdvmcall_info``\n+   field of the union.\n+\n+* ``TDVMCALL_SETUP_EVENT_NOTIFY_INTERRUPT``: the guest has requested to\n+set up a notification interrupt for vector ``vector``.\n \n KVM may add support for more values in the future that may cause a userspace\n exit, even without calls to ``KVM_ENABLE_CAP`` or similar.  In this case,\ndiff --git a/Documentation/virt/kvm/x86/intel-tdx.rst b/Documentation/virt/kvm/x86/intel-tdx.rst\nindex 76bdd95334d6..5efac62c92c7 100644\n--- a/Documentation/virt/kvm/x86/intel-tdx.rst\n+++ b/Documentation/virt/kvm/x86/intel-tdx.rst\n@@ -79,7 +79,20 @@ to be configured to the TDX guest.\n   struct kvm_tdx_capabilities {\n         __u64 supported_attrs;\n         __u64 supported_xfam;\n-        __u64 reserved[254];\n+\n+        /* TDG.VP.VMCALL hypercalls executed in kernel and forwarded to\n+         * userspace, respectively\n+         */\n+        __u64 kernel_tdvmcallinfo_1_r11;\n+        __u64 user_tdvmcallinfo_1_r11;\n+\n+        /* TDG.VP.VMCALL instruction executions subfunctions executed in kernel\n+         * and forwarded to userspace, respectively\n+         */\n+        __u64 kernel_tdvmcallinfo_1_r12;\n+        __u64 user_tdvmcallinfo_1_r12;\n+\n+        __u64 reserved[250];\n \n         /* Configurable CPUID bits for userspace */\n         struct kvm_cpuid2 cpuid;\ndiff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h\nindex d27079968341..3e41a880b062 100644\n--- a/arch/arm64/include/asm/kvm_host.h\n+++ b/arch/arm64/include/asm/kvm_host.h\n@@ -1480,7 +1480,6 @@ int kvm_vm_ioctl_get_reg_writable_masks(struct kvm *kvm,\n \t\t\t\t\tstruct reg_mask_range *range);\n \n /* Guest/host FPSIMD coordination helpers */\n-int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu);\n void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu);\n void kvm_arch_vcpu_ctxflush_fp(struct kvm_vcpu *vcpu);\n void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu);\ndiff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c\nindex 38a91bb5d4c7..23dd3f3fc3eb 100644\n--- a/arch/arm64/kvm/arm.c\n+++ b/arch/arm64/kvm/arm.c\n@@ -825,10 +825,6 @@ int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)\n \tif (!kvm_arm_vcpu_is_finalized(vcpu))\n \t\treturn -EPERM;\n \n-\tret = kvm_arch_vcpu_run_map_fp(vcpu);\n-\tif (ret)\n-\t\treturn ret;\n-\n \tif (likely(vcpu_has_run_once(vcpu)))\n \t\treturn 0;\n \n@@ -2129,7 +2125,7 @@ static void cpu_hyp_init(void *discard)\n \n static void cpu_hyp_uninit(void *discard)\n {\n-\tif (__this_cpu_read(kvm_hyp_initialized)) {\n+\tif (!is_protected_kvm_enabled() && __this_cpu_read(kvm_hyp_initialized)) {\n \t\tcpu_hyp_reset();\n \t\t__this_cpu_write(kvm_hyp_initialized, 0);\n \t}\n@@ -2345,8 +2341,13 @@ static void __init teardown_hyp_mode(void)\n \n \tfree_hyp_pgds();\n \tfor_each_possible_cpu(cpu) {\n+\t\tif (per_cpu(kvm_hyp_initialized, cpu))\n+\t\t\tcontinue;\n+\n \t\tfree_pages(per_cpu(kvm_arm_hyp_stack_base, cpu), NVHE_STACK_SHIFT - PAGE_SHIFT);\n-\t\tfree_pages(kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu], nvhe_percpu_order());\n+\n+\t\tif (!kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu])\n+\t\t\tcontinue;\n \n \t\tif (free_sve) {\n \t\t\tstruct cpu_sve_state *sve_state;\n@@ -2354,6 +2355,9 @@ static void __init teardown_hyp_mode(void)\n \t\t\tsve_state = per_cpu_ptr_nvhe_sym(kvm_host_data, cpu)->sve_state;\n \t\t\tfree_pages((unsigned long) sve_state, pkvm_host_sve_state_order());\n \t\t}\n+\n+\t\tfree_pages(kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu], nvhe_percpu_order());\n+\n \t}\n }\n \ndiff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c\nindex 8f6c8f57c6b9..15e17aca1dec 100644\n--- a/arch/arm64/kvm/fpsimd.c\n+++ b/arch/arm64/kvm/fpsimd.c\n@@ -14,32 +14,6 @@\n #include <asm/kvm_mmu.h>\n #include <asm/sysreg.h>\n \n-/*\n- * Called on entry to KVM_RUN unless this vcpu previously ran at least\n- * once and the most recent prior KVM_RUN for this vcpu was called from\n- * the same task as current (highly likely).\n- *\n- * This is guaranteed to execute before kvm_arch_vcpu_load_fp(vcpu),\n- * such that on entering hyp the relevant parts of current are already\n- * mapped.\n- */\n-int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu)\n-{\n-\tstruct user_fpsimd_state *fpsimd = &current->thread.uw.fpsimd_state;\n-\tint ret;\n-\n-\t/* pKVM has its own tracking of the host fpsimd state. */\n-\tif (is_protected_kvm_enabled())\n-\t\treturn 0;\n-\n-\t/* Make sure the host task fpsimd state is visible to hyp: */\n-\tret = kvm_share_hyp(fpsimd, fpsimd + 1);\n-\tif (ret)\n-\t\treturn ret;\n-\n-\treturn 0;\n-}\n-\n /*\n  * Prepare vcpu for saving the host's FPSIMD state and loading the guest's.\n  * The actual loading is done by the FPSIMD access trap taken to hyp.\ndiff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c\nindex 95d7534c9679..8957734d6183 100644\n--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c\n+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c\n@@ -479,6 +479,7 @@ static int host_stage2_adjust_range(u64 addr, struct kvm_mem_range *range)\n {\n \tstruct kvm_mem_range cur;\n \tkvm_pte_t pte;\n+\tu64 granule;\n \ts8 level;\n \tint ret;\n \n@@ -496,18 +497,21 @@ static int host_stage2_adjust_range(u64 addr, struct kvm_mem_range *range)\n \t\treturn -EPERM;\n \t}\n \n-\tdo {\n-\t\tu64 granule = kvm_granule_size(level);\n+\tfor (; level <= KVM_PGTABLE_LAST_LEVEL; level++) {\n+\t\tif (!kvm_level_supports_block_mapping(level))\n+\t\t\tcontinue;\n+\t\tgranule = kvm_granule_size(level);\n \t\tcur.start = ALIGN_DOWN(addr, granule);\n \t\tcur.end = cur.start + granule;\n-\t\tlevel++;\n-\t} while ((level <= KVM_PGTABLE_LAST_LEVEL) &&\n-\t\t\t!(kvm_level_supports_block_mapping(level) &&\n-\t\t\t  range_included(&cur, range)));\n+\t\tif (!range_included(&cur, range))\n+\t\t\tcontinue;\n+\t\t*range = cur;\n+\t\treturn 0;\n+\t}\n \n-\t*range = cur;\n+\tWARN_ON(1);\n \n-\treturn 0;\n+\treturn -EINVAL;\n }\n \n int host_stage2_idmap_locked(phys_addr_t addr, u64 size,\ndiff --git a/arch/arm64/kvm/nested.c b/arch/arm64/kvm/nested.c\nindex 5b191f4dc566..dc1d26559bfa 100644\n--- a/arch/arm64/kvm/nested.c\n+++ b/arch/arm64/kvm/nested.c\n@@ -1402,6 +1402,21 @@ static void kvm_map_l1_vncr(struct kvm_vcpu *vcpu)\n \t}\n }\n \n+#define has_tgran_2(__r, __sz)\t\t\t\t\t\t\\\n+\t({\t\t\t\t\t\t\t\t\\\n+\t\tu64 _s1, _s2, _mmfr0 = __r;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\t_s2 = SYS_FIELD_GET(ID_AA64MMFR0_EL1,\t\t\t\\\n+\t\t\t\t    TGRAN##__sz##_2, _mmfr0);\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\t_s1 = SYS_FIELD_GET(ID_AA64MMFR0_EL1,\t\t\t\\\n+\t\t\t\t    TGRAN##__sz, _mmfr0);\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\t((_s2 != ID_AA64MMFR0_EL1_TGRAN##__sz##_2_NI &&\t\t\\\n+\t\t  _s2 != ID_AA64MMFR0_EL1_TGRAN##__sz##_2_TGRAN##__sz) || \\\n+\t\t (_s2 == ID_AA64MMFR0_EL1_TGRAN##__sz##_2_TGRAN##__sz && \\\n+\t\t  _s1 != ID_AA64MMFR0_EL1_TGRAN##__sz##_NI));\t\t\\\n+\t})\n /*\n  * Our emulated CPU doesn't support all the possible features. For the\n  * sake of simplicity (and probably mental sanity), wipe out a number\n@@ -1411,6 +1426,8 @@ static void kvm_map_l1_vncr(struct kvm_vcpu *vcpu)\n  */\n u64 limit_nv_id_reg(struct kvm *kvm, u32 reg, u64 val)\n {\n+\tu64 orig_val = val;\n+\n \tswitch (reg) {\n \tcase SYS_ID_AA64ISAR0_EL1:\n \t\t/* Support everything but TME */\n@@ -1480,13 +1497,16 @@ u64 limit_nv_id_reg(struct kvm *kvm, u32 reg, u64 val)\n \t\t */\n \t\tswitch (PAGE_SIZE) {\n \t\tcase SZ_4K:\n-\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN4_2, IMP);\n+\t\t\tif (has_tgran_2(orig_val, 4))\n+\t\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN4_2, IMP);\n \t\t\tfallthrough;\n \t\tcase SZ_16K:\n-\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN16_2, IMP);\n+\t\t\tif (has_tgran_2(orig_val, 16))\n+\t\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN16_2, IMP);\n \t\t\tfallthrough;\n \t\tcase SZ_64K:\n-\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN64_2, IMP);\n+\t\t\tif (has_tgran_2(orig_val, 64))\n+\t\t\t\tval |= SYS_FIELD_PREP_ENUM(ID_AA64MMFR0_EL1, TGRAN64_2, IMP);\n \t\t\tbreak;\n \t\t}\n \ndiff --git a/arch/arm64/kvm/vgic/vgic-v3-nested.c b/arch/arm64/kvm/vgic/vgic-v3-nested.c\nindex a50fb7e6841f..679aafe77de2 100644\n--- a/arch/arm64/kvm/vgic/vgic-v3-nested.c\n+++ b/arch/arm64/kvm/vgic/vgic-v3-nested.c\n@@ -401,9 +401,7 @@ void vgic_v3_nested_update_mi(struct kvm_vcpu *vcpu)\n {\n \tbool level;\n \n-\tlevel  = __vcpu_sys_reg(vcpu, ICH_HCR_EL2) & ICH_HCR_EL2_En;\n-\tif (level)\n-\t\tlevel &= vgic_v3_get_misr(vcpu);\n+\tlevel = (__vcpu_sys_reg(vcpu, ICH_HCR_EL2) & ICH_HCR_EL2_En) && vgic_v3_get_misr(vcpu);\n \tkvm_vgic_inject_irq(vcpu->kvm, vcpu,\n \t\t\t    vcpu->kvm->arch.vgic.mi_intid, level, vcpu);\n }\ndiff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h\nindex fb01e456b624..f7af967aa16f 100644\n--- a/arch/x86/include/asm/kvm_host.h\n+++ b/arch/x86/include/asm/kvm_host.h\n@@ -700,8 +700,13 @@ struct kvm_vcpu_hv {\n \n \tstruct kvm_vcpu_hv_tlb_flush_fifo tlb_flush_fifo[HV_NR_TLB_FLUSH_FIFOS];\n \n-\t/* Preallocated buffer for handling hypercalls passing sparse vCPU set */\n+\t/*\n+\t * Preallocated buffers for handling hypercalls that pass sparse vCPU\n+\t * sets (for high vCPU counts, they're too large to comfortably fit on\n+\t * the stack).\n+\t */\n \tu64 sparse_banks[HV_MAX_SPARSE_VCPU_BANKS];\n+\tDECLARE_BITMAP(vcpu_mask, KVM_MAX_VCPUS);\n \n \tstruct hv_vp_assist_page vp_assist_page;\n \ndiff --git a/arch/x86/include/asm/shared/tdx.h b/arch/x86/include/asm/shared/tdx.h\nindex d8525e6ef50a..8bc074c8d7c6 100644\n--- a/arch/x86/include/asm/shared/tdx.h\n+++ b/arch/x86/include/asm/shared/tdx.h\n@@ -72,6 +72,7 @@\n #define TDVMCALL_MAP_GPA\t\t0x10001\n #define TDVMCALL_GET_QUOTE\t\t0x10002\n #define TDVMCALL_REPORT_FATAL_ERROR\t0x10003\n+#define TDVMCALL_SETUP_EVENT_NOTIFY_INTERRUPT 0x10004ULL\n \n /*\n  * TDG.VP.VMCALL Status Codes (returned in R10)\ndiff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h\nindex 6f3499507c5e..0f15d683817d 100644\n--- a/arch/x86/include/uapi/asm/kvm.h\n+++ b/arch/x86/include/uapi/asm/kvm.h\n@@ -965,7 +965,13 @@ struct kvm_tdx_cmd {\n struct kvm_tdx_capabilities {\n \t__u64 supported_attrs;\n \t__u64 supported_xfam;\n-\t__u64 reserved[254];\n+\n+\t__u64 kernel_tdvmcallinfo_1_r11;\n+\t__u64 user_tdvmcallinfo_1_r11;\n+\t__u64 kernel_tdvmcallinfo_1_r12;\n+\t__u64 user_tdvmcallinfo_1_r12;\n+\n+\t__u64 reserved[250];\n \n \t/* Configurable CPUID bits for userspace */\n \tstruct kvm_cpuid2 cpuid;\ndiff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c\nindex 24f0318c50d7..ee27064dd72f 100644\n--- a/arch/x86/kvm/hyperv.c\n+++ b/arch/x86/kvm/hyperv.c\n@@ -1979,6 +1979,9 @@ int kvm_hv_vcpu_flush_tlb(struct kvm_vcpu *vcpu)\n \t\tif (entries[i] == KVM_HV_TLB_FLUSHALL_ENTRY)\n \t\t\tgoto out_flush_all;\n \n+\t\tif (is_noncanonical_invlpg_address(entries[i], vcpu))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Lower 12 bits of 'address' encode the number of additional\n \t\t * pages to flush.\n@@ -2001,11 +2004,11 @@ int kvm_hv_vcpu_flush_tlb(struct kvm_vcpu *vcpu)\n static u64 kvm_hv_flush_tlb(struct kvm_vcpu *vcpu, struct kvm_hv_hcall *hc)\n {\n \tstruct kvm_vcpu_hv *hv_vcpu = to_hv_vcpu(vcpu);\n+\tunsigned long *vcpu_mask = hv_vcpu->vcpu_mask;\n \tu64 *sparse_banks = hv_vcpu->sparse_banks;\n \tstruct kvm *kvm = vcpu->kvm;\n \tstruct hv_tlb_flush_ex flush_ex;\n \tstruct hv_tlb_flush flush;\n-\tDECLARE_BITMAP(vcpu_mask, KVM_MAX_VCPUS);\n \tstruct kvm_vcpu_hv_tlb_flush_fifo *tlb_flush_fifo;\n \t/*\n \t * Normally, there can be no more than 'KVM_HV_TLB_FLUSH_FIFO_SIZE'\ndiff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c\nindex 459c3b791fd4..b201f77fcd49 100644\n--- a/arch/x86/kvm/svm/sev.c\n+++ b/arch/x86/kvm/svm/sev.c\n@@ -1971,6 +1971,10 @@ static int sev_check_source_vcpus(struct kvm *dst, struct kvm *src)\n \tstruct kvm_vcpu *src_vcpu;\n \tunsigned long i;\n \n+\tif (src->created_vcpus != atomic_read(&src->online_vcpus) ||\n+\t    dst->created_vcpus != atomic_read(&dst->online_vcpus))\n+\t\treturn -EBUSY;\n+\n \tif (!sev_es_guest(src))\n \t\treturn 0;\n \n@@ -4445,8 +4449,12 @@ static void sev_es_init_vmcb(struct vcpu_svm *svm)\n \t * the VMSA will be NULL if this vCPU is the destination for intrahost\n \t * migration, and will be copied later.\n \t */\n-\tif (svm->sev_es.vmsa && !svm->sev_es.snp_has_guest_vmsa)\n-\t\tsvm->vmcb->control.vmsa_pa = __pa(svm->sev_es.vmsa);\n+\tif (!svm->sev_es.snp_has_guest_vmsa) {\n+\t\tif (svm->sev_es.vmsa)\n+\t\t\tsvm->vmcb->control.vmsa_pa = __pa(svm->sev_es.vmsa);\n+\t\telse\n+\t\t\tsvm->vmcb->control.vmsa_pa = INVALID_PAGE;\n+\t}\n \n \tif (cpu_feature_enabled(X86_FEATURE_ALLOWED_SEV_FEATURES))\n \t\tsvm->vmcb->control.allowed_sev_features = sev->vmsa_features |\ndiff --git a/arch/x86/kvm/vmx/tdx.c b/arch/x86/kvm/vmx/tdx.c\nindex 1ad20c273f3b..f31ccdeb905b 100644\n--- a/arch/x86/kvm/vmx/tdx.c\n+++ b/arch/x86/kvm/vmx/tdx.c\n@@ -173,6 +173,9 @@ static void td_init_cpuid_entry2(struct kvm_cpuid_entry2 *entry, unsigned char i\n \ttdx_clear_unsupported_cpuid(entry);\n }\n \n+#define TDVMCALLINFO_GET_QUOTE\t\t\t\tBIT(0)\n+#define TDVMCALLINFO_SETUP_EVENT_NOTIFY_INTERRUPT\tBIT(1)\n+\n static int init_kvm_tdx_caps(const struct tdx_sys_info_td_conf *td_conf,\n \t\t\t     struct kvm_tdx_capabilities *caps)\n {\n@@ -188,6 +191,10 @@ static int init_kvm_tdx_caps(const struct tdx_sys_info_td_conf *td_conf,\n \n \tcaps->cpuid.nent = td_conf->num_cpuid_config;\n \n+\tcaps->user_tdvmcallinfo_1_r11 =\n+\t\tTDVMCALLINFO_GET_QUOTE |\n+\t\tTDVMCALLINFO_SETUP_EVENT_NOTIFY_INTERRUPT;\n+\n \tfor (i = 0; i < td_conf->num_cpuid_config; i++)\n \t\ttd_init_cpuid_entry2(&caps->cpuid.entries[i], i);\n \n@@ -1530,6 +1537,27 @@ static int tdx_get_quote(struct kvm_vcpu *vcpu)\n \treturn 0;\n }\n \n+static int tdx_setup_event_notify_interrupt(struct kvm_vcpu *vcpu)\n+{\n+\tstruct vcpu_tdx *tdx = to_tdx(vcpu);\n+\tu64 vector = tdx->vp_enter_args.r12;\n+\n+\tif (vector < 32 || vector > 255) {\n+\t\ttdvmcall_set_return_code(vcpu, TDVMCALL_STATUS_INVALID_OPERAND);\n+\t\treturn 1;\n+\t}\n+\n+\tvcpu->run->exit_reason = KVM_EXIT_TDX;\n+\tvcpu->run->tdx.flags = 0;\n+\tvcpu->run->tdx.nr = TDVMCALL_SETUP_EVENT_NOTIFY_INTERRUPT;\n+\tvcpu->run->tdx.setup_event_notify.ret = TDVMCALL_STATUS_SUBFUNC_UNSUPPORTED;\n+\tvcpu->run->tdx.setup_event_notify.vector = vector;\n+\n+\tvcpu->arch.complete_userspace_io = tdx_complete_simple;\n+\n+\treturn 0;\n+}\n+\n static int handle_tdvmcall(struct kvm_vcpu *vcpu)\n {\n \tswitch (tdvmcall_leaf(vcpu)) {\n@@ -1541,6 +1569,8 @@ static int handle_tdvmcall(struct kvm_vcpu *vcpu)\n \t\treturn tdx_get_td_vm_call_info(vcpu);\n \tcase TDVMCALL_GET_QUOTE:\n \t\treturn tdx_get_quote(vcpu);\n+\tcase TDVMCALL_SETUP_EVENT_NOTIFY_INTERRUPT:\n+\t\treturn tdx_setup_event_notify_interrupt(vcpu);\n \tdefault:\n \t\tbreak;\n \t}\ndiff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c\nindex a9d992d5652f..357b9e3a6cef 100644\n--- a/arch/x86/kvm/x86.c\n+++ b/arch/x86/kvm/x86.c\n@@ -3258,9 +3258,11 @@ int kvm_guest_time_update(struct kvm_vcpu *v)\n \n \t/* With all the info we got, fill in the values */\n \n-\tif (kvm_caps.has_tsc_control)\n+\tif (kvm_caps.has_tsc_control) {\n \t\ttgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,\n \t\t\t\t\t    v->arch.l1_tsc_scaling_ratio);\n+\t\ttgt_tsc_khz = tgt_tsc_khz ? : 1;\n+\t}\n \n \tif (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {\n \t\tkvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,\ndiff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c\nindex 9b029bb29a16..5fa2cca43653 100644\n--- a/arch/x86/kvm/xen.c\n+++ b/arch/x86/kvm/xen.c\n@@ -1971,8 +1971,19 @@ int kvm_xen_setup_evtchn(struct kvm *kvm,\n {\n \tstruct kvm_vcpu *vcpu;\n \n-\tif (ue->u.xen_evtchn.port >= max_evtchn_port(kvm))\n-\t\treturn -EINVAL;\n+\t/*\n+\t * Don't check for the port being within range of max_evtchn_port().\n+\t * Userspace can configure what ever targets it likes; events just won't\n+\t * be delivered if/while the target is invalid, just like userspace can\n+\t * configure MSIs which target non-existent APICs.\n+\t *\n+\t * This allow on Live Migration and Live Update, the IRQ routing table\n+\t * can be restored *independently* of other things like creating vCPUs,\n+\t * without imposing an ordering dependency on userspace.  In this\n+\t * particular case, the problematic ordering would be with setting the\n+\t * Xen 'long mode' flag, which changes max_evtchn_port() to allow 4096\n+\t * instead of 1024 event channels.\n+\t */\n \n \t/* We only support 2 level event channels for now */\n \tif (ue->u.xen_evtchn.priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL)\ndiff --git a/include/linux/psp-sev.h b/include/linux/psp-sev.h\nindex 0b3a36bdaa90..0f5f94137f6d 100644\n--- a/include/linux/psp-sev.h\n+++ b/include/linux/psp-sev.h\n@@ -594,6 +594,7 @@ struct sev_data_snp_addr {\n  * @imi_en: launch flow is launching an IMI (Incoming Migration Image) for the\n  *          purpose of guest-assisted migration.\n  * @rsvd: reserved\n+ * @desired_tsc_khz: hypervisor desired mean TSC freq in kHz of the guest\n  * @gosvw: guest OS-visible workarounds, as defined by hypervisor\n  */\n struct sev_data_snp_launch_start {\n@@ -603,6 +604,7 @@ struct sev_data_snp_launch_start {\n \tu32 ma_en:1;\t\t\t\t/* In */\n \tu32 imi_en:1;\t\t\t\t/* In */\n \tu32 rsvd:30;\n+\tu32 desired_tsc_khz;\t\t\t/* In */\n \tu8 gosvw[16];\t\t\t\t/* In */\n } __packed;\n \ndiff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h\nindex 37891580d05d..7a4c35ff03fe 100644\n--- a/include/uapi/linux/kvm.h\n+++ b/include/uapi/linux/kvm.h\n@@ -467,6 +467,10 @@ struct kvm_run {\n \t\t\t\t\t__u64 leaf;\n \t\t\t\t\t__u64 r11, r12, r13, r14;\n \t\t\t\t} get_tdvmcall_info;\n+\t\t\t\tstruct {\n+\t\t\t\t\t__u64 ret;\n+\t\t\t\t\t__u64 vector;\n+\t\t\t\t} setup_event_notify;\n \t\t\t};\n \t\t} tdx;\n \t\t/* Fix the size of the union. */\ndiff --git a/tools/testing/selftests/kvm/x86/monitor_mwait_test.c b/tools/testing/selftests/kvm/x86/monitor_mwait_test.c\nindex 390ae2d87493..0eb371c62ab8 100644\n--- a/tools/testing/selftests/kvm/x86/monitor_mwait_test.c\n+++ b/tools/testing/selftests/kvm/x86/monitor_mwait_test.c\n@@ -74,6 +74,7 @@ int main(int argc, char *argv[])\n \tint testcase;\n \tchar test[80];\n \n+\tTEST_REQUIRE(this_cpu_has(X86_FEATURE_MWAIT));\n \tTEST_REQUIRE(kvm_has_cap(KVM_CAP_DISABLE_QUIRKS2));\n \n \tksft_print_header();\ndiff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c\nindex eec82775c5bf..222f0e894a0c 100644\n--- a/virt/kvm/kvm_main.c\n+++ b/virt/kvm/kvm_main.c\n@@ -2572,6 +2572,8 @@ static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,\n \t\tr = xa_reserve(&kvm->mem_attr_array, i, GFP_KERNEL_ACCOUNT);\n \t\tif (r)\n \t\t\tgoto out_unlock;\n+\n+\t\tcond_resched();\n \t}\n \n \tkvm_handle_gfn_range(kvm, &pre_set_range);\n@@ -2580,6 +2582,7 @@ static int kvm_vm_set_mem_attributes(struct kvm *kvm, gfn_t start, gfn_t end,\n \t\tr = xa_err(xa_store(&kvm->mem_attr_array, i, entry,\n \t\t\t\t    GFP_KERNEL_ACCOUNT));\n \t\tKVM_BUG_ON(r, kvm);\n+\t\tcond_resched();\n \t}\n \n \tkvm_handle_gfn_range(kvm, &post_set_range);",
    "stats": {
      "insertions": 166,
      "deletions": 71,
      "files": 20
    }
  },
  {
    "sha": "d7a54d02db41f72f0581a3c77c75b0993ed3f6e2",
    "message": "wifi: mac80211: always initialize sdata::key_list\n\nThis is currently not initialized for a virtual monitor, leading to a\nNULL pointer dereference when - for example - iterating over all the\nkeys of all the vifs.\n\nReviewed-by: Johannes Berg <johannes.berg@intel.com>\nSigned-off-by: Miri Korenblit <miriam.rachel.korenblit@intel.com>\nLink: https://patch.msgid.link/20250709233400.8dcefe578497.I4c90a00ae3256520e063199d7f6f2580d5451acf@changeid\nSigned-off-by: Johannes Berg <johannes.berg@intel.com>",
    "author": "Miri Korenblit",
    "date": "2025-07-10T13:26:13+02:00",
    "files_changed": [
      "net/mac80211/iface.c"
    ],
    "diff": "diff --git a/net/mac80211/iface.c b/net/mac80211/iface.c\nindex 7c27f3cd841c..c01634fdba78 100644\n--- a/net/mac80211/iface.c\n+++ b/net/mac80211/iface.c\n@@ -1150,6 +1150,8 @@ static void ieee80211_sdata_init(struct ieee80211_local *local,\n {\n \tsdata->local = local;\n \n+\tINIT_LIST_HEAD(&sdata->key_list);\n+\n \t/*\n \t * Initialize the default link, so we can use link_id 0 for non-MLD,\n \t * and that continues to work for non-MLD-aware drivers that use just\n@@ -2210,8 +2212,6 @@ int ieee80211_if_add(struct ieee80211_local *local, const char *name,\n \n \tieee80211_init_frag_cache(&sdata->frags);\n \n-\tINIT_LIST_HEAD(&sdata->key_list);\n-\n \twiphy_delayed_work_init(&sdata->dec_tailroom_needed_wk,\n \t\t\t\tieee80211_delayed_tailroom_dec);\n ",
    "stats": {
      "insertions": 2,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "db6cc3f4ac2e6cdc898fc9cbc8b32ae1bf56bdad",
    "message": "Revert \"sched/numa: add statistics of numa balance task\"\n\nThis reverts commit ad6b26b6a0a79166b53209df2ca1cf8636296382.\n\nThis commit introduces per-memcg/task NUMA balance statistics, but\nunfortunately it introduced a NULL pointer exception due to the following\nrace condition: After a swap task candidate was chosen, its mm_struct\npointer was set to NULL due to task exit.  Later, when performing the\nactual task swapping, the p->mm caused the problem.\n\nCPU0                                   CPU1\n:\n...\ntask_numa_migrate\n     task_numa_find_cpu\n      task_numa_compare\n        # a normal task p is chosen\n        env->best_task = p\n\n                                          # p exit:\n                                          exit_signals(p);\n                                             p->flags |= PF_EXITING\n                                          exit_mm\n                                             p->mm = NULL;\n\n      migrate_swap_stop\n        __migrate_swap_task((arg->src_task, arg->dst_cpu)\n         count_memcg_event_mm(p->mm, NUMA_TASK_SWAP)# p->mm is NULL\n\ntask_lock() should be held and the PF_EXITING flag needs to be checked to\nprevent this from happening.  After discussion, the conclusion was that\nadding a lock is not worthwhile for some statistics calculations.  Revert\nthe change and rely on the tracepoint for this purpose.\n\nLink: https://lkml.kernel.org/r/20250704135620.685752-1-yu.c.chen@intel.com\nLink: https://lkml.kernel.org/r/20250708064917.BBD13C4CEED@smtp.kernel.org\nFixes: ad6b26b6a0a7 (\"sched/numa: add statistics of numa balance task\")\nSigned-off-by: Chen Yu <yu.c.chen@intel.com>\nReported-by: Jirka Hladky <jhladky@redhat.com>\nCloses: https://lore.kernel.org/all/CAE4VaGBLJxpd=NeRJXpSCuw=REhC5LWJpC29kDy-Zh2ZDyzQZA@mail.gmail.com/\nReported-by: Srikanth Aithal <Srikanth.Aithal@amd.com>\nReported-by: Suneeth D <Suneeth.D@amd.com>\nAcked-by: Michal Hocko <mhocko@suse.com>\nCc: Borislav Petkov <bp@alien8.de>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Jiri Hladky <jhladky@redhat.com>\nCc: Libo Chen <libo.chen@oracle.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Chen Yu",
    "date": "2025-07-09T21:07:56-07:00",
    "files_changed": [
      "include/linux/sched.h",
      "include/linux/vm_event_item.h",
      "kernel/sched/core.c",
      "kernel/sched/debug.c",
      "mm/memcontrol.c",
      "mm/vmstat.c"
    ],
    "diff": "diff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 0cc35a14afbe..bd98ea3175ec 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1732,12 +1732,6 @@ The following nested keys are defined.\n \t  numa_hint_faults (npn)\n \t\tNumber of NUMA hinting faults.\n \n-\t  numa_task_migrated (npn)\n-\t\tNumber of task migration by NUMA balancing.\n-\n-\t  numa_task_swapped (npn)\n-\t\tNumber of task swap by NUMA balancing.\n-\n \t  pgdemote_kswapd\n \t\tNumber of pages demoted by kswapd.\n \ndiff --git a/include/linux/sched.h b/include/linux/sched.h\nindex 4f78a64beb52..aa9c5be7a632 100644\n--- a/include/linux/sched.h\n+++ b/include/linux/sched.h\n@@ -548,10 +548,6 @@ struct sched_statistics {\n \tu64\t\t\t\tnr_failed_migrations_running;\n \tu64\t\t\t\tnr_failed_migrations_hot;\n \tu64\t\t\t\tnr_forced_migrations;\n-#ifdef CONFIG_NUMA_BALANCING\n-\tu64\t\t\t\tnuma_task_migrated;\n-\tu64\t\t\t\tnuma_task_swapped;\n-#endif\n \n \tu64\t\t\t\tnr_wakeups;\n \tu64\t\t\t\tnr_wakeups_sync;\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 91a3ce9a2687..9e15a088ba38 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -66,8 +66,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tNUMA_HINT_FAULTS,\n \t\tNUMA_HINT_FAULTS_LOCAL,\n \t\tNUMA_PAGE_MIGRATE,\n-\t\tNUMA_TASK_MIGRATE,\n-\t\tNUMA_TASK_SWAP,\n #endif\n #ifdef CONFIG_MIGRATION\n \t\tPGMIGRATE_SUCCESS, PGMIGRATE_FAIL,\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex ec68fc686bd7..81c6df746df1 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -3362,10 +3362,6 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n #ifdef CONFIG_NUMA_BALANCING\n static void __migrate_swap_task(struct task_struct *p, int cpu)\n {\n-\t__schedstat_inc(p->stats.numa_task_swapped);\n-\tcount_vm_numa_event(NUMA_TASK_SWAP);\n-\tcount_memcg_event_mm(p->mm, NUMA_TASK_SWAP);\n-\n \tif (task_on_rq_queued(p)) {\n \t\tstruct rq *src_rq, *dst_rq;\n \t\tstruct rq_flags srf, drf;\n@@ -7939,9 +7935,8 @@ int migrate_task_to(struct task_struct *p, int target_cpu)\n \tif (!cpumask_test_cpu(target_cpu, p->cpus_ptr))\n \t\treturn -EINVAL;\n \n-\t__schedstat_inc(p->stats.numa_task_migrated);\n-\tcount_vm_numa_event(NUMA_TASK_MIGRATE);\n-\tcount_memcg_event_mm(p->mm, NUMA_TASK_MIGRATE);\n+\t/* TODO: This is not properly updating schedstats */\n+\n \ttrace_sched_move_numa(p, curr_cpu, target_cpu);\n \treturn stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);\n }\ndiff --git a/kernel/sched/debug.c b/kernel/sched/debug.c\nindex 9d71baf08075..557246880a7e 100644\n--- a/kernel/sched/debug.c\n+++ b/kernel/sched/debug.c\n@@ -1210,10 +1210,6 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,\n \t\tP_SCHEDSTAT(nr_failed_migrations_running);\n \t\tP_SCHEDSTAT(nr_failed_migrations_hot);\n \t\tP_SCHEDSTAT(nr_forced_migrations);\n-#ifdef CONFIG_NUMA_BALANCING\n-\t\tP_SCHEDSTAT(numa_task_migrated);\n-\t\tP_SCHEDSTAT(numa_task_swapped);\n-#endif\n \t\tP_SCHEDSTAT(nr_wakeups);\n \t\tP_SCHEDSTAT(nr_wakeups_sync);\n \t\tP_SCHEDSTAT(nr_wakeups_migrate);\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 902da8a9c643..70fdeda1120b 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -474,8 +474,6 @@ static const unsigned int memcg_vm_event_stat[] = {\n \tNUMA_PAGE_MIGRATE,\n \tNUMA_PTE_UPDATES,\n \tNUMA_HINT_FAULTS,\n-\tNUMA_TASK_MIGRATE,\n-\tNUMA_TASK_SWAP,\n #endif\n };\n \ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 429ae5339bfe..a78d70ddeacd 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1346,8 +1346,6 @@ const char * const vmstat_text[] = {\n \t\"numa_hint_faults\",\n \t\"numa_hint_faults_local\",\n \t\"numa_pages_migrated\",\n-\t\"numa_task_migrated\",\n-\t\"numa_task_swapped\",\n #endif\n #ifdef CONFIG_MIGRATION\n \t\"pgmigrate_success\",",
    "stats": {
      "insertions": 2,
      "deletions": 27,
      "files": 7
    }
  },
  {
    "sha": "82241a83cd15aaaf28200a40ad1a8b480012edaf",
    "message": "mm: fix the inaccurate memory statistics issue for users\n\nOn some large machines with a high number of CPUs running a 64K pagesize\nkernel, we found that the 'RES' field is always 0 displayed by the top\ncommand for some processes, which will cause a lot of confusion for users.\n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n 875525 root      20   0   12480      0      0 R   0.3   0.0   0:00.08 top\n      1 root      20   0  172800      0      0 S   0.0   0.0   0:04.52 systemd\n\nThe main reason is that the batch size of the percpu counter is quite\nlarge on these machines, caching a significant percpu value, since\nconverting mm's rss stats into percpu_counter by commit f1a7941243c1 (\"mm:\nconvert mm's rss stats into percpu_counter\").  Intuitively, the batch\nnumber should be optimized, but on some paths, performance may take\nprecedence over statistical accuracy.  Therefore, introducing a new\ninterface to add the percpu statistical count and display it to users,\nwhich can remove the confusion.  In addition, this change is not expected\nto be on a performance-critical path, so the modification should be\nacceptable.\n\nIn addition, the 'mm->rss_stat' is updated by using add_mm_counter() and\ndec/inc_mm_counter(), which are all wrappers around\npercpu_counter_add_batch().  In percpu_counter_add_batch(), there is\npercpu batch caching to avoid 'fbc->lock' contention.  This patch changes\ntask_mem() and task_statm() to get the accurate mm counters under the\n'fbc->lock', but this should not exacerbate kernel 'mm->rss_stat' lock\ncontention due to the percpu batch caching of the mm counters.  The\nfollowing test also confirm the theoretical analysis.\n\nI run the stress-ng that stresses anon page faults in 32 threads on my 32\ncores machine, while simultaneously running a script that starts 32\nthreads to busy-loop pread each stress-ng thread's /proc/pid/status\ninterface.  From the following data, I did not observe any obvious impact\nof this patch on the stress-ng tests.\n\nw/o patch:\nstress-ng: info:  [6848]          4,399,219,085,152 CPU Cycles          67.327 B/sec\nstress-ng: info:  [6848]          1,616,524,844,832 Instructions          24.740 B/sec (0.367 instr. per cycle)\nstress-ng: info:  [6848]          39,529,792 Page Faults Total           0.605 M/sec\nstress-ng: info:  [6848]          39,529,792 Page Faults Minor           0.605 M/sec\n\nw/patch:\nstress-ng: info:  [2485]          4,462,440,381,856 CPU Cycles          68.382 B/sec\nstress-ng: info:  [2485]          1,615,101,503,296 Instructions          24.750 B/sec (0.362 instr. per cycle)\nstress-ng: info:  [2485]          39,439,232 Page Faults Total           0.604 M/sec\nstress-ng: info:  [2485]          39,439,232 Page Faults Minor           0.604 M/sec\n\nOn comparing a very simple app which just allocates & touches some\nmemory against v6.1 (which doesn't have f1a7941243c1) and latest Linus\ntree (4c06e63b9203) I can see that on latest Linus tree the values for\nVmRSS, RssAnon and RssFile from /proc/self/status are all zeroes while\nthey do report values on v6.1 and a Linus tree with this patch.\n\nLink: https://lkml.kernel.org/r/f4586b17f66f97c174f7fd1f8647374fdb53de1c.1749119050.git.baolin.wang@linux.alibaba.com\nFixes: f1a7941243c1 (\"mm: convert mm's rss stats into percpu_counter\")\nSigned-off-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Aboorva Devarajan <aboorvad@linux.ibm.com>\nTested-by: Aboorva Devarajan <aboorvad@linux.ibm.com>\nTested-by Donet Tom <donettom@linux.ibm.com>\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\nAcked-by: SeongJae Park <sj@kernel.org>\nAcked-by: Michal Hocko <mhocko@suse.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nCc: David Hildenbrand <david@redhat.com>\nCc: Liam Howlett <liam.howlett@oracle.com>\nCc: Lorenzo Stoakes <lorenzo.stoakes@oracle.com>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Baolin Wang",
    "date": "2025-07-09T21:07:55-07:00",
    "files_changed": [
      "fs/proc/task_mmu.c",
      "include/linux/mm.h"
    ],
    "diff": "diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c\nindex 4be91eb6ea5c..751479eb128f 100644\n--- a/fs/proc/task_mmu.c\n+++ b/fs/proc/task_mmu.c\n@@ -36,9 +36,9 @@ void task_mem(struct seq_file *m, struct mm_struct *mm)\n \tunsigned long text, lib, swap, anon, file, shmem;\n \tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n \n-\tanon = get_mm_counter(mm, MM_ANONPAGES);\n-\tfile = get_mm_counter(mm, MM_FILEPAGES);\n-\tshmem = get_mm_counter(mm, MM_SHMEMPAGES);\n+\tanon = get_mm_counter_sum(mm, MM_ANONPAGES);\n+\tfile = get_mm_counter_sum(mm, MM_FILEPAGES);\n+\tshmem = get_mm_counter_sum(mm, MM_SHMEMPAGES);\n \n \t/*\n \t * Note: to minimize their overhead, mm maintains hiwater_vm and\n@@ -59,7 +59,7 @@ void task_mem(struct seq_file *m, struct mm_struct *mm)\n \ttext = min(text, mm->exec_vm << PAGE_SHIFT);\n \tlib = (mm->exec_vm << PAGE_SHIFT) - text;\n \n-\tswap = get_mm_counter(mm, MM_SWAPENTS);\n+\tswap = get_mm_counter_sum(mm, MM_SWAPENTS);\n \tSEQ_PUT_DEC(\"VmPeak:\\t\", hiwater_vm);\n \tSEQ_PUT_DEC(\" kB\\nVmSize:\\t\", total_vm);\n \tSEQ_PUT_DEC(\" kB\\nVmLck:\\t\", mm->locked_vm);\n@@ -92,12 +92,12 @@ unsigned long task_statm(struct mm_struct *mm,\n \t\t\t unsigned long *shared, unsigned long *text,\n \t\t\t unsigned long *data, unsigned long *resident)\n {\n-\t*shared = get_mm_counter(mm, MM_FILEPAGES) +\n-\t\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n+\t*shared = get_mm_counter_sum(mm, MM_FILEPAGES) +\n+\t\t\tget_mm_counter_sum(mm, MM_SHMEMPAGES);\n \t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n \t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n \t*data = mm->data_vm + mm->stack_vm;\n-\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n+\t*resident = *shared + get_mm_counter_sum(mm, MM_ANONPAGES);\n \treturn mm->total_vm;\n }\n \ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 0ef2ba0c667a..fa538feaa8d9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2568,6 +2568,11 @@ static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n \treturn percpu_counter_read_positive(&mm->rss_stat[member]);\n }\n \n+static inline unsigned long get_mm_counter_sum(struct mm_struct *mm, int member)\n+{\n+\treturn percpu_counter_sum_positive(&mm->rss_stat[member]);\n+}\n+\n void mm_trace_rss_stat(struct mm_struct *mm, int member);\n \n static inline void add_mm_counter(struct mm_struct *mm, int member, long value)",
    "stats": {
      "insertions": 12,
      "deletions": 7,
      "files": 2
    }
  },
  {
    "sha": "d9e01c62b7a0c258a7481c083f84c766a8f5597c",
    "message": "samples/damon: fix damon sample prcl for start failure\n\nPatch series \"mm/damon: fix divide by zero and its samples\", v3.\n\nThis series includes fixes against damon and its samples to make it safer\nwhen damon sample starting fails.\n\nIt includes the following changes.\n- fix unexpected divide by zero crash for zero size regions\n- fix bugs for damon samples in case of start failures\n\n\nThis patch (of 4):\n\nThe damon_sample_prcl_start() can fail so we must reset the \"enable\"\nparameter to \"false\" again for proper rollback.\n\nIn such cases, setting Y to \"enable\" then N triggers the following crash\nbecause damon sample start failed but the \"enable\" stays as Y.\n\n  [ 2441.419649] damon_sample_prcl: start\n  [ 2454.146817] damon_sample_prcl: stop\n  [ 2454.146862] ------------[ cut here ]------------\n  [ 2454.146865] kernel BUG at mm/slub.c:546!\n  [ 2454.148183] Oops: invalid opcode: 0000 [#1] SMP NOPTI\n  \t...\n  [ 2454.167555] Call Trace:\n  [ 2454.167822]  <TASK>\n  [ 2454.168061]  damon_destroy_ctx+0x78/0x140\n  [ 2454.168454]  damon_sample_prcl_enable_store+0x8d/0xd0\n  [ 2454.168932]  param_attr_store+0xa1/0x120\n  [ 2454.169315]  module_attr_store+0x20/0x50\n  [ 2454.169695]  sysfs_kf_write+0x72/0x90\n  [ 2454.170065]  kernfs_fop_write_iter+0x150/0x1e0\n  [ 2454.170491]  vfs_write+0x315/0x440\n  [ 2454.170833]  ksys_write+0x69/0xf0\n  [ 2454.171162]  __x64_sys_write+0x19/0x30\n  [ 2454.171525]  x64_sys_call+0x18b2/0x2700\n  [ 2454.171900]  do_syscall_64+0x7f/0x680\n  [ 2454.172258]  ? exit_to_user_mode_loop+0xf6/0x180\n  [ 2454.172694]  ? clear_bhb_loop+0x30/0x80\n  [ 2454.173067]  ? clear_bhb_loop+0x30/0x80\n  [ 2454.173439]  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n\nLink: https://lkml.kernel.org/r/20250702000205.1921-1-honggyu.kim@sk.com\nLink: https://lkml.kernel.org/r/20250702000205.1921-2-honggyu.kim@sk.com\nFixes: 2aca254620a8 (\"samples/damon: introduce a skeleton of a smaple DAMON module for proactive reclamation\")\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nReviewed-by: SeongJae Park <sj@kernel.org>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Honggyu Kim",
    "date": "2025-07-09T21:07:54-07:00",
    "files_changed": [
      "samples/damon/prcl.c"
    ],
    "diff": "diff --git a/samples/damon/prcl.c b/samples/damon/prcl.c\nindex 056b1b21a0fe..5597e6a08ab2 100644\n--- a/samples/damon/prcl.c\n+++ b/samples/damon/prcl.c\n@@ -122,8 +122,12 @@ static int damon_sample_prcl_enable_store(\n \tif (enable == enabled)\n \t\treturn 0;\n \n-\tif (enable)\n-\t\treturn damon_sample_prcl_start();\n+\tif (enable) {\n+\t\terr = damon_sample_prcl_start();\n+\t\tif (err)\n+\t\t\tenable = false;\n+\t\treturn err;\n+\t}\n \tdamon_sample_prcl_stop();\n \treturn 0;\n }",
    "stats": {
      "insertions": 6,
      "deletions": 2,
      "files": 1
    }
  }
]
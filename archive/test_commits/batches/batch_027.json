[
  {
    "sha": "3a3de75a68ff8d52466980c4cfb2c16192d5e4e7",
    "message": "Merge tag 'loongarch-fixes-6.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/chenhuacai/linux-loongson\n\nPull LoongArch fixes from Huacai Chen:\n\n - replace __ASSEMBLY__ with __ASSEMBLER__ in headers like others\n\n - fix build warnings about export.h\n\n - reserve the EFI memory map region for kdump\n\n - handle __init vs inline mismatches\n\n - fix some KVM bugs\n\n* tag 'loongarch-fixes-6.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/chenhuacai/linux-loongson:\n  LoongArch: KVM: Disable updating of \"num_cpu\" and \"feature\"\n  LoongArch: KVM: Check validity of \"num_cpu\" from user space\n  LoongArch: KVM: Check interrupt route from physical CPU\n  LoongArch: KVM: Fix interrupt route update with EIOINTC\n  LoongArch: KVM: Add address alignment check for IOCSR emulation\n  LoongArch: KVM: Avoid overflow with array index\n  LoongArch: Handle KCOV __init vs inline mismatches\n  LoongArch: Reserve the EFI memory map region\n  LoongArch: Fix build warnings about export.h\n  LoongArch: Replace __ASSEMBLY__ with __ASSEMBLER__ in headers",
    "author": "Linus Torvalds",
    "date": "2025-06-28T11:35:11-07:00",
    "files_changed": [
      "arch/loongarch/include/asm/addrspace.h",
      "arch/loongarch/include/asm/alternative-asm.h",
      "arch/loongarch/include/asm/alternative.h",
      "arch/loongarch/include/asm/asm-extable.h",
      "arch/loongarch/include/asm/asm.h",
      "arch/loongarch/include/asm/cpu.h",
      "arch/loongarch/include/asm/ftrace.h",
      "arch/loongarch/include/asm/gpr-num.h",
      "arch/loongarch/include/asm/irqflags.h",
      "arch/loongarch/include/asm/jump_label.h",
      "arch/loongarch/include/asm/kasan.h",
      "arch/loongarch/include/asm/loongarch.h",
      "arch/loongarch/include/asm/orc_types.h",
      "arch/loongarch/include/asm/page.h",
      "arch/loongarch/include/asm/pgtable-bits.h",
      "arch/loongarch/include/asm/pgtable.h",
      "arch/loongarch/include/asm/prefetch.h",
      "arch/loongarch/include/asm/smp.h",
      "arch/loongarch/include/asm/thread_info.h",
      "arch/loongarch/include/asm/types.h",
      "arch/loongarch/include/asm/unwind_hints.h",
      "arch/loongarch/include/asm/vdso/arch_data.h",
      "arch/loongarch/include/asm/vdso/getrandom.h",
      "arch/loongarch/include/asm/vdso/gettimeofday.h",
      "arch/loongarch/include/asm/vdso/processor.h",
      "arch/loongarch/include/asm/vdso/vdso.h",
      "arch/loongarch/include/asm/vdso/vsyscall.h",
      "arch/loongarch/kernel/acpi.c",
      "arch/loongarch/kernel/alternative.c",
      "arch/loongarch/kernel/efi.c",
      "arch/loongarch/kernel/elf.c",
      "arch/loongarch/kernel/kfpu.c",
      "arch/loongarch/kernel/paravirt.c",
      "arch/loongarch/kernel/time.c",
      "arch/loongarch/kernel/traps.c",
      "arch/loongarch/kernel/unwind_guess.c",
      "arch/loongarch/kernel/unwind_orc.c",
      "arch/loongarch/kernel/unwind_prologue.c",
      "arch/loongarch/kvm/intc/eiointc.c",
      "arch/loongarch/lib/crc32-loongarch.c",
      "arch/loongarch/lib/csum.c",
      "arch/loongarch/mm/ioremap.c",
      "arch/loongarch/pci/pci.c",
      "tools/arch/loongarch/include/asm/orc_types.h"
    ],
    "diff": "diff --git a/arch/loongarch/include/asm/addrspace.h b/arch/loongarch/include/asm/addrspace.h\nindex fe198b473f84..e739dbc6329d 100644\n--- a/arch/loongarch/include/asm/addrspace.h\n+++ b/arch/loongarch/include/asm/addrspace.h\n@@ -18,12 +18,12 @@\n /*\n  * This gives the physical RAM offset.\n  */\n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n #ifndef PHYS_OFFSET\n #define PHYS_OFFSET\t_UL(0)\n #endif\n extern unsigned long vm_map_base;\n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #ifndef IO_BASE\n #define IO_BASE\t\t\tCSR_DMW0_BASE\n@@ -66,7 +66,7 @@ extern unsigned long vm_map_base;\n #define FIXADDR_TOP\t\t((unsigned long)(long)(int)0xfffe0000)\n #endif\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define _ATYPE_\n #define _ATYPE32_\n #define _ATYPE64_\n@@ -85,7 +85,7 @@ extern unsigned long vm_map_base;\n /*\n  *  32/64-bit LoongArch address spaces\n  */\n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define _ACAST32_\n #define _ACAST64_\n #else\ndiff --git a/arch/loongarch/include/asm/alternative-asm.h b/arch/loongarch/include/asm/alternative-asm.h\nindex ff3d10ac393f..7dc29bd9b2f0 100644\n--- a/arch/loongarch/include/asm/alternative-asm.h\n+++ b/arch/loongarch/include/asm/alternative-asm.h\n@@ -2,7 +2,7 @@\n #ifndef _ASM_ALTERNATIVE_ASM_H\n #define _ASM_ALTERNATIVE_ASM_H\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n #include <asm/asm.h>\n \n@@ -77,6 +77,6 @@\n \t.previous\n .endm\n \n-#endif  /*  __ASSEMBLY__  */\n+#endif  /*  __ASSEMBLER__  */\n \n #endif /* _ASM_ALTERNATIVE_ASM_H */\ndiff --git a/arch/loongarch/include/asm/alternative.h b/arch/loongarch/include/asm/alternative.h\nindex cee7b29785ab..b5bae21fb3c8 100644\n--- a/arch/loongarch/include/asm/alternative.h\n+++ b/arch/loongarch/include/asm/alternative.h\n@@ -2,7 +2,7 @@\n #ifndef _ASM_ALTERNATIVE_H\n #define _ASM_ALTERNATIVE_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/types.h>\n #include <linux/stddef.h>\n@@ -106,6 +106,6 @@ extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);\n #define alternative_2(oldinstr, newinstr1, feature1, newinstr2, feature2) \\\n \t(asm volatile(ALTERNATIVE_2(oldinstr, newinstr1, feature1, newinstr2, feature2) ::: \"memory\"))\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* _ASM_ALTERNATIVE_H */\ndiff --git a/arch/loongarch/include/asm/asm-extable.h b/arch/loongarch/include/asm/asm-extable.h\nindex df05005f2b80..d60bdf2e6377 100644\n--- a/arch/loongarch/include/asm/asm-extable.h\n+++ b/arch/loongarch/include/asm/asm-extable.h\n@@ -7,7 +7,7 @@\n #define EX_TYPE_UACCESS_ERR_ZERO\t2\n #define EX_TYPE_BPF\t\t\t3\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n #define __ASM_EXTABLE_RAW(insn, fixup, type, data)\t\\\n \t.pushsection\t__ex_table, \"a\";\t\t\\\n@@ -22,7 +22,7 @@\n \t__ASM_EXTABLE_RAW(\\insn, \\fixup, EX_TYPE_FIXUP, 0)\n \t.endm\n \n-#else /* __ASSEMBLY__ */\n+#else /* __ASSEMBLER__ */\n \n #include <linux/bits.h>\n #include <linux/stringify.h>\n@@ -60,6 +60,6 @@\n #define _ASM_EXTABLE_UACCESS_ERR(insn, fixup, err)\t\t\t\\\n \t_ASM_EXTABLE_UACCESS_ERR_ZERO(insn, fixup, err, zero)\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* __ASM_ASM_EXTABLE_H */\ndiff --git a/arch/loongarch/include/asm/asm.h b/arch/loongarch/include/asm/asm.h\nindex f591b3245def..f018d26fc995 100644\n--- a/arch/loongarch/include/asm/asm.h\n+++ b/arch/loongarch/include/asm/asm.h\n@@ -110,7 +110,7 @@\n #define LONG_SRA\tsrai.w\n #define LONG_SRAV\tsra.w\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define LONG\t\t.word\n #endif\n #define LONGSIZE\t4\n@@ -131,7 +131,7 @@\n #define LONG_SRA\tsrai.d\n #define LONG_SRAV\tsra.d\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define LONG\t\t.dword\n #endif\n #define LONGSIZE\t8\n@@ -158,7 +158,7 @@\n \n #define PTR_SCALESHIFT\t2\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define PTR\t\t.word\n #endif\n #define PTRSIZE\t\t4\n@@ -181,7 +181,7 @@\n \n #define PTR_SCALESHIFT\t3\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define PTR\t\t.dword\n #endif\n #define PTRSIZE\t\t8\ndiff --git a/arch/loongarch/include/asm/cpu.h b/arch/loongarch/include/asm/cpu.h\nindex 98cf4d7b4b0a..dfb982fe8701 100644\n--- a/arch/loongarch/include/asm/cpu.h\n+++ b/arch/loongarch/include/asm/cpu.h\n@@ -46,7 +46,7 @@\n \n #define PRID_PRODUCT_MASK\t0x0fff\n \n-#if !defined(__ASSEMBLY__)\n+#if !defined(__ASSEMBLER__)\n \n enum cpu_type_enum {\n \tCPU_UNKNOWN,\n@@ -55,7 +55,7 @@ enum cpu_type_enum {\n \tCPU_LAST\n };\n \n-#endif /* !__ASSEMBLY */\n+#endif /* !__ASSEMBLER__ */\n \n /*\n  * ISA Level encodings\ndiff --git a/arch/loongarch/include/asm/ftrace.h b/arch/loongarch/include/asm/ftrace.h\nindex 6e0a99763a9a..f4caaf764f9e 100644\n--- a/arch/loongarch/include/asm/ftrace.h\n+++ b/arch/loongarch/include/asm/ftrace.h\n@@ -14,7 +14,7 @@\n \n #define MCOUNT_INSN_SIZE 4\t\t/* sizeof mcount call */\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #ifndef CONFIG_DYNAMIC_FTRACE\n \n@@ -84,7 +84,7 @@ __arch_ftrace_set_direct_caller(struct pt_regs *regs, unsigned long addr)\n \n #endif\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* CONFIG_FUNCTION_TRACER */\n \ndiff --git a/arch/loongarch/include/asm/gpr-num.h b/arch/loongarch/include/asm/gpr-num.h\nindex 996038da806d..af95b941f48b 100644\n--- a/arch/loongarch/include/asm/gpr-num.h\n+++ b/arch/loongarch/include/asm/gpr-num.h\n@@ -2,7 +2,7 @@\n #ifndef __ASM_GPR_NUM_H\n #define __ASM_GPR_NUM_H\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n \t.equ\t.L__gpr_num_zero, 0\n \t.irp\tnum,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31\n@@ -25,7 +25,7 @@\n \t.equ\t.L__gpr_num_$s\\num, 23 + \\num\n \t.endr\n \n-#else /* __ASSEMBLY__ */\n+#else /* __ASSEMBLER__ */\n \n #define __DEFINE_ASM_GPR_NUMS\t\t\t\t\t\\\n \"\t.equ\t.L__gpr_num_zero, 0\\n\"\t\t\t\t\\\n@@ -47,6 +47,6 @@\n \"\t.equ\t.L__gpr_num_$s\\\\num, 23 + \\\\num\\n\"\t\t\\\n \"\t.endr\\n\"\t\t\t\t\t\t\\\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* __ASM_GPR_NUM_H */\ndiff --git a/arch/loongarch/include/asm/irqflags.h b/arch/loongarch/include/asm/irqflags.h\nindex 003172b8406b..620163628a7f 100644\n--- a/arch/loongarch/include/asm/irqflags.h\n+++ b/arch/loongarch/include/asm/irqflags.h\n@@ -5,7 +5,7 @@\n #ifndef _ASM_IRQFLAGS_H\n #define _ASM_IRQFLAGS_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/compiler.h>\n #include <linux/stringify.h>\n@@ -80,6 +80,6 @@ static inline int arch_irqs_disabled(void)\n \treturn arch_irqs_disabled_flags(arch_local_save_flags());\n }\n \n-#endif /* #ifndef __ASSEMBLY__ */\n+#endif /* #ifndef __ASSEMBLER__ */\n \n #endif /* _ASM_IRQFLAGS_H */\ndiff --git a/arch/loongarch/include/asm/jump_label.h b/arch/loongarch/include/asm/jump_label.h\nindex 8a924bd69d19..4000c7603d8e 100644\n--- a/arch/loongarch/include/asm/jump_label.h\n+++ b/arch/loongarch/include/asm/jump_label.h\n@@ -7,7 +7,7 @@\n #ifndef __ASM_JUMP_LABEL_H\n #define __ASM_JUMP_LABEL_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/types.h>\n \n@@ -50,5 +50,5 @@ static __always_inline bool arch_static_branch_jump(struct static_key * const ke\n \treturn true;\n }\n \n-#endif  /* __ASSEMBLY__ */\n+#endif  /* __ASSEMBLER__ */\n #endif\t/* __ASM_JUMP_LABEL_H */\ndiff --git a/arch/loongarch/include/asm/kasan.h b/arch/loongarch/include/asm/kasan.h\nindex 7f52bd31b9d4..62f139a9c87d 100644\n--- a/arch/loongarch/include/asm/kasan.h\n+++ b/arch/loongarch/include/asm/kasan.h\n@@ -2,7 +2,7 @@\n #ifndef __ASM_KASAN_H\n #define __ASM_KASAN_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/linkage.h>\n #include <linux/mmzone.h>\ndiff --git a/arch/loongarch/include/asm/loongarch.h b/arch/loongarch/include/asm/loongarch.h\nindex d84dac88a584..a0994d226eff 100644\n--- a/arch/loongarch/include/asm/loongarch.h\n+++ b/arch/loongarch/include/asm/loongarch.h\n@@ -9,15 +9,15 @@\n #include <linux/linkage.h>\n #include <linux/types.h>\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n #include <larchintrin.h>\n \n /* CPUCFG */\n #define read_cpucfg(reg) __cpucfg(reg)\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n /* LoongArch Registers */\n #define REG_ZERO\t0x0\n@@ -53,7 +53,7 @@\n #define REG_S7\t\t0x1e\n #define REG_S8\t\t0x1f\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n /* Bit fields for CPUCFG registers */\n #define LOONGARCH_CPUCFG0\t\t0x0\n@@ -171,7 +171,7 @@\n  * SW emulation for KVM hypervirsor, see arch/loongarch/include/uapi/asm/kvm_para.h\n  */\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n /* CSR */\n #define csr_read32(reg) __csrrd_w(reg)\n@@ -187,7 +187,7 @@\n #define iocsr_write32(val, reg) __iocsrwr_w(val, reg)\n #define iocsr_write64(val, reg) __iocsrwr_d(val, reg)\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n /* CSR register number */\n \n@@ -1195,7 +1195,7 @@\n #define LOONGARCH_IOCSR_EXTIOI_ROUTE_BASE\t0x1c00\n #define IOCSR_EXTIOI_VECTOR_NUM\t\t\t256\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n static __always_inline u64 drdtime(void)\n {\n@@ -1357,7 +1357,7 @@ __BUILD_CSR_OP(tlbidx)\n #define clear_csr_estat(val)\t\\\n \tcsr_xchg32(~(val), val, LOONGARCH_CSR_ESTAT)\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n /* Generic EntryLo bit definitions */\n #define ENTRYLO_V\t\t(_ULCAST_(1) << 0)\ndiff --git a/arch/loongarch/include/asm/orc_types.h b/arch/loongarch/include/asm/orc_types.h\nindex caf1f71a1057..d5fa98d1d177 100644\n--- a/arch/loongarch/include/asm/orc_types.h\n+++ b/arch/loongarch/include/asm/orc_types.h\n@@ -34,7 +34,7 @@\n #define ORC_TYPE_REGS\t\t\t3\n #define ORC_TYPE_REGS_PARTIAL\t\t4\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n /*\n  * This struct is more or less a vastly simplified version of the DWARF Call\n  * Frame Information standard.  It contains only the necessary parts of DWARF\n@@ -53,6 +53,6 @@ struct orc_entry {\n \tunsigned int\ttype:3;\n \tunsigned int\tsignal:1;\n };\n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* _ORC_TYPES_H */\ndiff --git a/arch/loongarch/include/asm/page.h b/arch/loongarch/include/asm/page.h\nindex 7368f12b7cb1..a3aaf34fba16 100644\n--- a/arch/loongarch/include/asm/page.h\n+++ b/arch/loongarch/include/asm/page.h\n@@ -15,7 +15,7 @@\n #define HPAGE_MASK\t(~(HPAGE_SIZE - 1))\n #define HUGETLB_PAGE_ORDER\t(HPAGE_SHIFT - PAGE_SHIFT)\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/kernel.h>\n #include <linux/pfn.h>\n@@ -110,6 +110,6 @@ extern int __virt_addr_valid(volatile void *kaddr);\n #include <asm-generic/memory_model.h>\n #include <asm-generic/getorder.h>\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* _ASM_PAGE_H */\ndiff --git a/arch/loongarch/include/asm/pgtable-bits.h b/arch/loongarch/include/asm/pgtable-bits.h\nindex 45bfc65a0c9f..7bbfb04a54cc 100644\n--- a/arch/loongarch/include/asm/pgtable-bits.h\n+++ b/arch/loongarch/include/asm/pgtable-bits.h\n@@ -92,7 +92,7 @@\n #define PAGE_KERNEL_WUC __pgprot(_PAGE_PRESENT | __READABLE | __WRITEABLE | \\\n \t\t\t\t _PAGE_GLOBAL | _PAGE_KERN |  _CACHE_WUC)\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #define _PAGE_IOREMAP\t\tpgprot_val(PAGE_KERNEL_SUC)\n \n@@ -127,6 +127,6 @@ static inline pgprot_t pgprot_writecombine(pgprot_t _prot)\n \treturn __pgprot(prot);\n }\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* _ASM_PGTABLE_BITS_H */\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex b30185302c07..f2aeff544cee 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -55,7 +55,7 @@\n \n #define USER_PTRS_PER_PGD       ((TASK_SIZE64 / PGDIR_SIZE)?(TASK_SIZE64 / PGDIR_SIZE):1)\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <linux/mm_types.h>\n #include <linux/mmzone.h>\n@@ -618,6 +618,6 @@ static inline long pmd_protnone(pmd_t pmd)\n #define HAVE_ARCH_UNMAPPED_AREA\n #define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* _ASM_PGTABLE_H */\ndiff --git a/arch/loongarch/include/asm/prefetch.h b/arch/loongarch/include/asm/prefetch.h\nindex 1672262a5e2e..0b168cdaae9a 100644\n--- a/arch/loongarch/include/asm/prefetch.h\n+++ b/arch/loongarch/include/asm/prefetch.h\n@@ -8,7 +8,7 @@\n #define Pref_Load\t0\n #define Pref_Store\t8\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n \t.macro\t__pref hint addr\n #ifdef CONFIG_CPU_HAS_PREFETCH\ndiff --git a/arch/loongarch/include/asm/smp.h b/arch/loongarch/include/asm/smp.h\nindex ad0bd234a0f1..3a47f52959a8 100644\n--- a/arch/loongarch/include/asm/smp.h\n+++ b/arch/loongarch/include/asm/smp.h\n@@ -39,7 +39,7 @@ int loongson_cpu_disable(void);\n void loongson_cpu_die(unsigned int cpu);\n #endif\n \n-static inline void plat_smp_setup(void)\n+static inline void __init plat_smp_setup(void)\n {\n \tloongson_smp_setup();\n }\ndiff --git a/arch/loongarch/include/asm/thread_info.h b/arch/loongarch/include/asm/thread_info.h\nindex 4f5a9441754e..9dfa2ef00816 100644\n--- a/arch/loongarch/include/asm/thread_info.h\n+++ b/arch/loongarch/include/asm/thread_info.h\n@@ -10,7 +10,7 @@\n \n #ifdef __KERNEL__\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <asm/processor.h>\n \n@@ -53,7 +53,7 @@ static inline struct thread_info *current_thread_info(void)\n \n register unsigned long current_stack_pointer __asm__(\"$sp\");\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n /* thread information allocation */\n #define THREAD_SIZE\t\tSZ_16K\ndiff --git a/arch/loongarch/include/asm/types.h b/arch/loongarch/include/asm/types.h\nindex baf15a0dcf8b..0edd731f3d6a 100644\n--- a/arch/loongarch/include/asm/types.h\n+++ b/arch/loongarch/include/asm/types.h\n@@ -8,7 +8,7 @@\n #include <asm-generic/int-ll64.h>\n #include <uapi/asm/types.h>\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n #define _ULCAST_\n #define _U64CAST_\n #else\ndiff --git a/arch/loongarch/include/asm/unwind_hints.h b/arch/loongarch/include/asm/unwind_hints.h\nindex 2c68bc72736c..16c7f7e465a0 100644\n--- a/arch/loongarch/include/asm/unwind_hints.h\n+++ b/arch/loongarch/include/asm/unwind_hints.h\n@@ -5,7 +5,7 @@\n #include <linux/objtool.h>\n #include <asm/orc_types.h>\n \n-#ifdef __ASSEMBLY__\n+#ifdef __ASSEMBLER__\n \n .macro UNWIND_HINT_UNDEFINED\n \tUNWIND_HINT type=UNWIND_HINT_TYPE_UNDEFINED\n@@ -23,7 +23,7 @@\n \tUNWIND_HINT sp_reg=ORC_REG_SP type=UNWIND_HINT_TYPE_CALL\n .endm\n \n-#else /* !__ASSEMBLY__ */\n+#else /* !__ASSEMBLER__ */\n \n #define UNWIND_HINT_SAVE \\\n \tUNWIND_HINT(UNWIND_HINT_TYPE_SAVE, 0, 0, 0)\n@@ -31,6 +31,6 @@\n #define UNWIND_HINT_RESTORE \\\n \tUNWIND_HINT(UNWIND_HINT_TYPE_RESTORE, 0, 0, 0)\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* _ASM_LOONGARCH_UNWIND_HINTS_H */\ndiff --git a/arch/loongarch/include/asm/vdso/arch_data.h b/arch/loongarch/include/asm/vdso/arch_data.h\nindex 322d0a5f1c84..395ec223bcbe 100644\n--- a/arch/loongarch/include/asm/vdso/arch_data.h\n+++ b/arch/loongarch/include/asm/vdso/arch_data.h\n@@ -7,7 +7,7 @@\n #ifndef _VDSO_ARCH_DATA_H\n #define _VDSO_ARCH_DATA_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <asm/asm.h>\n #include <asm/vdso.h>\n@@ -20,6 +20,6 @@ struct vdso_arch_data {\n \tstruct vdso_pcpu_data pdata[NR_CPUS];\n };\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif\ndiff --git a/arch/loongarch/include/asm/vdso/getrandom.h b/arch/loongarch/include/asm/vdso/getrandom.h\nindex a81724b69f29..2ff05003c6e7 100644\n--- a/arch/loongarch/include/asm/vdso/getrandom.h\n+++ b/arch/loongarch/include/asm/vdso/getrandom.h\n@@ -5,7 +5,7 @@\n #ifndef __ASM_VDSO_GETRANDOM_H\n #define __ASM_VDSO_GETRANDOM_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <asm/unistd.h>\n #include <asm/vdso/vdso.h>\n@@ -28,6 +28,6 @@ static __always_inline ssize_t getrandom_syscall(void *_buffer, size_t _len, uns\n \treturn ret;\n }\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* __ASM_VDSO_GETRANDOM_H */\ndiff --git a/arch/loongarch/include/asm/vdso/gettimeofday.h b/arch/loongarch/include/asm/vdso/gettimeofday.h\nindex f15503e3336c..dcafabca9bb6 100644\n--- a/arch/loongarch/include/asm/vdso/gettimeofday.h\n+++ b/arch/loongarch/include/asm/vdso/gettimeofday.h\n@@ -7,7 +7,7 @@\n #ifndef __ASM_VDSO_GETTIMEOFDAY_H\n #define __ASM_VDSO_GETTIMEOFDAY_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <asm/unistd.h>\n #include <asm/vdso/vdso.h>\n@@ -89,6 +89,6 @@ static inline bool loongarch_vdso_hres_capable(void)\n }\n #define __arch_vdso_hres_capable loongarch_vdso_hres_capable\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* __ASM_VDSO_GETTIMEOFDAY_H */\ndiff --git a/arch/loongarch/include/asm/vdso/processor.h b/arch/loongarch/include/asm/vdso/processor.h\nindex ef5770b343a0..1e255373b0b8 100644\n--- a/arch/loongarch/include/asm/vdso/processor.h\n+++ b/arch/loongarch/include/asm/vdso/processor.h\n@@ -5,10 +5,10 @@\n #ifndef __ASM_VDSO_PROCESSOR_H\n #define __ASM_VDSO_PROCESSOR_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #define cpu_relax()\tbarrier()\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* __ASM_VDSO_PROCESSOR_H */\ndiff --git a/arch/loongarch/include/asm/vdso/vdso.h b/arch/loongarch/include/asm/vdso/vdso.h\nindex 50c65fb29daf..04bd2d452876 100644\n--- a/arch/loongarch/include/asm/vdso/vdso.h\n+++ b/arch/loongarch/include/asm/vdso/vdso.h\n@@ -7,7 +7,7 @@\n #ifndef _ASM_VDSO_VDSO_H\n #define _ASM_VDSO_VDSO_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <asm/asm.h>\n #include <asm/page.h>\n@@ -16,6 +16,6 @@\n \n #define VVAR_SIZE (VDSO_NR_PAGES << PAGE_SHIFT)\n \n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif\ndiff --git a/arch/loongarch/include/asm/vdso/vsyscall.h b/arch/loongarch/include/asm/vdso/vsyscall.h\nindex 1140b54b4bc8..558eb9dfda52 100644\n--- a/arch/loongarch/include/asm/vdso/vsyscall.h\n+++ b/arch/loongarch/include/asm/vdso/vsyscall.h\n@@ -2,13 +2,13 @@\n #ifndef __ASM_VDSO_VSYSCALL_H\n #define __ASM_VDSO_VSYSCALL_H\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n \n #include <vdso/datapage.h>\n \n /* The asm-generic header needs to be included after the definitions above */\n #include <asm-generic/vdso/vsyscall.h>\n \n-#endif /* !__ASSEMBLY__ */\n+#endif /* !__ASSEMBLER__ */\n \n #endif /* __ASM_VDSO_VSYSCALL_H */\ndiff --git a/arch/loongarch/kernel/acpi.c b/arch/loongarch/kernel/acpi.c\nindex a54cd6fd3796..1367ca759468 100644\n--- a/arch/loongarch/kernel/acpi.c\n+++ b/arch/loongarch/kernel/acpi.c\n@@ -10,6 +10,7 @@\n #include <linux/init.h>\n #include <linux/acpi.h>\n #include <linux/efi-bgrt.h>\n+#include <linux/export.h>\n #include <linux/irq.h>\n #include <linux/irqdomain.h>\n #include <linux/memblock.h>\ndiff --git a/arch/loongarch/kernel/alternative.c b/arch/loongarch/kernel/alternative.c\nindex 4ad13847e962..0e0c766df1e3 100644\n--- a/arch/loongarch/kernel/alternative.c\n+++ b/arch/loongarch/kernel/alternative.c\n@@ -1,4 +1,5 @@\n // SPDX-License-Identifier: GPL-2.0-only\n+#include <linux/export.h>\n #include <linux/mm.h>\n #include <linux/module.h>\n #include <asm/alternative.h>\ndiff --git a/arch/loongarch/kernel/efi.c b/arch/loongarch/kernel/efi.c\nindex de21e72759ee..860a3bc030e0 100644\n--- a/arch/loongarch/kernel/efi.c\n+++ b/arch/loongarch/kernel/efi.c\n@@ -144,6 +144,18 @@ void __init efi_init(void)\n \t\tif (efi_memmap_init_early(&data) < 0)\n \t\t\tpanic(\"Unable to map EFI memory map.\\n\");\n \n+\t\t/*\n+\t\t * Reserve the physical memory region occupied by the EFI\n+\t\t * memory map table (header + descriptors). This is crucial\n+\t\t * for kdump, as the kdump kernel relies on this original\n+\t\t * memmap passed by the bootloader. Without reservation,\n+\t\t * this region could be overwritten by the primary kernel.\n+\t\t * Also, set the EFI_PRESERVE_BS_REGIONS flag to indicate that\n+\t\t * critical boot services code/data regions like this are preserved.\n+\t\t */\n+\t\tmemblock_reserve((phys_addr_t)boot_memmap, sizeof(*tbl) + data.size);\n+\t\tset_bit(EFI_PRESERVE_BS_REGIONS, &efi.flags);\n+\n \t\tearly_memunmap(tbl, sizeof(*tbl));\n \t}\n \ndiff --git a/arch/loongarch/kernel/elf.c b/arch/loongarch/kernel/elf.c\nindex 0fa81ced28dc..3d98c6aa00db 100644\n--- a/arch/loongarch/kernel/elf.c\n+++ b/arch/loongarch/kernel/elf.c\n@@ -6,7 +6,6 @@\n \n #include <linux/binfmts.h>\n #include <linux/elf.h>\n-#include <linux/export.h>\n #include <linux/sched.h>\n \n #include <asm/cpu-features.h>\ndiff --git a/arch/loongarch/kernel/kfpu.c b/arch/loongarch/kernel/kfpu.c\nindex 4c476904227f..141b49bd989c 100644\n--- a/arch/loongarch/kernel/kfpu.c\n+++ b/arch/loongarch/kernel/kfpu.c\n@@ -4,6 +4,7 @@\n  */\n \n #include <linux/cpu.h>\n+#include <linux/export.h>\n #include <linux/init.h>\n #include <asm/fpu.h>\n #include <asm/smp.h>\ndiff --git a/arch/loongarch/kernel/paravirt.c b/arch/loongarch/kernel/paravirt.c\nindex e5a39bbad078..b1b51f920b23 100644\n--- a/arch/loongarch/kernel/paravirt.c\n+++ b/arch/loongarch/kernel/paravirt.c\n@@ -1,5 +1,4 @@\n // SPDX-License-Identifier: GPL-2.0\n-#include <linux/export.h>\n #include <linux/types.h>\n #include <linux/interrupt.h>\n #include <linux/irq_work.h>\ndiff --git a/arch/loongarch/kernel/time.c b/arch/loongarch/kernel/time.c\nindex bc75a3a69fc8..367906b10f81 100644\n--- a/arch/loongarch/kernel/time.c\n+++ b/arch/loongarch/kernel/time.c\n@@ -102,7 +102,7 @@ static int constant_timer_next_event(unsigned long delta, struct clock_event_dev\n \treturn 0;\n }\n \n-static unsigned long __init get_loops_per_jiffy(void)\n+static unsigned long get_loops_per_jiffy(void)\n {\n \tunsigned long lpj = (unsigned long)const_clock_freq;\n \ndiff --git a/arch/loongarch/kernel/traps.c b/arch/loongarch/kernel/traps.c\nindex 47fc2de6d150..3d9be6ca7ec5 100644\n--- a/arch/loongarch/kernel/traps.c\n+++ b/arch/loongarch/kernel/traps.c\n@@ -13,6 +13,7 @@\n #include <linux/kernel.h>\n #include <linux/kexec.h>\n #include <linux/module.h>\n+#include <linux/export.h>\n #include <linux/extable.h>\n #include <linux/mm.h>\n #include <linux/sched/mm.h>\ndiff --git a/arch/loongarch/kernel/unwind_guess.c b/arch/loongarch/kernel/unwind_guess.c\nindex 98379b7d4147..08d7951b2f60 100644\n--- a/arch/loongarch/kernel/unwind_guess.c\n+++ b/arch/loongarch/kernel/unwind_guess.c\n@@ -3,6 +3,7 @@\n  * Copyright (C) 2022 Loongson Technology Corporation Limited\n  */\n #include <asm/unwind.h>\n+#include <linux/export.h>\n \n unsigned long unwind_get_return_address(struct unwind_state *state)\n {\ndiff --git a/arch/loongarch/kernel/unwind_orc.c b/arch/loongarch/kernel/unwind_orc.c\nindex d623935a7547..0005be49b056 100644\n--- a/arch/loongarch/kernel/unwind_orc.c\n+++ b/arch/loongarch/kernel/unwind_orc.c\n@@ -1,6 +1,7 @@\n // SPDX-License-Identifier: GPL-2.0-only\n-#include <linux/objtool.h>\n+#include <linux/export.h>\n #include <linux/module.h>\n+#include <linux/objtool.h>\n #include <linux/sort.h>\n #include <asm/exception.h>\n #include <asm/orc_header.h>\ndiff --git a/arch/loongarch/kernel/unwind_prologue.c b/arch/loongarch/kernel/unwind_prologue.c\nindex 929ae240280a..729e775bd40d 100644\n--- a/arch/loongarch/kernel/unwind_prologue.c\n+++ b/arch/loongarch/kernel/unwind_prologue.c\n@@ -3,6 +3,7 @@\n  * Copyright (C) 2022 Loongson Technology Corporation Limited\n  */\n #include <linux/cpumask.h>\n+#include <linux/export.h>\n #include <linux/ftrace.h>\n #include <linux/kallsyms.h>\n \ndiff --git a/arch/loongarch/kvm/intc/eiointc.c b/arch/loongarch/kvm/intc/eiointc.c\nindex f39929d7bf8a..a75f865d6fb9 100644\n--- a/arch/loongarch/kvm/intc/eiointc.c\n+++ b/arch/loongarch/kvm/intc/eiointc.c\n@@ -9,7 +9,8 @@\n \n static void eiointc_set_sw_coreisr(struct loongarch_eiointc *s)\n {\n-\tint ipnum, cpu, irq_index, irq_mask, irq;\n+\tint ipnum, cpu, cpuid, irq_index, irq_mask, irq;\n+\tstruct kvm_vcpu *vcpu;\n \n \tfor (irq = 0; irq < EIOINTC_IRQS; irq++) {\n \t\tipnum = s->ipmap.reg_u8[irq / 32];\n@@ -20,7 +21,12 @@ static void eiointc_set_sw_coreisr(struct loongarch_eiointc *s)\n \t\tirq_index = irq / 32;\n \t\tirq_mask = BIT(irq & 0x1f);\n \n-\t\tcpu = s->coremap.reg_u8[irq];\n+\t\tcpuid = s->coremap.reg_u8[irq];\n+\t\tvcpu = kvm_get_vcpu_by_cpuid(s->kvm, cpuid);\n+\t\tif (!vcpu)\n+\t\t\tcontinue;\n+\n+\t\tcpu = vcpu->vcpu_id;\n \t\tif (!!(s->coreisr.reg_u32[cpu][irq_index] & irq_mask))\n \t\t\tset_bit(irq, s->sw_coreisr[cpu][ipnum]);\n \t\telse\n@@ -66,20 +72,25 @@ static void eiointc_update_irq(struct loongarch_eiointc *s, int irq, int level)\n }\n \n static inline void eiointc_update_sw_coremap(struct loongarch_eiointc *s,\n-\t\t\t\t\tint irq, void *pvalue, u32 len, bool notify)\n+\t\t\t\t\tint irq, u64 val, u32 len, bool notify)\n {\n-\tint i, cpu;\n-\tu64 val = *(u64 *)pvalue;\n+\tint i, cpu, cpuid;\n+\tstruct kvm_vcpu *vcpu;\n \n \tfor (i = 0; i < len; i++) {\n-\t\tcpu = val & 0xff;\n+\t\tcpuid = val & 0xff;\n \t\tval = val >> 8;\n \n \t\tif (!(s->status & BIT(EIOINTC_ENABLE_CPU_ENCODE))) {\n-\t\t\tcpu = ffs(cpu) - 1;\n-\t\t\tcpu = (cpu >= 4) ? 0 : cpu;\n+\t\t\tcpuid = ffs(cpuid) - 1;\n+\t\t\tcpuid = (cpuid >= 4) ? 0 : cpuid;\n \t\t}\n \n+\t\tvcpu = kvm_get_vcpu_by_cpuid(s->kvm, cpuid);\n+\t\tif (!vcpu)\n+\t\t\tcontinue;\n+\n+\t\tcpu = vcpu->vcpu_id;\n \t\tif (s->sw_coremap[irq + i] == cpu)\n \t\t\tcontinue;\n \n@@ -305,6 +316,11 @@ static int kvm_eiointc_read(struct kvm_vcpu *vcpu,\n \t\treturn -EINVAL;\n \t}\n \n+\tif (addr & (len - 1)) {\n+\t\tkvm_err(\"%s: eiointc not aligned addr %llx len %d\\n\", __func__, addr, len);\n+\t\treturn -EINVAL;\n+\t}\n+\n \tvcpu->kvm->stat.eiointc_read_exits++;\n \tspin_lock_irqsave(&eiointc->lock, flags);\n \tswitch (len) {\n@@ -398,7 +414,7 @@ static int loongarch_eiointc_writeb(struct kvm_vcpu *vcpu,\n \t\tirq = offset - EIOINTC_COREMAP_START;\n \t\tindex = irq;\n \t\ts->coremap.reg_u8[index] = data;\n-\t\teiointc_update_sw_coremap(s, irq, (void *)&data, sizeof(data), true);\n+\t\teiointc_update_sw_coremap(s, irq, data, sizeof(data), true);\n \t\tbreak;\n \tdefault:\n \t\tret = -EINVAL;\n@@ -436,17 +452,16 @@ static int loongarch_eiointc_writew(struct kvm_vcpu *vcpu,\n \t\tbreak;\n \tcase EIOINTC_ENABLE_START ... EIOINTC_ENABLE_END:\n \t\tindex = (offset - EIOINTC_ENABLE_START) >> 1;\n-\t\told_data = s->enable.reg_u32[index];\n+\t\told_data = s->enable.reg_u16[index];\n \t\ts->enable.reg_u16[index] = data;\n \t\t/*\n \t\t * 1: enable irq.\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u16[index] & ~old_data & s->isr.reg_u16[index];\n-\t\tindex = index << 1;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 2 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -455,7 +470,7 @@ static int loongarch_eiointc_writew(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u16[index] & old_data & s->isr.reg_u16[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 2 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:\n@@ -484,7 +499,7 @@ static int loongarch_eiointc_writew(struct kvm_vcpu *vcpu,\n \t\tirq = offset - EIOINTC_COREMAP_START;\n \t\tindex = irq >> 1;\n \t\ts->coremap.reg_u16[index] = data;\n-\t\teiointc_update_sw_coremap(s, irq, (void *)&data, sizeof(data), true);\n+\t\teiointc_update_sw_coremap(s, irq, data, sizeof(data), true);\n \t\tbreak;\n \tdefault:\n \t\tret = -EINVAL;\n@@ -529,10 +544,9 @@ static int loongarch_eiointc_writel(struct kvm_vcpu *vcpu,\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u32[index] & ~old_data & s->isr.reg_u32[index];\n-\t\tindex = index << 2;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 4 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -541,7 +555,7 @@ static int loongarch_eiointc_writel(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u32[index] & old_data & s->isr.reg_u32[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 4 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:\n@@ -570,7 +584,7 @@ static int loongarch_eiointc_writel(struct kvm_vcpu *vcpu,\n \t\tirq = offset - EIOINTC_COREMAP_START;\n \t\tindex = irq >> 2;\n \t\ts->coremap.reg_u32[index] = data;\n-\t\teiointc_update_sw_coremap(s, irq, (void *)&data, sizeof(data), true);\n+\t\teiointc_update_sw_coremap(s, irq, data, sizeof(data), true);\n \t\tbreak;\n \tdefault:\n \t\tret = -EINVAL;\n@@ -615,10 +629,9 @@ static int loongarch_eiointc_writeq(struct kvm_vcpu *vcpu,\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u64[index] & ~old_data & s->isr.reg_u64[index];\n-\t\tindex = index << 3;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 8 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -627,7 +640,7 @@ static int loongarch_eiointc_writeq(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u64[index] & old_data & s->isr.reg_u64[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 8 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:\n@@ -656,7 +669,7 @@ static int loongarch_eiointc_writeq(struct kvm_vcpu *vcpu,\n \t\tirq = offset - EIOINTC_COREMAP_START;\n \t\tindex = irq >> 3;\n \t\ts->coremap.reg_u64[index] = data;\n-\t\teiointc_update_sw_coremap(s, irq, (void *)&data, sizeof(data), true);\n+\t\teiointc_update_sw_coremap(s, irq, data, sizeof(data), true);\n \t\tbreak;\n \tdefault:\n \t\tret = -EINVAL;\n@@ -679,6 +692,11 @@ static int kvm_eiointc_write(struct kvm_vcpu *vcpu,\n \t\treturn -EINVAL;\n \t}\n \n+\tif (addr & (len - 1)) {\n+\t\tkvm_err(\"%s: eiointc not aligned addr %llx len %d\\n\", __func__, addr, len);\n+\t\treturn -EINVAL;\n+\t}\n+\n \tvcpu->kvm->stat.eiointc_write_exits++;\n \tspin_lock_irqsave(&eiointc->lock, flags);\n \tswitch (len) {\n@@ -787,7 +805,7 @@ static int kvm_eiointc_ctrl_access(struct kvm_device *dev,\n \tint ret = 0;\n \tunsigned long flags;\n \tunsigned long type = (unsigned long)attr->attr;\n-\tu32 i, start_irq;\n+\tu32 i, start_irq, val;\n \tvoid __user *data;\n \tstruct loongarch_eiointc *s = dev->kvm->arch.eiointc;\n \n@@ -795,8 +813,14 @@ static int kvm_eiointc_ctrl_access(struct kvm_device *dev,\n \tspin_lock_irqsave(&s->lock, flags);\n \tswitch (type) {\n \tcase KVM_DEV_LOONGARCH_EXTIOI_CTRL_INIT_NUM_CPU:\n-\t\tif (copy_from_user(&s->num_cpu, data, 4))\n+\t\tif (copy_from_user(&val, data, 4))\n \t\t\tret = -EFAULT;\n+\t\telse {\n+\t\t\tif (val >= EIOINTC_ROUTE_MAX_VCPUS)\n+\t\t\t\tret = -EINVAL;\n+\t\t\telse\n+\t\t\t\ts->num_cpu = val;\n+\t\t}\n \t\tbreak;\n \tcase KVM_DEV_LOONGARCH_EXTIOI_CTRL_INIT_FEATURE:\n \t\tif (copy_from_user(&s->features, data, 4))\n@@ -809,7 +833,7 @@ static int kvm_eiointc_ctrl_access(struct kvm_device *dev,\n \t\tfor (i = 0; i < (EIOINTC_IRQS / 4); i++) {\n \t\t\tstart_irq = i * 4;\n \t\t\teiointc_update_sw_coremap(s, start_irq,\n-\t\t\t\t\t(void *)&s->coremap.reg_u32[i], sizeof(u32), false);\n+\t\t\t\t\ts->coremap.reg_u32[i], sizeof(u32), false);\n \t\t}\n \t\tbreak;\n \tdefault:\n@@ -824,7 +848,7 @@ static int kvm_eiointc_regs_access(struct kvm_device *dev,\n \t\t\t\t\tstruct kvm_device_attr *attr,\n \t\t\t\t\tbool is_write)\n {\n-\tint addr, cpuid, offset, ret = 0;\n+\tint addr, cpu, offset, ret = 0;\n \tunsigned long flags;\n \tvoid *p = NULL;\n \tvoid __user *data;\n@@ -832,7 +856,7 @@ static int kvm_eiointc_regs_access(struct kvm_device *dev,\n \n \ts = dev->kvm->arch.eiointc;\n \taddr = attr->attr;\n-\tcpuid = addr >> 16;\n+\tcpu = addr >> 16;\n \taddr &= 0xffff;\n \tdata = (void __user *)attr->addr;\n \tswitch (addr) {\n@@ -857,8 +881,11 @@ static int kvm_eiointc_regs_access(struct kvm_device *dev,\n \t\tp = &s->isr.reg_u32[offset];\n \t\tbreak;\n \tcase EIOINTC_COREISR_START ... EIOINTC_COREISR_END:\n+\t\tif (cpu >= s->num_cpu)\n+\t\t\treturn -EINVAL;\n+\n \t\toffset = (addr - EIOINTC_COREISR_START) / 4;\n-\t\tp = &s->coreisr.reg_u32[cpuid][offset];\n+\t\tp = &s->coreisr.reg_u32[cpu][offset];\n \t\tbreak;\n \tcase EIOINTC_COREMAP_START ... EIOINTC_COREMAP_END:\n \t\toffset = (addr - EIOINTC_COREMAP_START) / 4;\n@@ -899,9 +926,15 @@ static int kvm_eiointc_sw_status_access(struct kvm_device *dev,\n \tdata = (void __user *)attr->addr;\n \tswitch (addr) {\n \tcase KVM_DEV_LOONGARCH_EXTIOI_SW_STATUS_NUM_CPU:\n+\t\tif (is_write)\n+\t\t\treturn ret;\n+\n \t\tp = &s->num_cpu;\n \t\tbreak;\n \tcase KVM_DEV_LOONGARCH_EXTIOI_SW_STATUS_FEATURE:\n+\t\tif (is_write)\n+\t\t\treturn ret;\n+\n \t\tp = &s->features;\n \t\tbreak;\n \tcase KVM_DEV_LOONGARCH_EXTIOI_SW_STATUS_STATE:\ndiff --git a/arch/loongarch/lib/crc32-loongarch.c b/arch/loongarch/lib/crc32-loongarch.c\nindex b37cd8537b45..db22c2ec55e2 100644\n--- a/arch/loongarch/lib/crc32-loongarch.c\n+++ b/arch/loongarch/lib/crc32-loongarch.c\n@@ -11,6 +11,7 @@\n \n #include <asm/cpu-features.h>\n #include <linux/crc32.h>\n+#include <linux/export.h>\n #include <linux/module.h>\n #include <linux/unaligned.h>\n \ndiff --git a/arch/loongarch/lib/csum.c b/arch/loongarch/lib/csum.c\nindex df309ae4045d..bcc9d01d8c41 100644\n--- a/arch/loongarch/lib/csum.c\n+++ b/arch/loongarch/lib/csum.c\n@@ -2,6 +2,7 @@\n // Copyright (C) 2019-2020 Arm Ltd.\n \n #include <linux/compiler.h>\n+#include <linux/export.h>\n #include <linux/kasan-checks.h>\n #include <linux/kernel.h>\n \ndiff --git a/arch/loongarch/mm/ioremap.c b/arch/loongarch/mm/ioremap.c\nindex 70ca73019811..df949a3d0f34 100644\n--- a/arch/loongarch/mm/ioremap.c\n+++ b/arch/loongarch/mm/ioremap.c\n@@ -16,12 +16,12 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)\n \n }\n \n-void *early_memremap_ro(resource_size_t phys_addr, unsigned long size)\n+void * __init early_memremap_ro(resource_size_t phys_addr, unsigned long size)\n {\n \treturn early_memremap(phys_addr, size);\n }\n \n-void *early_memremap_prot(resource_size_t phys_addr, unsigned long size,\n+void * __init early_memremap_prot(resource_size_t phys_addr, unsigned long size,\n \t\t    unsigned long prot_val)\n {\n \treturn early_memremap(phys_addr, size);\ndiff --git a/arch/loongarch/pci/pci.c b/arch/loongarch/pci/pci.c\nindex 2726639150bc..5bc9627a6cf9 100644\n--- a/arch/loongarch/pci/pci.c\n+++ b/arch/loongarch/pci/pci.c\n@@ -3,7 +3,6 @@\n  * Copyright (C) 2020-2022 Loongson Technology Corporation Limited\n  */\n #include <linux/kernel.h>\n-#include <linux/export.h>\n #include <linux/init.h>\n #include <linux/acpi.h>\n #include <linux/types.h>\ndiff --git a/tools/arch/loongarch/include/asm/orc_types.h b/tools/arch/loongarch/include/asm/orc_types.h\nindex caf1f71a1057..d5fa98d1d177 100644\n--- a/tools/arch/loongarch/include/asm/orc_types.h\n+++ b/tools/arch/loongarch/include/asm/orc_types.h\n@@ -34,7 +34,7 @@\n #define ORC_TYPE_REGS\t\t\t3\n #define ORC_TYPE_REGS_PARTIAL\t\t4\n \n-#ifndef __ASSEMBLY__\n+#ifndef __ASSEMBLER__\n /*\n  * This struct is more or less a vastly simplified version of the DWARF Call\n  * Frame Information standard.  It contains only the necessary parts of DWARF\n@@ -53,6 +53,6 @@ struct orc_entry {\n \tunsigned int\ttype:3;\n \tunsigned int\tsignal:1;\n };\n-#endif /* __ASSEMBLY__ */\n+#endif /* __ASSEMBLER__ */\n \n #endif /* _ORC_TYPES_H */",
    "stats": {
      "insertions": 151,
      "deletions": 100,
      "files": 44
    }
  },
  {
    "sha": "45537926dd2aaa9190ac0fac5a0fbeefcadfea95",
    "message": "s390/pci: Fix stale function handles in error handling\n\nThe error event information for PCI error events contains a function\nhandle for the respective function. This handle is generally captured at\nthe time the error event was recorded. Due to delays in processing or\ncascading issues, it may happen that during firmware recovery multiple\nevents are generated. When processing these events in order Linux may\nalready have recovered an affected function making the event information\nstale. Fix this by doing an unconditional CLP List PCI function\nretrieving the current function handle with the zdev->state_lock held\nand ignoring the event if its function handle is stale.\n\nCc: stable@vger.kernel.org\nFixes: 4cdf2f4e24ff (\"s390/pci: implement minimal PCI error recovery\")\nReviewed-by: Julian Ruess <julianr@linux.ibm.com>\nReviewed-by: Gerd Bayer <gbayer@linux.ibm.com>\nReviewed-by: Farhan Ali <alifm@linux.ibm.com>\nSigned-off-by: Niklas Schnelle <schnelle@linux.ibm.com>\nSigned-off-by: Alexander Gordeev <agordeev@linux.ibm.com>",
    "author": "Niklas Schnelle",
    "date": "2025-06-28T18:58:59+02:00",
    "files_changed": [
      "arch/s390/pci/pci_event.c"
    ],
    "diff": "diff --git a/arch/s390/pci/pci_event.c b/arch/s390/pci/pci_event.c\nindex 2fbee3887d13..82ee2578279a 100644\n--- a/arch/s390/pci/pci_event.c\n+++ b/arch/s390/pci/pci_event.c\n@@ -273,6 +273,8 @@ static void __zpci_event_error(struct zpci_ccdf_err *ccdf)\n \tstruct zpci_dev *zdev = get_zdev_by_fid(ccdf->fid);\n \tstruct pci_dev *pdev = NULL;\n \tpci_ers_result_t ers_res;\n+\tu32 fh = 0;\n+\tint rc;\n \n \tzpci_dbg(3, \"err fid:%x, fh:%x, pec:%x\\n\",\n \t\t ccdf->fid, ccdf->fh, ccdf->pec);\n@@ -281,6 +283,15 @@ static void __zpci_event_error(struct zpci_ccdf_err *ccdf)\n \n \tif (zdev) {\n \t\tmutex_lock(&zdev->state_lock);\n+\t\trc = clp_refresh_fh(zdev->fid, &fh);\n+\t\tif (rc)\n+\t\t\tgoto no_pdev;\n+\t\tif (!fh || ccdf->fh != fh) {\n+\t\t\t/* Ignore events with stale handles */\n+\t\t\tzpci_dbg(3, \"err fid:%x, fh:%x (stale %x)\\n\",\n+\t\t\t\t ccdf->fid, fh, ccdf->fh);\n+\t\t\tgoto no_pdev;\n+\t\t}\n \t\tzpci_update_fh(zdev, ccdf->fh);\n \t\tif (zdev->zbus->bus)\n \t\t\tpdev = pci_get_slot(zdev->zbus->bus, zdev->devfn);",
    "stats": {
      "insertions": 11,
      "deletions": 0,
      "files": 1
    }
  },
  {
    "sha": "aaf724ed69264719550ec4f194d3ab17b886af9a",
    "message": "Merge tag 'v6.16-rc3-smb3-client-fixes' of git://git.samba.org/sfrench/cifs-2.6\n\nPull smb client fixes from Steve French:\n\n - Multichannel reconnect lock ordering deadlock fix\n\n - Fix for regression in handling native Windows symlinks\n\n - Three smbdirect fixes:\n     - oops in RDMA response processing\n     - smbdirect memcpy issue\n     - fix smbdirect regression with large writes (smbdirect test cases\n       now all passing)\n\n - Fix for \"FAILED_TO_PARSE\" warning in trace-cmd report output\n\n* tag 'v6.16-rc3-smb3-client-fixes' of git://git.samba.org/sfrench/cifs-2.6:\n  cifs: Fix reading into an ITER_FOLIOQ from the smbdirect code\n  cifs: Fix the smbd_response slab to allow usercopy\n  smb: client: fix potential deadlock when reconnecting channels\n  smb: client: remove \\t from TP_printk statements\n  smb: client: let smbd_post_send_iter() respect the peers max_send_size and transmit all data\n  smb: client: fix regression with native SMB symlinks",
    "author": "Linus Torvalds",
    "date": "2025-06-27T20:38:05-07:00",
    "files_changed": [
      "fs/smb/client/cifsglob.h",
      "fs/smb/client/connect.c",
      "fs/smb/client/reparse.c",
      "fs/smb/client/smbdirect.c",
      "fs/smb/client/trace.h"
    ],
    "diff": "diff --git a/fs/smb/client/cifsglob.h b/fs/smb/client/cifsglob.h\nindex 45e94e18f4d5..318a8405d475 100644\n--- a/fs/smb/client/cifsglob.h\n+++ b/fs/smb/client/cifsglob.h\n@@ -709,6 +709,7 @@ inc_rfc1001_len(void *buf, int count)\n struct TCP_Server_Info {\n \tstruct list_head tcp_ses_list;\n \tstruct list_head smb_ses_list;\n+\tstruct list_head rlist; /* reconnect list */\n \tspinlock_t srv_lock;  /* protect anything here that is not protected */\n \t__u64 conn_id; /* connection identifier (useful for debugging) */\n \tint srv_count; /* reference counter */\ndiff --git a/fs/smb/client/connect.c b/fs/smb/client/connect.c\nindex c48869c29e15..685c65dcb8c4 100644\n--- a/fs/smb/client/connect.c\n+++ b/fs/smb/client/connect.c\n@@ -124,6 +124,14 @@ static void smb2_query_server_interfaces(struct work_struct *work)\n \t\t\t   (SMB_INTERFACE_POLL_INTERVAL * HZ));\n }\n \n+#define set_need_reco(server) \\\n+do { \\\n+\tspin_lock(&server->srv_lock); \\\n+\tif (server->tcpStatus != CifsExiting) \\\n+\t\tserver->tcpStatus = CifsNeedReconnect; \\\n+\tspin_unlock(&server->srv_lock); \\\n+} while (0)\n+\n /*\n  * Update the tcpStatus for the server.\n  * This is used to signal the cifsd thread to call cifs_reconnect\n@@ -137,39 +145,45 @@ void\n cifs_signal_cifsd_for_reconnect(struct TCP_Server_Info *server,\n \t\t\t\tbool all_channels)\n {\n-\tstruct TCP_Server_Info *pserver;\n+\tstruct TCP_Server_Info *nserver;\n \tstruct cifs_ses *ses;\n+\tLIST_HEAD(reco);\n \tint i;\n \n-\t/* If server is a channel, select the primary channel */\n-\tpserver = SERVER_IS_CHAN(server) ? server->primary_server : server;\n-\n \t/* if we need to signal just this channel */\n \tif (!all_channels) {\n-\t\tspin_lock(&server->srv_lock);\n-\t\tif (server->tcpStatus != CifsExiting)\n-\t\t\tserver->tcpStatus = CifsNeedReconnect;\n-\t\tspin_unlock(&server->srv_lock);\n+\t\tset_need_reco(server);\n \t\treturn;\n \t}\n \n-\tspin_lock(&cifs_tcp_ses_lock);\n-\tlist_for_each_entry(ses, &pserver->smb_ses_list, smb_ses_list) {\n-\t\tif (cifs_ses_exiting(ses))\n-\t\t\tcontinue;\n-\t\tspin_lock(&ses->chan_lock);\n-\t\tfor (i = 0; i < ses->chan_count; i++) {\n-\t\t\tif (!ses->chans[i].server)\n+\tif (SERVER_IS_CHAN(server))\n+\t\tserver = server->primary_server;\n+\tscoped_guard(spinlock, &cifs_tcp_ses_lock) {\n+\t\tset_need_reco(server);\n+\t\tlist_for_each_entry(ses, &server->smb_ses_list, smb_ses_list) {\n+\t\t\tspin_lock(&ses->ses_lock);\n+\t\t\tif (ses->ses_status == SES_EXITING) {\n+\t\t\t\tspin_unlock(&ses->ses_lock);\n \t\t\t\tcontinue;\n-\n-\t\t\tspin_lock(&ses->chans[i].server->srv_lock);\n-\t\t\tif (ses->chans[i].server->tcpStatus != CifsExiting)\n-\t\t\t\tses->chans[i].server->tcpStatus = CifsNeedReconnect;\n-\t\t\tspin_unlock(&ses->chans[i].server->srv_lock);\n+\t\t\t}\n+\t\t\tspin_lock(&ses->chan_lock);\n+\t\t\tfor (i = 1; i < ses->chan_count; i++) {\n+\t\t\t\tnserver = ses->chans[i].server;\n+\t\t\t\tif (!nserver)\n+\t\t\t\t\tcontinue;\n+\t\t\t\tnserver->srv_count++;\n+\t\t\t\tlist_add(&nserver->rlist, &reco);\n+\t\t\t}\n+\t\t\tspin_unlock(&ses->chan_lock);\n+\t\t\tspin_unlock(&ses->ses_lock);\n \t\t}\n-\t\tspin_unlock(&ses->chan_lock);\n \t}\n-\tspin_unlock(&cifs_tcp_ses_lock);\n+\n+\tlist_for_each_entry_safe(server, nserver, &reco, rlist) {\n+\t\tlist_del_init(&server->rlist);\n+\t\tset_need_reco(server);\n+\t\tcifs_put_tcp_session(server, 0);\n+\t}\n }\n \n /*\ndiff --git a/fs/smb/client/reparse.c b/fs/smb/client/reparse.c\nindex 511611206dab..1c40e42e4d89 100644\n--- a/fs/smb/client/reparse.c\n+++ b/fs/smb/client/reparse.c\n@@ -875,15 +875,8 @@ int smb2_parse_native_symlink(char **target, const char *buf, unsigned int len,\n \t\t\tabs_path += sizeof(\"\\\\DosDevices\\\\\")-1;\n \t\telse if (strstarts(abs_path, \"\\\\GLOBAL??\\\\\"))\n \t\t\tabs_path += sizeof(\"\\\\GLOBAL??\\\\\")-1;\n-\t\telse {\n-\t\t\t/* Unhandled absolute symlink, points outside of DOS/Win32 */\n-\t\t\tcifs_dbg(VFS,\n-\t\t\t\t \"absolute symlink '%s' cannot be converted from NT format \"\n-\t\t\t\t \"because points to unknown target\\n\",\n-\t\t\t\t smb_target);\n-\t\t\trc = -EIO;\n-\t\t\tgoto out;\n-\t\t}\n+\t\telse\n+\t\t\tgoto out_unhandled_target;\n \n \t\t/* Sometimes path separator after \\?? is double backslash */\n \t\tif (abs_path[0] == '\\\\')\n@@ -910,13 +903,7 @@ int smb2_parse_native_symlink(char **target, const char *buf, unsigned int len,\n \t\t\tabs_path++;\n \t\t\tabs_path[0] = drive_letter;\n \t\t} else {\n-\t\t\t/* Unhandled absolute symlink. Report an error. */\n-\t\t\tcifs_dbg(VFS,\n-\t\t\t\t \"absolute symlink '%s' cannot be converted from NT format \"\n-\t\t\t\t \"because points to unknown target\\n\",\n-\t\t\t\t smb_target);\n-\t\t\trc = -EIO;\n-\t\t\tgoto out;\n+\t\t\tgoto out_unhandled_target;\n \t\t}\n \n \t\tabs_path_len = strlen(abs_path)+1;\n@@ -966,6 +953,7 @@ int smb2_parse_native_symlink(char **target, const char *buf, unsigned int len,\n \t\t * These paths have same format as Linux symlinks, so no\n \t\t * conversion is needed.\n \t\t */\n+out_unhandled_target:\n \t\tlinux_target = smb_target;\n \t\tsmb_target = NULL;\n \t}\ndiff --git a/fs/smb/client/smbdirect.c b/fs/smb/client/smbdirect.c\nindex cbc85bca006f..754e94a0e07f 100644\n--- a/fs/smb/client/smbdirect.c\n+++ b/fs/smb/client/smbdirect.c\n@@ -907,8 +907,10 @@ static int smbd_post_send_iter(struct smbd_connection *info,\n \t\t\t.local_dma_lkey\t= sc->ib.pd->local_dma_lkey,\n \t\t\t.direction\t= DMA_TO_DEVICE,\n \t\t};\n+\t\tsize_t payload_len = umin(*_remaining_data_length,\n+\t\t\t\t\t  sp->max_send_size - sizeof(*packet));\n \n-\t\trc = smb_extract_iter_to_rdma(iter, *_remaining_data_length,\n+\t\trc = smb_extract_iter_to_rdma(iter, payload_len,\n \t\t\t\t\t      &extract);\n \t\tif (rc < 0)\n \t\t\tgoto err_dma;\n@@ -1013,6 +1015,27 @@ static int smbd_post_send_empty(struct smbd_connection *info)\n \treturn smbd_post_send_iter(info, NULL, &remaining_data_length);\n }\n \n+static int smbd_post_send_full_iter(struct smbd_connection *info,\n+\t\t\t\t    struct iov_iter *iter,\n+\t\t\t\t    int *_remaining_data_length)\n+{\n+\tint rc = 0;\n+\n+\t/*\n+\t * smbd_post_send_iter() respects the\n+\t * negotiated max_send_size, so we need to\n+\t * loop until the full iter is posted\n+\t */\n+\n+\twhile (iov_iter_count(iter) > 0) {\n+\t\trc = smbd_post_send_iter(info, iter, _remaining_data_length);\n+\t\tif (rc < 0)\n+\t\t\tbreak;\n+\t}\n+\n+\treturn rc;\n+}\n+\n /*\n  * Post a receive request to the transport\n  * The remote peer can only send data when a receive request is posted\n@@ -1452,6 +1475,9 @@ static int allocate_caches_and_workqueue(struct smbd_connection *info)\n \tchar name[MAX_NAME_LEN];\n \tint rc;\n \n+\tif (WARN_ON_ONCE(sp->max_recv_size < sizeof(struct smbdirect_data_transfer)))\n+\t\treturn -ENOMEM;\n+\n \tscnprintf(name, MAX_NAME_LEN, \"smbd_request_%p\", info);\n \tinfo->request_cache =\n \t\tkmem_cache_create(\n@@ -1469,12 +1495,17 @@ static int allocate_caches_and_workqueue(struct smbd_connection *info)\n \t\tgoto out1;\n \n \tscnprintf(name, MAX_NAME_LEN, \"smbd_response_%p\", info);\n+\n+\tstruct kmem_cache_args response_args = {\n+\t\t.align\t\t= __alignof__(struct smbd_response),\n+\t\t.useroffset\t= (offsetof(struct smbd_response, packet) +\n+\t\t\t\t   sizeof(struct smbdirect_data_transfer)),\n+\t\t.usersize\t= sp->max_recv_size - sizeof(struct smbdirect_data_transfer),\n+\t};\n \tinfo->response_cache =\n-\t\tkmem_cache_create(\n-\t\t\tname,\n-\t\t\tsizeof(struct smbd_response) +\n-\t\t\t\tsp->max_recv_size,\n-\t\t\t0, SLAB_HWCACHE_ALIGN, NULL);\n+\t\tkmem_cache_create(name,\n+\t\t\t\t  sizeof(struct smbd_response) + sp->max_recv_size,\n+\t\t\t\t  &response_args, SLAB_HWCACHE_ALIGN);\n \tif (!info->response_cache)\n \t\tgoto out2;\n \n@@ -1747,35 +1778,39 @@ struct smbd_connection *smbd_get_connection(\n }\n \n /*\n- * Receive data from receive reassembly queue\n+ * Receive data from the transport's receive reassembly queue\n  * All the incoming data packets are placed in reassembly queue\n- * buf: the buffer to read data into\n+ * iter: the buffer to read data into\n  * size: the length of data to read\n  * return value: actual data read\n- * Note: this implementation copies the data from reassebmly queue to receive\n+ *\n+ * Note: this implementation copies the data from reassembly queue to receive\n  * buffers used by upper layer. This is not the optimal code path. A better way\n  * to do it is to not have upper layer allocate its receive buffers but rather\n  * borrow the buffer from reassembly queue, and return it after data is\n  * consumed. But this will require more changes to upper layer code, and also\n  * need to consider packet boundaries while they still being reassembled.\n  */\n-static int smbd_recv_buf(struct smbd_connection *info, char *buf,\n-\t\tunsigned int size)\n+int smbd_recv(struct smbd_connection *info, struct msghdr *msg)\n {\n \tstruct smbdirect_socket *sc = &info->socket;\n \tstruct smbd_response *response;\n \tstruct smbdirect_data_transfer *data_transfer;\n+\tsize_t size = iov_iter_count(&msg->msg_iter);\n \tint to_copy, to_read, data_read, offset;\n \tu32 data_length, remaining_data_length, data_offset;\n \tint rc;\n \n+\tif (WARN_ON_ONCE(iov_iter_rw(&msg->msg_iter) == WRITE))\n+\t\treturn -EINVAL; /* It's a bug in upper layer to get there */\n+\n again:\n \t/*\n \t * No need to hold the reassembly queue lock all the time as we are\n \t * the only one reading from the front of the queue. The transport\n \t * may add more entries to the back of the queue at the same time\n \t */\n-\tlog_read(INFO, \"size=%d info->reassembly_data_length=%d\\n\", size,\n+\tlog_read(INFO, \"size=%zd info->reassembly_data_length=%d\\n\", size,\n \t\tinfo->reassembly_data_length);\n \tif (info->reassembly_data_length >= size) {\n \t\tint queue_length;\n@@ -1813,7 +1848,10 @@ static int smbd_recv_buf(struct smbd_connection *info, char *buf,\n \t\t\tif (response->first_segment && size == 4) {\n \t\t\t\tunsigned int rfc1002_len =\n \t\t\t\t\tdata_length + remaining_data_length;\n-\t\t\t\t*((__be32 *)buf) = cpu_to_be32(rfc1002_len);\n+\t\t\t\t__be32 rfc1002_hdr = cpu_to_be32(rfc1002_len);\n+\t\t\t\tif (copy_to_iter(&rfc1002_hdr, sizeof(rfc1002_hdr),\n+\t\t\t\t\t\t &msg->msg_iter) != sizeof(rfc1002_hdr))\n+\t\t\t\t\treturn -EFAULT;\n \t\t\t\tdata_read = 4;\n \t\t\t\tresponse->first_segment = false;\n \t\t\t\tlog_read(INFO, \"returning rfc1002 length %d\\n\",\n@@ -1822,10 +1860,9 @@ static int smbd_recv_buf(struct smbd_connection *info, char *buf,\n \t\t\t}\n \n \t\t\tto_copy = min_t(int, data_length - offset, to_read);\n-\t\t\tmemcpy(\n-\t\t\t\tbuf + data_read,\n-\t\t\t\t(char *)data_transfer + data_offset + offset,\n-\t\t\t\tto_copy);\n+\t\t\tif (copy_to_iter((char *)data_transfer + data_offset + offset,\n+\t\t\t\t\t to_copy, &msg->msg_iter) != to_copy)\n+\t\t\t\treturn -EFAULT;\n \n \t\t\t/* move on to the next buffer? */\n \t\t\tif (to_copy == data_length - offset) {\n@@ -1890,90 +1927,6 @@ static int smbd_recv_buf(struct smbd_connection *info, char *buf,\n \tgoto again;\n }\n \n-/*\n- * Receive a page from receive reassembly queue\n- * page: the page to read data into\n- * to_read: the length of data to read\n- * return value: actual data read\n- */\n-static int smbd_recv_page(struct smbd_connection *info,\n-\t\tstruct page *page, unsigned int page_offset,\n-\t\tunsigned int to_read)\n-{\n-\tstruct smbdirect_socket *sc = &info->socket;\n-\tint ret;\n-\tchar *to_address;\n-\tvoid *page_address;\n-\n-\t/* make sure we have the page ready for read */\n-\tret = wait_event_interruptible(\n-\t\tinfo->wait_reassembly_queue,\n-\t\tinfo->reassembly_data_length >= to_read ||\n-\t\t\tsc->status != SMBDIRECT_SOCKET_CONNECTED);\n-\tif (ret)\n-\t\treturn ret;\n-\n-\t/* now we can read from reassembly queue and not sleep */\n-\tpage_address = kmap_atomic(page);\n-\tto_address = (char *) page_address + page_offset;\n-\n-\tlog_read(INFO, \"reading from page=%p address=%p to_read=%d\\n\",\n-\t\tpage, to_address, to_read);\n-\n-\tret = smbd_recv_buf(info, to_address, to_read);\n-\tkunmap_atomic(page_address);\n-\n-\treturn ret;\n-}\n-\n-/*\n- * Receive data from transport\n- * msg: a msghdr point to the buffer, can be ITER_KVEC or ITER_BVEC\n- * return: total bytes read, or 0. SMB Direct will not do partial read.\n- */\n-int smbd_recv(struct smbd_connection *info, struct msghdr *msg)\n-{\n-\tchar *buf;\n-\tstruct page *page;\n-\tunsigned int to_read, page_offset;\n-\tint rc;\n-\n-\tif (iov_iter_rw(&msg->msg_iter) == WRITE) {\n-\t\t/* It's a bug in upper layer to get there */\n-\t\tcifs_dbg(VFS, \"Invalid msg iter dir %u\\n\",\n-\t\t\t iov_iter_rw(&msg->msg_iter));\n-\t\trc = -EINVAL;\n-\t\tgoto out;\n-\t}\n-\n-\tswitch (iov_iter_type(&msg->msg_iter)) {\n-\tcase ITER_KVEC:\n-\t\tbuf = msg->msg_iter.kvec->iov_base;\n-\t\tto_read = msg->msg_iter.kvec->iov_len;\n-\t\trc = smbd_recv_buf(info, buf, to_read);\n-\t\tbreak;\n-\n-\tcase ITER_BVEC:\n-\t\tpage = msg->msg_iter.bvec->bv_page;\n-\t\tpage_offset = msg->msg_iter.bvec->bv_offset;\n-\t\tto_read = msg->msg_iter.bvec->bv_len;\n-\t\trc = smbd_recv_page(info, page, page_offset, to_read);\n-\t\tbreak;\n-\n-\tdefault:\n-\t\t/* It's a bug in upper layer to get there */\n-\t\tcifs_dbg(VFS, \"Invalid msg type %d\\n\",\n-\t\t\t iov_iter_type(&msg->msg_iter));\n-\t\trc = -EINVAL;\n-\t}\n-\n-out:\n-\t/* SMBDirect will read it all or nothing */\n-\tif (rc > 0)\n-\t\tmsg->msg_iter.count = 0;\n-\treturn rc;\n-}\n-\n /*\n  * Send data to transport\n  * Each rqst is transported as a SMBDirect payload\n@@ -2032,14 +1985,14 @@ int smbd_send(struct TCP_Server_Info *server,\n \t\t\tklen += rqst->rq_iov[i].iov_len;\n \t\tiov_iter_kvec(&iter, ITER_SOURCE, rqst->rq_iov, rqst->rq_nvec, klen);\n \n-\t\trc = smbd_post_send_iter(info, &iter, &remaining_data_length);\n+\t\trc = smbd_post_send_full_iter(info, &iter, &remaining_data_length);\n \t\tif (rc < 0)\n \t\t\tbreak;\n \n \t\tif (iov_iter_count(&rqst->rq_iter) > 0) {\n \t\t\t/* And then the data pages if there are any */\n-\t\t\trc = smbd_post_send_iter(info, &rqst->rq_iter,\n-\t\t\t\t\t\t &remaining_data_length);\n+\t\t\trc = smbd_post_send_full_iter(info, &rqst->rq_iter,\n+\t\t\t\t\t\t      &remaining_data_length);\n \t\t\tif (rc < 0)\n \t\t\t\tbreak;\n \t\t}\ndiff --git a/fs/smb/client/trace.h b/fs/smb/client/trace.h\nindex 52bcb55d9952..93e5b2bb9f28 100644\n--- a/fs/smb/client/trace.h\n+++ b/fs/smb/client/trace.h\n@@ -140,7 +140,7 @@ DECLARE_EVENT_CLASS(smb3_rw_err_class,\n \t\t__entry->len = len;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\tR=%08x[%x] xid=%u sid=0x%llx tid=0x%x fid=0x%llx offset=0x%llx len=0x%x rc=%d\",\n+\tTP_printk(\"R=%08x[%x] xid=%u sid=0x%llx tid=0x%x fid=0x%llx offset=0x%llx len=0x%x rc=%d\",\n \t\t  __entry->rreq_debug_id, __entry->rreq_debug_index,\n \t\t  __entry->xid, __entry->sesid, __entry->tid, __entry->fid,\n \t\t  __entry->offset, __entry->len, __entry->rc)\n@@ -190,7 +190,7 @@ DECLARE_EVENT_CLASS(smb3_other_err_class,\n \t\t__entry->len = len;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\txid=%u sid=0x%llx tid=0x%x fid=0x%llx offset=0x%llx len=0x%x rc=%d\",\n+\tTP_printk(\"xid=%u sid=0x%llx tid=0x%x fid=0x%llx offset=0x%llx len=0x%x rc=%d\",\n \t\t__entry->xid, __entry->sesid, __entry->tid, __entry->fid,\n \t\t__entry->offset, __entry->len, __entry->rc)\n )\n@@ -247,7 +247,7 @@ DECLARE_EVENT_CLASS(smb3_copy_range_err_class,\n \t\t__entry->len = len;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\txid=%u sid=0x%llx tid=0x%x source fid=0x%llx source offset=0x%llx target fid=0x%llx target offset=0x%llx len=0x%x rc=%d\",\n+\tTP_printk(\"xid=%u sid=0x%llx tid=0x%x source fid=0x%llx source offset=0x%llx target fid=0x%llx target offset=0x%llx len=0x%x rc=%d\",\n \t\t__entry->xid, __entry->sesid, __entry->tid, __entry->target_fid,\n \t\t__entry->src_offset, __entry->target_fid, __entry->target_offset, __entry->len, __entry->rc)\n )\n@@ -298,7 +298,7 @@ DECLARE_EVENT_CLASS(smb3_copy_range_done_class,\n \t\t__entry->target_offset = target_offset;\n \t\t__entry->len = len;\n \t),\n-\tTP_printk(\"\\txid=%u sid=0x%llx tid=0x%x source fid=0x%llx source offset=0x%llx target fid=0x%llx target offset=0x%llx len=0x%x\",\n+\tTP_printk(\"xid=%u sid=0x%llx tid=0x%x source fid=0x%llx source offset=0x%llx target fid=0x%llx target offset=0x%llx len=0x%x\",\n \t\t__entry->xid, __entry->sesid, __entry->tid, __entry->target_fid,\n \t\t__entry->src_offset, __entry->target_fid, __entry->target_offset, __entry->len)\n )\n@@ -482,7 +482,7 @@ DECLARE_EVENT_CLASS(smb3_fd_class,\n \t\t__entry->tid = tid;\n \t\t__entry->sesid = sesid;\n \t),\n-\tTP_printk(\"\\txid=%u sid=0x%llx tid=0x%x fid=0x%llx\",\n+\tTP_printk(\"xid=%u sid=0x%llx tid=0x%x fid=0x%llx\",\n \t\t__entry->xid, __entry->sesid, __entry->tid, __entry->fid)\n )\n \n@@ -521,7 +521,7 @@ DECLARE_EVENT_CLASS(smb3_fd_err_class,\n \t\t__entry->sesid = sesid;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\txid=%u sid=0x%llx tid=0x%x fid=0x%llx rc=%d\",\n+\tTP_printk(\"xid=%u sid=0x%llx tid=0x%x fid=0x%llx rc=%d\",\n \t\t__entry->xid, __entry->sesid, __entry->tid, __entry->fid,\n \t\t__entry->rc)\n )\n@@ -794,7 +794,7 @@ DECLARE_EVENT_CLASS(smb3_cmd_err_class,\n \t\t__entry->status = status;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\tsid=0x%llx tid=0x%x cmd=%u mid=%llu status=0x%x rc=%d\",\n+\tTP_printk(\"sid=0x%llx tid=0x%x cmd=%u mid=%llu status=0x%x rc=%d\",\n \t\t__entry->sesid, __entry->tid, __entry->cmd, __entry->mid,\n \t\t__entry->status, __entry->rc)\n )\n@@ -829,7 +829,7 @@ DECLARE_EVENT_CLASS(smb3_cmd_done_class,\n \t\t__entry->cmd = cmd;\n \t\t__entry->mid = mid;\n \t),\n-\tTP_printk(\"\\tsid=0x%llx tid=0x%x cmd=%u mid=%llu\",\n+\tTP_printk(\"sid=0x%llx tid=0x%x cmd=%u mid=%llu\",\n \t\t__entry->sesid, __entry->tid,\n \t\t__entry->cmd, __entry->mid)\n )\n@@ -867,7 +867,7 @@ DECLARE_EVENT_CLASS(smb3_mid_class,\n \t\t__entry->when_sent = when_sent;\n \t\t__entry->when_received = when_received;\n \t),\n-\tTP_printk(\"\\tcmd=%u mid=%llu pid=%u, when_sent=%lu when_rcv=%lu\",\n+\tTP_printk(\"cmd=%u mid=%llu pid=%u, when_sent=%lu when_rcv=%lu\",\n \t\t__entry->cmd, __entry->mid, __entry->pid, __entry->when_sent,\n \t\t__entry->when_received)\n )\n@@ -898,7 +898,7 @@ DECLARE_EVENT_CLASS(smb3_exit_err_class,\n \t\t__assign_str(func_name);\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\t%s: xid=%u rc=%d\",\n+\tTP_printk(\"%s: xid=%u rc=%d\",\n \t\t__get_str(func_name), __entry->xid, __entry->rc)\n )\n \n@@ -924,7 +924,7 @@ DECLARE_EVENT_CLASS(smb3_sync_err_class,\n \t\t__entry->ino = ino;\n \t\t__entry->rc = rc;\n \t),\n-\tTP_printk(\"\\tino=%lu rc=%d\",\n+\tTP_printk(\"ino=%lu rc=%d\",\n \t\t__entry->ino, __entry->rc)\n )\n \n@@ -950,7 +950,7 @@ DECLARE_EVENT_CLASS(smb3_enter_exit_class,\n \t\t__entry->xid = xid;\n \t\t__assign_str(func_name);\n \t),\n-\tTP_printk(\"\\t%s: xid=%u\",\n+\tTP_printk(\"%s: xid=%u\",\n \t\t__get_str(func_name), __entry->xid)\n )\n ",
    "stats": {
      "insertions": 110,
      "deletions": 154,
      "files": 5
    }
  },
  {
    "sha": "0fd39af24e37a6866c479ca385301845f6029787",
    "message": "Merge tag 'mm-hotfixes-stable-2025-06-27-16-56' of git://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm\n\nPull misc fixes from Andrew Morton:\n \"16 hotfixes.\n\n  6 are cc:stable and the remainder address post-6.15 issues or aren't\n  considered necessary for -stable kernels. 5 are for MM\"\n\n* tag 'mm-hotfixes-stable-2025-06-27-16-56' of git://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm:\n  MAINTAINERS: add Lorenzo as THP co-maintainer\n  mailmap: update Duje Mihanović's email address\n  selftests/mm: fix validate_addr() helper\n  crashdump: add CONFIG_KEYS dependency\n  mailmap: correct name for a historical account of Zijun Hu\n  mailmap: add entries for Zijun Hu\n  fuse: fix runtime warning on truncate_folio_batch_exceptionals()\n  scripts/gdb: fix dentry_name() lookup\n  mm/damon/sysfs-schemes: free old damon_sysfs_scheme_filter->memcg_path on write\n  mm/alloc_tag: fix the kmemleak false positive issue in the allocation of the percpu variable tag->counters\n  lib/group_cpus: fix NULL pointer dereference from group_cpus_evenly()\n  mm/hugetlb: remove unnecessary holding of hugetlb_lock\n  MAINTAINERS: add missing files to mm page alloc section\n  MAINTAINERS: add tree entry to mm init block\n  mm: add OOM killer maintainer structure\n  fs/proc/task_mmu: fix PAGE_IS_PFNZERO detection for the huge zero folio",
    "author": "Linus Torvalds",
    "date": "2025-06-27T20:34:10-07:00",
    "files_changed": [
      "fs/fuse/inode.c",
      "fs/proc/task_mmu.c",
      "include/linux/kmemleak.h",
      "lib/alloc_tag.c",
      "lib/group_cpus.c",
      "mm/damon/sysfs-schemes.c",
      "mm/hugetlb.c",
      "mm/kmemleak.c",
      "tools/testing/selftests/mm/virtual_address_range.c"
    ],
    "diff": "diff --git a/.mailmap b/.mailmap\nindex 93e94b0b9376..b0ace71968ab 100644\n--- a/.mailmap\n+++ b/.mailmap\n@@ -224,6 +224,7 @@ Dmitry Safonov <0x7f454c46@gmail.com> <dsafonov@virtuozzo.com>\n Domen Puncer <domen@coderock.org>\n Douglas Gilbert <dougg@torque.net>\n Drew Fustini <fustini@kernel.org> <drew@pdp7.com>\n+<duje@dujemihanovic.xyz> <duje.mihanovic@skole.hr>\n Ed L. Cashin <ecashin@coraid.com>\n Elliot Berman <quic_eberman@quicinc.com> <eberman@codeaurora.org>\n Enric Balletbo i Serra <eballetbo@kernel.org> <enric.balletbo@collabora.com>\n@@ -831,3 +832,6 @@ Yosry Ahmed <yosry.ahmed@linux.dev> <yosryahmed@google.com>\n Yusuke Goda <goda.yusuke@renesas.com>\n Zack Rusin <zack.rusin@broadcom.com> <zackr@vmware.com>\n Zhu Yanjun <zyjzyj2000@gmail.com> <yanjunz@nvidia.com>\n+Zijun Hu <zijun.hu@oss.qualcomm.com> <quic_zijuhu@quicinc.com>\n+Zijun Hu <zijun.hu@oss.qualcomm.com> <zijuhu@codeaurora.org>\n+Zijun Hu <zijun_hu@htc.com>\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 0fbfe93ba9f2..7aff9b0ead32 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -15676,6 +15676,8 @@ MEMBLOCK AND MEMORY MANAGEMENT INITIALIZATION\n M:\tMike Rapoport <rppt@kernel.org>\n L:\tlinux-mm@kvack.org\n S:\tMaintained\n+T:\tgit git://git.kernel.org/pub/scm/linux/kernel/git/rppt/memblock.git for-next\n+T:\tgit git://git.kernel.org/pub/scm/linux/kernel/git/rppt/memblock.git fixes\n F:\tDocumentation/core-api/boot-time-mm.rst\n F:\tDocumentation/core-api/kho/bindings/memblock/*\n F:\tinclude/linux/memblock.h\n@@ -15848,6 +15850,17 @@ F:\tmm/numa.c\n F:\tmm/numa_emulation.c\n F:\tmm/numa_memblks.c\n \n+MEMORY MANAGEMENT - OOM KILLER\n+M:\tMichal Hocko <mhocko@suse.com>\n+R:\tDavid Rientjes <rientjes@google.com>\n+R:\tShakeel Butt <shakeel.butt@linux.dev>\n+L:\tlinux-mm@kvack.org\n+S:\tMaintained\n+F:\tinclude/linux/oom.h\n+F:\tinclude/trace/events/oom.h\n+F:\tinclude/uapi/linux/oom.h\n+F:\tmm/oom_kill.c\n+\n MEMORY MANAGEMENT - PAGE ALLOCATOR\n M:\tAndrew Morton <akpm@linux-foundation.org>\n M:\tVlastimil Babka <vbabka@suse.cz>\n@@ -15862,8 +15875,17 @@ F:\tinclude/linux/compaction.h\n F:\tinclude/linux/gfp.h\n F:\tinclude/linux/page-isolation.h\n F:\tmm/compaction.c\n+F:\tmm/debug_page_alloc.c\n+F:\tmm/fail_page_alloc.c\n F:\tmm/page_alloc.c\n+F:\tmm/page_ext.c\n+F:\tmm/page_frag_cache.c\n F:\tmm/page_isolation.c\n+F:\tmm/page_owner.c\n+F:\tmm/page_poison.c\n+F:\tmm/page_reporting.c\n+F:\tmm/show_mem.c\n+F:\tmm/shuffle.c\n \n MEMORY MANAGEMENT - RECLAIM\n M:\tAndrew Morton <akpm@linux-foundation.org>\n@@ -15923,9 +15945,9 @@ F:\tmm/swapfile.c\n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\n M:\tAndrew Morton <akpm@linux-foundation.org>\n M:\tDavid Hildenbrand <david@redhat.com>\n+M:\tLorenzo Stoakes <lorenzo.stoakes@oracle.com>\n R:\tZi Yan <ziy@nvidia.com>\n R:\tBaolin Wang <baolin.wang@linux.alibaba.com>\n-R:\tLorenzo Stoakes <lorenzo.stoakes@oracle.com>\n R:\tLiam R. Howlett <Liam.Howlett@oracle.com>\n R:\tNico Pache <npache@redhat.com>\n R:\tRyan Roberts <ryan.roberts@arm.com>\ndiff --git a/fs/fuse/inode.c b/fs/fuse/inode.c\nindex bfe8d8af46f3..9572bdef49ee 100644\n--- a/fs/fuse/inode.c\n+++ b/fs/fuse/inode.c\n@@ -9,6 +9,7 @@\n #include \"fuse_i.h\"\n #include \"dev_uring_i.h\"\n \n+#include <linux/dax.h>\n #include <linux/pagemap.h>\n #include <linux/slab.h>\n #include <linux/file.h>\n@@ -162,6 +163,9 @@ static void fuse_evict_inode(struct inode *inode)\n \t/* Will write inode on close/munmap and in all other dirtiers */\n \tWARN_ON(inode->i_state & I_DIRTY_INODE);\n \n+\tif (FUSE_IS_DAX(inode))\n+\t\tdax_break_layout_final(inode);\n+\n \ttruncate_inode_pages_final(&inode->i_data);\n \tclear_inode(inode);\n \tif (inode->i_sb->s_flags & SB_ACTIVE) {\ndiff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c\nindex 27972c0749e7..4be91eb6ea5c 100644\n--- a/fs/proc/task_mmu.c\n+++ b/fs/proc/task_mmu.c\n@@ -2182,7 +2182,7 @@ static unsigned long pagemap_thp_category(struct pagemap_scan_private *p,\n \t\t\t\tcategories |= PAGE_IS_FILE;\n \t\t}\n \n-\t\tif (is_zero_pfn(pmd_pfn(pmd)))\n+\t\tif (is_huge_zero_pmd(pmd))\n \t\t\tcategories |= PAGE_IS_PFNZERO;\n \t\tif (pmd_soft_dirty(pmd))\n \t\t\tcategories |= PAGE_IS_SOFT_DIRTY;\ndiff --git a/include/linux/kmemleak.h b/include/linux/kmemleak.h\nindex 93a73c076d16..fbd424b2abb1 100644\n--- a/include/linux/kmemleak.h\n+++ b/include/linux/kmemleak.h\n@@ -28,6 +28,7 @@ extern void kmemleak_update_trace(const void *ptr) __ref;\n extern void kmemleak_not_leak(const void *ptr) __ref;\n extern void kmemleak_transient_leak(const void *ptr) __ref;\n extern void kmemleak_ignore(const void *ptr) __ref;\n+extern void kmemleak_ignore_percpu(const void __percpu *ptr) __ref;\n extern void kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp) __ref;\n extern void kmemleak_no_scan(const void *ptr) __ref;\n extern void kmemleak_alloc_phys(phys_addr_t phys, size_t size,\n@@ -97,6 +98,9 @@ static inline void kmemleak_not_leak(const void *ptr)\n static inline void kmemleak_transient_leak(const void *ptr)\n {\n }\n+static inline void kmemleak_ignore_percpu(const void __percpu *ptr)\n+{\n+}\n static inline void kmemleak_ignore(const void *ptr)\n {\n }\ndiff --git a/kernel/Kconfig.kexec b/kernel/Kconfig.kexec\nindex e64ce21f9a80..2ee603a98813 100644\n--- a/kernel/Kconfig.kexec\n+++ b/kernel/Kconfig.kexec\n@@ -134,6 +134,7 @@ config CRASH_DM_CRYPT\n \tdepends on KEXEC_FILE\n \tdepends on CRASH_DUMP\n \tdepends on DM_CRYPT\n+\tdepends on KEYS\n \thelp\n \t  With this option enabled, user space can intereact with\n \t  /sys/kernel/config/crash_dm_crypt_keys to make the dm crypt keys\ndiff --git a/lib/alloc_tag.c b/lib/alloc_tag.c\nindex d48b80f3f007..3a74d63a959e 100644\n--- a/lib/alloc_tag.c\n+++ b/lib/alloc_tag.c\n@@ -10,6 +10,7 @@\n #include <linux/seq_buf.h>\n #include <linux/seq_file.h>\n #include <linux/vmalloc.h>\n+#include <linux/kmemleak.h>\n \n #define ALLOCINFO_FILE_NAME\t\t\"allocinfo\"\n #define MODULE_ALLOC_TAG_VMAP_SIZE\t(100000UL * sizeof(struct alloc_tag))\n@@ -632,8 +633,13 @@ static int load_module(struct module *mod, struct codetag *start, struct codetag\n \t\t\t       mod->name);\n \t\t\treturn -ENOMEM;\n \t\t}\n-\t}\n \n+\t\t/*\n+\t\t * Avoid a kmemleak false positive. The pointer to the counters is stored\n+\t\t * in the alloc_tag section of the module and cannot be directly accessed.\n+\t\t */\n+\t\tkmemleak_ignore_percpu(tag->counters);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/lib/group_cpus.c b/lib/group_cpus.c\nindex ee272c4cefcc..18d43a406114 100644\n--- a/lib/group_cpus.c\n+++ b/lib/group_cpus.c\n@@ -352,6 +352,9 @@ struct cpumask *group_cpus_evenly(unsigned int numgrps)\n \tint ret = -ENOMEM;\n \tstruct cpumask *masks = NULL;\n \n+\tif (numgrps == 0)\n+\t\treturn NULL;\n+\n \tif (!zalloc_cpumask_var(&nmsk, GFP_KERNEL))\n \t\treturn NULL;\n \n@@ -426,8 +429,12 @@ struct cpumask *group_cpus_evenly(unsigned int numgrps)\n #else /* CONFIG_SMP */\n struct cpumask *group_cpus_evenly(unsigned int numgrps)\n {\n-\tstruct cpumask *masks = kcalloc(numgrps, sizeof(*masks), GFP_KERNEL);\n+\tstruct cpumask *masks;\n \n+\tif (numgrps == 0)\n+\t\treturn NULL;\n+\n+\tmasks = kcalloc(numgrps, sizeof(*masks), GFP_KERNEL);\n \tif (!masks)\n \t\treturn NULL;\n \ndiff --git a/mm/damon/sysfs-schemes.c b/mm/damon/sysfs-schemes.c\nindex 0f6c9e1fec0b..30ae7518ffbf 100644\n--- a/mm/damon/sysfs-schemes.c\n+++ b/mm/damon/sysfs-schemes.c\n@@ -472,6 +472,7 @@ static ssize_t memcg_path_store(struct kobject *kobj,\n \t\treturn -ENOMEM;\n \n \tstrscpy(path, buf, count + 1);\n+\tkfree(filter->memcg_path);\n \tfilter->memcg_path = path;\n \treturn count;\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 8746ed2fec13..9dc95eac558c 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -2787,20 +2787,24 @@ void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,\n /*\n  * alloc_and_dissolve_hugetlb_folio - Allocate a new folio and dissolve\n  * the old one\n- * @h: struct hstate old page belongs to\n  * @old_folio: Old folio to dissolve\n  * @list: List to isolate the page in case we need to\n  * Returns 0 on success, otherwise negated error.\n  */\n-static int alloc_and_dissolve_hugetlb_folio(struct hstate *h,\n-\t\t\tstruct folio *old_folio, struct list_head *list)\n+static int alloc_and_dissolve_hugetlb_folio(struct folio *old_folio,\n+\t\t\tstruct list_head *list)\n {\n-\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n+\tgfp_t gfp_mask;\n+\tstruct hstate *h;\n \tint nid = folio_nid(old_folio);\n \tstruct folio *new_folio = NULL;\n \tint ret = 0;\n \n retry:\n+\t/*\n+\t * The old_folio might have been dissolved from under our feet, so make sure\n+\t * to carefully check the state under the lock.\n+\t */\n \tspin_lock_irq(&hugetlb_lock);\n \tif (!folio_test_hugetlb(old_folio)) {\n \t\t/*\n@@ -2829,8 +2833,10 @@ static int alloc_and_dissolve_hugetlb_folio(struct hstate *h,\n \t\tcond_resched();\n \t\tgoto retry;\n \t} else {\n+\t\th = folio_hstate(old_folio);\n \t\tif (!new_folio) {\n \t\t\tspin_unlock_irq(&hugetlb_lock);\n+\t\t\tgfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n \t\t\tnew_folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid,\n \t\t\t\t\t\t\t      NULL, NULL);\n \t\t\tif (!new_folio)\n@@ -2874,35 +2880,24 @@ static int alloc_and_dissolve_hugetlb_folio(struct hstate *h,\n \n int isolate_or_dissolve_huge_folio(struct folio *folio, struct list_head *list)\n {\n-\tstruct hstate *h;\n \tint ret = -EBUSY;\n \n-\t/*\n-\t * The page might have been dissolved from under our feet, so make sure\n-\t * to carefully check the state under the lock.\n-\t * Return success when racing as if we dissolved the page ourselves.\n-\t */\n-\tspin_lock_irq(&hugetlb_lock);\n-\tif (folio_test_hugetlb(folio)) {\n-\t\th = folio_hstate(folio);\n-\t} else {\n-\t\tspin_unlock_irq(&hugetlb_lock);\n+\t/* Not to disrupt normal path by vainly holding hugetlb_lock */\n+\tif (!folio_test_hugetlb(folio))\n \t\treturn 0;\n-\t}\n-\tspin_unlock_irq(&hugetlb_lock);\n \n \t/*\n \t * Fence off gigantic pages as there is a cyclic dependency between\n \t * alloc_contig_range and them. Return -ENOMEM as this has the effect\n \t * of bailing out right away without further retrying.\n \t */\n-\tif (hstate_is_gigantic(h))\n+\tif (folio_order(folio) > MAX_PAGE_ORDER)\n \t\treturn -ENOMEM;\n \n \tif (folio_ref_count(folio) && folio_isolate_hugetlb(folio, list))\n \t\tret = 0;\n \telse if (!folio_ref_count(folio))\n-\t\tret = alloc_and_dissolve_hugetlb_folio(h, folio, list);\n+\t\tret = alloc_and_dissolve_hugetlb_folio(folio, list);\n \n \treturn ret;\n }\n@@ -2916,7 +2911,6 @@ int isolate_or_dissolve_huge_folio(struct folio *folio, struct list_head *list)\n  */\n int replace_free_hugepage_folios(unsigned long start_pfn, unsigned long end_pfn)\n {\n-\tstruct hstate *h;\n \tstruct folio *folio;\n \tint ret = 0;\n \n@@ -2925,23 +2919,9 @@ int replace_free_hugepage_folios(unsigned long start_pfn, unsigned long end_pfn)\n \twhile (start_pfn < end_pfn) {\n \t\tfolio = pfn_folio(start_pfn);\n \n-\t\t/*\n-\t\t * The folio might have been dissolved from under our feet, so make sure\n-\t\t * to carefully check the state under the lock.\n-\t\t */\n-\t\tspin_lock_irq(&hugetlb_lock);\n-\t\tif (folio_test_hugetlb(folio)) {\n-\t\t\th = folio_hstate(folio);\n-\t\t} else {\n-\t\t\tspin_unlock_irq(&hugetlb_lock);\n-\t\t\tstart_pfn++;\n-\t\t\tcontinue;\n-\t\t}\n-\t\tspin_unlock_irq(&hugetlb_lock);\n-\n-\t\tif (!folio_ref_count(folio)) {\n-\t\t\tret = alloc_and_dissolve_hugetlb_folio(h, folio,\n-\t\t\t\t\t\t\t       &isolate_list);\n+\t\t/* Not to disrupt normal path by vainly holding hugetlb_lock */\n+\t\tif (folio_test_hugetlb(folio) && !folio_ref_count(folio)) {\n+\t\t\tret = alloc_and_dissolve_hugetlb_folio(folio, &isolate_list);\n \t\t\tif (ret)\n \t\t\t\tbreak;\n \ndiff --git a/mm/kmemleak.c b/mm/kmemleak.c\nindex da9cee34ee1b..8d588e685311 100644\n--- a/mm/kmemleak.c\n+++ b/mm/kmemleak.c\n@@ -1246,6 +1246,20 @@ void __ref kmemleak_transient_leak(const void *ptr)\n }\n EXPORT_SYMBOL(kmemleak_transient_leak);\n \n+/**\n+ * kmemleak_ignore_percpu - similar to kmemleak_ignore but taking a percpu\n+ *\t\t\t    address argument\n+ * @ptr:\tpercpu address of the object\n+ */\n+void __ref kmemleak_ignore_percpu(const void __percpu *ptr)\n+{\n+\tpr_debug(\"%s(0x%px)\\n\", __func__, ptr);\n+\n+\tif (kmemleak_enabled && ptr && !IS_ERR_PCPU(ptr))\n+\t\tmake_black_object((unsigned long)ptr, OBJECT_PERCPU);\n+}\n+EXPORT_SYMBOL_GPL(kmemleak_ignore_percpu);\n+\n /**\n  * kmemleak_ignore - ignore an allocated object\n  * @ptr:\tpointer to beginning of the object\ndiff --git a/scripts/gdb/linux/vfs.py b/scripts/gdb/linux/vfs.py\nindex c77b9ce75f6d..b5fbb18ccb77 100644\n--- a/scripts/gdb/linux/vfs.py\n+++ b/scripts/gdb/linux/vfs.py\n@@ -22,7 +22,7 @@ def dentry_name(d):\n     if parent == d or parent == 0:\n         return \"\"\n     p = dentry_name(d['d_parent']) + \"/\"\n-    return p + d['d_iname'].string()\n+    return p + d['d_shortname']['string'].string()\n \n class DentryName(gdb.Function):\n     \"\"\"Return string of the full path of a dentry.\ndiff --git a/tools/testing/selftests/mm/virtual_address_range.c b/tools/testing/selftests/mm/virtual_address_range.c\nindex b380e102b22f..169dbd692bf5 100644\n--- a/tools/testing/selftests/mm/virtual_address_range.c\n+++ b/tools/testing/selftests/mm/virtual_address_range.c\n@@ -77,8 +77,11 @@ static void validate_addr(char *ptr, int high_addr)\n {\n \tunsigned long addr = (unsigned long) ptr;\n \n-\tif (high_addr && addr < HIGH_ADDR_MARK)\n-\t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);\n+\tif (high_addr) {\n+\t\tif (addr < HIGH_ADDR_MARK)\n+\t\t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);\n+\t\treturn;\n+\t}\n \n \tif (addr > HIGH_ADDR_MARK)\n \t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);",
    "stats": {
      "insertions": 90,
      "deletions": 44,
      "files": 13
    }
  },
  {
    "sha": "867b9987a30b7f68a6e9e89d3670730692222a4a",
    "message": "Merge tag 'riscv-for-linus-5.16-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux\n\nPull RISC-V Fixes for 5.16-rc4\n\n - .rodata is no longer linkd into PT_DYNAMIC.\n\n   It was not supposed to be there in the first place and resulted in\n   invalid (but unused) entries. This manifests as at least warnings in\n   llvm-readelf\n\n - A fix for runtime constants with all-0 upper 32-bits. This should\n   only manifest on MMU=n kernels\n\n - A fix for context save/restore on systems using the T-Head vector\n   extensions\n\n - A fix for a conflicting \"+r\"/\"r\" register constraint in the VDSO\n   getrandom syscall wrapper, which is undefined behavior in clang\n\n - A fix for a missing register clobber in the RVV raid6 implementation.\n\n   This manifests as a NULL pointer reference on some compilers, but\n   could trigger in other ways\n\n - Misaligned accesses from userspace at faulting addresses are now\n   handled correctly\n\n - A fix for an incorrect optimization that allowed access_ok() to mark\n   invalid addresses as accessible, which can result in userspace\n   triggering BUG()s\n\n - A few fixes for build warnings, and an update to Drew's email address\n\n* tag 'riscv-for-linus-5.16-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux:\n  riscv: export boot_cpu_hartid\n  Revert \"riscv: Define TASK_SIZE_MAX for __access_ok()\"\n  riscv: Fix sparse warning in vendor_extensions/sifive.c\n  Revert \"riscv: misaligned: fix sleeping function called during misaligned access handling\"\n  MAINTAINERS: Update Drew Fustini's email address\n  RISC-V: uaccess: Wrap the get_user_8 uaccess macro\n  raid6: riscv: Fix NULL pointer dereference caused by a missing clobber\n  RISC-V: vDSO: Correct inline assembly constraints in the getrandom syscall wrapper\n  riscv: vector: Fix context save/restore with xtheadvector\n  riscv: fix runtime constant support for nommu kernels\n  riscv: vdso: Exclude .rodata from the PT_DYNAMIC segment",
    "author": "Linus Torvalds",
    "date": "2025-06-27T20:22:18-07:00",
    "files_changed": [
      "arch/riscv/include/asm/pgtable.h",
      "arch/riscv/include/asm/runtime-const.h",
      "arch/riscv/include/asm/uaccess.h",
      "arch/riscv/include/asm/vdso/getrandom.h",
      "arch/riscv/include/asm/vector.h",
      "arch/riscv/kernel/setup.c",
      "arch/riscv/kernel/traps_misaligned.c",
      "arch/riscv/kernel/vendor_extensions/sifive.c",
      "lib/raid6/rvv.c"
    ],
    "diff": "diff --git a/.mailmap b/.mailmap\nindex d57531dab08b..93e94b0b9376 100644\n--- a/.mailmap\n+++ b/.mailmap\n@@ -223,6 +223,7 @@ Dmitry Safonov <0x7f454c46@gmail.com> <d.safonov@partner.samsung.com>\n Dmitry Safonov <0x7f454c46@gmail.com> <dsafonov@virtuozzo.com>\n Domen Puncer <domen@coderock.org>\n Douglas Gilbert <dougg@torque.net>\n+Drew Fustini <fustini@kernel.org> <drew@pdp7.com>\n Ed L. Cashin <ecashin@coraid.com>\n Elliot Berman <quic_eberman@quicinc.com> <eberman@codeaurora.org>\n Enric Balletbo i Serra <eballetbo@kernel.org> <enric.balletbo@collabora.com>\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex efb51ee92683..0fbfe93ba9f2 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -21388,7 +21388,7 @@ N:\tspacemit\n K:\tspacemit\n \n RISC-V THEAD SoC SUPPORT\n-M:\tDrew Fustini <drew@pdp7.com>\n+M:\tDrew Fustini <fustini@kernel.org>\n M:\tGuo Ren <guoren@kernel.org>\n M:\tFu Wei <wefu@redhat.com>\n L:\tlinux-riscv@lists.infradead.org\ndiff --git a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h\nindex 438ce7df24c3..5bd5aae60d53 100644\n--- a/arch/riscv/include/asm/pgtable.h\n+++ b/arch/riscv/include/asm/pgtable.h\n@@ -1075,7 +1075,6 @@ static inline pte_t pte_swp_clear_exclusive(pte_t pte)\n  */\n #ifdef CONFIG_64BIT\n #define TASK_SIZE_64\t(PGDIR_SIZE * PTRS_PER_PGD / 2)\n-#define TASK_SIZE_MAX\tLONG_MAX\n \n #ifdef CONFIG_COMPAT\n #define TASK_SIZE_32\t(_AC(0x80000000, UL) - PAGE_SIZE)\ndiff --git a/arch/riscv/include/asm/runtime-const.h b/arch/riscv/include/asm/runtime-const.h\nindex 451fd76b8811..d766e2b9e6df 100644\n--- a/arch/riscv/include/asm/runtime-const.h\n+++ b/arch/riscv/include/asm/runtime-const.h\n@@ -206,7 +206,7 @@ static inline void __runtime_fixup_32(__le16 *lui_parcel, __le16 *addi_parcel, u\n \t\taddi_insn_mask &= 0x07fff;\n \t}\n \n-\tif (lower_immediate & 0x00000fff) {\n+\tif (lower_immediate & 0x00000fff || lui_insn == RISCV_INSN_NOP4) {\n \t\t/* replace upper 12 bits of addi with lower 12 bits of val */\n \t\taddi_insn &= addi_insn_mask;\n \t\taddi_insn |= (lower_immediate & 0x00000fff) << 20;\ndiff --git a/arch/riscv/include/asm/uaccess.h b/arch/riscv/include/asm/uaccess.h\nindex d472da4450e6..525e50db24f7 100644\n--- a/arch/riscv/include/asm/uaccess.h\n+++ b/arch/riscv/include/asm/uaccess.h\n@@ -127,6 +127,7 @@ do {\t\t\t\t\t\t\t\t\\\n \n #ifdef CONFIG_CC_HAS_ASM_GOTO_OUTPUT\n #define __get_user_8(x, ptr, label)\t\t\t\t\\\n+do {\t\t\t\t\t\t\t\t\\\n \tu32 __user *__ptr = (u32 __user *)(ptr);\t\t\\\n \tu32 __lo, __hi;\t\t\t\t\t\t\\\n \tasm_goto_output(\t\t\t\t\t\\\n@@ -141,7 +142,7 @@ do {\t\t\t\t\t\t\t\t\\\n \t\t: : label);                                     \\\n \t(x) = (__typeof__(x))((__typeof__((x) - (x)))(\t\t\\\n \t\t(((u64)__hi << 32) | __lo)));\t\t\t\\\n-\n+} while (0)\n #else /* !CONFIG_CC_HAS_ASM_GOTO_OUTPUT */\n #define __get_user_8(x, ptr, label)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\ndiff --git a/arch/riscv/include/asm/vdso/getrandom.h b/arch/riscv/include/asm/vdso/getrandom.h\nindex 8dc92441702a..c6d66895c1f5 100644\n--- a/arch/riscv/include/asm/vdso/getrandom.h\n+++ b/arch/riscv/include/asm/vdso/getrandom.h\n@@ -18,7 +18,7 @@ static __always_inline ssize_t getrandom_syscall(void *_buffer, size_t _len, uns\n \tregister unsigned int flags asm(\"a2\") = _flags;\n \n \tasm volatile (\"ecall\\n\"\n-\t\t      : \"+r\" (ret)\n+\t\t      : \"=r\" (ret)\n \t\t      : \"r\" (nr), \"r\" (buffer), \"r\" (len), \"r\" (flags)\n \t\t      : \"memory\");\n \ndiff --git a/arch/riscv/include/asm/vector.h b/arch/riscv/include/asm/vector.h\nindex 45c9b426fcc5..b61786d43c20 100644\n--- a/arch/riscv/include/asm/vector.h\n+++ b/arch/riscv/include/asm/vector.h\n@@ -205,11 +205,11 @@ static inline void __riscv_v_vstate_save(struct __riscv_v_ext_state *save_to,\n \t\t\tTHEAD_VSETVLI_T4X0E8M8D1\n \t\t\tTHEAD_VSB_V_V0T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VSB_V_V0T0\n+\t\t\tTHEAD_VSB_V_V8T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VSB_V_V0T0\n+\t\t\tTHEAD_VSB_V_V16T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VSB_V_V0T0\n+\t\t\tTHEAD_VSB_V_V24T0\n \t\t\t: : \"r\" (datap) : \"memory\", \"t0\", \"t4\");\n \t} else {\n \t\tasm volatile (\n@@ -241,11 +241,11 @@ static inline void __riscv_v_vstate_restore(struct __riscv_v_ext_state *restore_\n \t\t\tTHEAD_VSETVLI_T4X0E8M8D1\n \t\t\tTHEAD_VLB_V_V0T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VLB_V_V0T0\n+\t\t\tTHEAD_VLB_V_V8T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VLB_V_V0T0\n+\t\t\tTHEAD_VLB_V_V16T0\n \t\t\t\"add\t\tt0, t0, t4\\n\\t\"\n-\t\t\tTHEAD_VLB_V_V0T0\n+\t\t\tTHEAD_VLB_V_V24T0\n \t\t\t: : \"r\" (datap) : \"memory\", \"t0\", \"t4\");\n \t} else {\n \t\tasm volatile (\ndiff --git a/arch/riscv/kernel/setup.c b/arch/riscv/kernel/setup.c\nindex f7c9a1caa83e..14888e5ea19a 100644\n--- a/arch/riscv/kernel/setup.c\n+++ b/arch/riscv/kernel/setup.c\n@@ -50,6 +50,7 @@ atomic_t hart_lottery __section(\".sdata\")\n #endif\n ;\n unsigned long boot_cpu_hartid;\n+EXPORT_SYMBOL_GPL(boot_cpu_hartid);\n \n /*\n  * Place kernel memory regions on the resource tree so that\ndiff --git a/arch/riscv/kernel/traps_misaligned.c b/arch/riscv/kernel/traps_misaligned.c\nindex dd8e4af6583f..93043924fe6c 100644\n--- a/arch/riscv/kernel/traps_misaligned.c\n+++ b/arch/riscv/kernel/traps_misaligned.c\n@@ -454,7 +454,7 @@ static int handle_scalar_misaligned_load(struct pt_regs *regs)\n \n \tval.data_u64 = 0;\n \tif (user_mode(regs)) {\n-\t\tif (copy_from_user_nofault(&val, (u8 __user *)addr, len))\n+\t\tif (copy_from_user(&val, (u8 __user *)addr, len))\n \t\t\treturn -1;\n \t} else {\n \t\tmemcpy(&val, (u8 *)addr, len);\n@@ -555,7 +555,7 @@ static int handle_scalar_misaligned_store(struct pt_regs *regs)\n \t\treturn -EOPNOTSUPP;\n \n \tif (user_mode(regs)) {\n-\t\tif (copy_to_user_nofault((u8 __user *)addr, &val, len))\n+\t\tif (copy_to_user((u8 __user *)addr, &val, len))\n \t\t\treturn -1;\n \t} else {\n \t\tmemcpy((u8 *)addr, &val, len);\ndiff --git a/arch/riscv/kernel/vdso/vdso.lds.S b/arch/riscv/kernel/vdso/vdso.lds.S\nindex 7c15b0f4ee3b..c29ef12a63bb 100644\n--- a/arch/riscv/kernel/vdso/vdso.lds.S\n+++ b/arch/riscv/kernel/vdso/vdso.lds.S\n@@ -30,7 +30,7 @@ SECTIONS\n \t\t*(.data .data.* .gnu.linkonce.d.*)\n \t\t*(.dynbss)\n \t\t*(.bss .bss.* .gnu.linkonce.b.*)\n-\t}\n+\t}\t\t\t\t\t\t:text\n \n \t.note\t\t: { *(.note.*) }\t\t:text\t:note\n \ndiff --git a/arch/riscv/kernel/vendor_extensions/sifive.c b/arch/riscv/kernel/vendor_extensions/sifive.c\nindex 1411337dc1e6..8fcf67e8c07f 100644\n--- a/arch/riscv/kernel/vendor_extensions/sifive.c\n+++ b/arch/riscv/kernel/vendor_extensions/sifive.c\n@@ -8,7 +8,7 @@\n #include <linux/types.h>\n \n /* All SiFive vendor extensions supported in Linux */\n-const struct riscv_isa_ext_data riscv_isa_vendor_ext_sifive[] = {\n+static const struct riscv_isa_ext_data riscv_isa_vendor_ext_sifive[] = {\n \t__RISCV_ISA_EXT_DATA(xsfvfnrclipxfqf, RISCV_ISA_VENDOR_EXT_XSFVFNRCLIPXFQF),\n \t__RISCV_ISA_EXT_DATA(xsfvfwmaccqqq, RISCV_ISA_VENDOR_EXT_XSFVFWMACCQQQ),\n \t__RISCV_ISA_EXT_DATA(xsfvqmaccdod, RISCV_ISA_VENDOR_EXT_XSFVQMACCDOD),\ndiff --git a/lib/raid6/rvv.c b/lib/raid6/rvv.c\nindex f0887344b274..7d82efa5b14f 100644\n--- a/lib/raid6/rvv.c\n+++ b/lib/raid6/rvv.c\n@@ -26,9 +26,9 @@ static int rvv_has_vector(void)\n static void raid6_rvv1_gen_syndrome_real(int disks, unsigned long bytes, void **ptrs)\n {\n \tu8 **dptr = (u8 **)ptrs;\n-\tunsigned long d;\n-\tint z, z0;\n \tu8 *p, *q;\n+\tunsigned long vl, d;\n+\tint z, z0;\n \n \tz0 = disks - 3;\t\t/* Highest data disk */\n \tp = dptr[z0 + 1];\t\t/* XOR parity */\n@@ -36,8 +36,9 @@ static void raid6_rvv1_gen_syndrome_real(int disks, unsigned long bytes, void **\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t /* v0:wp0, v1:wq0, v2:wd0/w20, v3:w10 */\n@@ -99,7 +100,7 @@ static void raid6_rvv1_xor_syndrome_real(int disks, int start, int stop,\n {\n \tu8 **dptr = (u8 **)ptrs;\n \tu8 *p, *q;\n-\tunsigned long d;\n+\tunsigned long vl, d;\n \tint z, z0;\n \n \tz0 = stop;\t\t/* P/Q right side optimization */\n@@ -108,8 +109,9 @@ static void raid6_rvv1_xor_syndrome_real(int disks, int start, int stop,\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/* v0:wp0, v1:wq0, v2:wd0/w20, v3:w10 */\n@@ -195,9 +197,9 @@ static void raid6_rvv1_xor_syndrome_real(int disks, int start, int stop,\n static void raid6_rvv2_gen_syndrome_real(int disks, unsigned long bytes, void **ptrs)\n {\n \tu8 **dptr = (u8 **)ptrs;\n-\tunsigned long d;\n-\tint z, z0;\n \tu8 *p, *q;\n+\tunsigned long vl, d;\n+\tint z, z0;\n \n \tz0 = disks - 3;\t\t/* Highest data disk */\n \tp = dptr[z0 + 1];\t\t/* XOR parity */\n@@ -205,8 +207,9 @@ static void raid6_rvv2_gen_syndrome_real(int disks, unsigned long bytes, void **\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*\n@@ -287,7 +290,7 @@ static void raid6_rvv2_xor_syndrome_real(int disks, int start, int stop,\n {\n \tu8 **dptr = (u8 **)ptrs;\n \tu8 *p, *q;\n-\tunsigned long d;\n+\tunsigned long vl, d;\n \tint z, z0;\n \n \tz0 = stop;\t\t/* P/Q right side optimization */\n@@ -296,8 +299,9 @@ static void raid6_rvv2_xor_syndrome_real(int disks, int start, int stop,\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*\n@@ -413,9 +417,9 @@ static void raid6_rvv2_xor_syndrome_real(int disks, int start, int stop,\n static void raid6_rvv4_gen_syndrome_real(int disks, unsigned long bytes, void **ptrs)\n {\n \tu8 **dptr = (u8 **)ptrs;\n-\tunsigned long d;\n-\tint z, z0;\n \tu8 *p, *q;\n+\tunsigned long vl, d;\n+\tint z, z0;\n \n \tz0 = disks - 3;\t/* Highest data disk */\n \tp = dptr[z0 + 1];\t/* XOR parity */\n@@ -423,8 +427,9 @@ static void raid6_rvv4_gen_syndrome_real(int disks, unsigned long bytes, void **\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*\n@@ -539,7 +544,7 @@ static void raid6_rvv4_xor_syndrome_real(int disks, int start, int stop,\n {\n \tu8 **dptr = (u8 **)ptrs;\n \tu8 *p, *q;\n-\tunsigned long d;\n+\tunsigned long vl, d;\n \tint z, z0;\n \n \tz0 = stop;\t\t/* P/Q right side optimization */\n@@ -548,8 +553,9 @@ static void raid6_rvv4_xor_syndrome_real(int disks, int start, int stop,\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*\n@@ -721,9 +727,9 @@ static void raid6_rvv4_xor_syndrome_real(int disks, int start, int stop,\n static void raid6_rvv8_gen_syndrome_real(int disks, unsigned long bytes, void **ptrs)\n {\n \tu8 **dptr = (u8 **)ptrs;\n-\tunsigned long d;\n-\tint z, z0;\n \tu8 *p, *q;\n+\tunsigned long vl, d;\n+\tint z, z0;\n \n \tz0 = disks - 3;\t/* Highest data disk */\n \tp = dptr[z0 + 1];\t/* XOR parity */\n@@ -731,8 +737,9 @@ static void raid6_rvv8_gen_syndrome_real(int disks, unsigned long bytes, void **\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*\n@@ -915,7 +922,7 @@ static void raid6_rvv8_xor_syndrome_real(int disks, int start, int stop,\n {\n \tu8 **dptr = (u8 **)ptrs;\n \tu8 *p, *q;\n-\tunsigned long d;\n+\tunsigned long vl, d;\n \tint z, z0;\n \n \tz0 = stop;\t\t/* P/Q right side optimization */\n@@ -924,8 +931,9 @@ static void raid6_rvv8_xor_syndrome_real(int disks, int start, int stop,\n \n \tasm volatile (\".option\tpush\\n\"\n \t\t      \".option\tarch,+v\\n\"\n-\t\t      \"vsetvli\tt0, x0, e8, m1, ta, ma\\n\"\n+\t\t      \"vsetvli\t%0, x0, e8, m1, ta, ma\\n\"\n \t\t      \".option\tpop\\n\"\n+\t\t      : \"=&r\" (vl)\n \t);\n \n \t/*",
    "stats": {
      "insertions": 45,
      "deletions": 35,
      "files": 12
    }
  }
]
[
  {
    "sha": "9a2b9416fd1d18d97ce1b737a11fcbc521140e5d",
    "message": "KVM: arm64: Fix error path in init_hyp_mode()\n\nIn the unlikely case pKVM failed to allocate carveout, the error path\ntries to access NULL ptr when it de-reference the SVE state from the\nuninitialized nVHE per-cpu base.\n\n[    1.575420] pstate: 61400005 (nZCv daif +PAN -UAO -TCO +DIT -SSBS BTYPE=--)\n[    1.576010] pc : teardown_hyp_mode+0xe4/0x180\n[    1.576920] lr : teardown_hyp_mode+0xd0/0x180\n[    1.577308] sp : ffff8000826fb9d0\n[    1.577600] x29: ffff8000826fb9d0 x28: 0000000000000000 x27: ffff80008209b000\n[    1.578383] x26: ffff800081dde000 x25: ffff8000820493c0 x24: ffff80008209eb00\n[    1.579180] x23: 0000000000000040 x22: 0000000000000001 x21: 0000000000000000\n[    1.579881] x20: 0000000000000002 x19: ffff800081d540b8 x18: 0000000000000000\n[    1.580544] x17: ffff800081205230 x16: 0000000000000152 x15: 00000000fffffff8\n[    1.581183] x14: 0000000000000008 x13: fff00000ff7f6880 x12: 000000000000003e\n[    1.581813] x11: 0000000000000002 x10: 00000000000000ff x9 : 0000000000000000\n[    1.582503] x8 : 0000000000000000 x7 : 7f7f7f7f7f7f7f7f x6 : 43485e525851ff30\n[    1.583140] x5 : fff00000ff6e9030 x4 : fff00000ff6e8f80 x3 : 0000000000000000\n[    1.583780] x2 : 0000000000000000 x1 : 0000000000000002 x0 : 0000000000000000\n[    1.584526] Call trace:\n[    1.584945]  teardown_hyp_mode+0xe4/0x180 (P)\n[    1.585578]  init_hyp_mode+0x920/0x994\n[    1.586005]  kvm_arm_init+0xb4/0x25c\n[    1.586387]  do_one_initcall+0xe0/0x258\n[    1.586819]  do_initcall_level+0xa0/0xd4\n[    1.587224]  do_initcalls+0x54/0x94\n[    1.587606]  do_basic_setup+0x1c/0x28\n[    1.587998]  kernel_init_freeable+0xc8/0x130\n[    1.588409]  kernel_init+0x20/0x1a4\n[    1.588768]  ret_from_fork+0x10/0x20\n[    1.589568] Code: f875db48 8b1c0109 f100011f 9a8903e8 (f9463100)\n[    1.590332] ---[ end trace 0000000000000000 ]---\n\nAs Quentin pointed, the order of free is also wrong, we need to free\nSVE state first before freeing the per CPU ptrs.\n\nI initially observed this on 6.12, but I could also repro in master.\n\nSigned-off-by: Mostafa Saleh <smostafa@google.com>\nFixes: 66d5b53e20a6 (\"KVM: arm64: Allocate memory mapped at hyp for host sve state in pKVM\")\nReviewed-by: Quentin Perret <qperret@google.com>\nLink: https://lore.kernel.org/r/20250625123058.875179-1-smostafa@google.com\nSigned-off-by: Marc Zyngier <maz@kernel.org>",
    "author": "Mostafa Saleh",
    "date": "2025-06-26T08:05:04+01:00",
    "files_changed": [
      "arch/arm64/kvm/arm.c"
    ],
    "diff": "diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c\nindex 38a91bb5d4c7..6bdf79bc5d95 100644\n--- a/arch/arm64/kvm/arm.c\n+++ b/arch/arm64/kvm/arm.c\n@@ -2346,7 +2346,9 @@ static void __init teardown_hyp_mode(void)\n \tfree_hyp_pgds();\n \tfor_each_possible_cpu(cpu) {\n \t\tfree_pages(per_cpu(kvm_arm_hyp_stack_base, cpu), NVHE_STACK_SHIFT - PAGE_SHIFT);\n-\t\tfree_pages(kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu], nvhe_percpu_order());\n+\n+\t\tif (!kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu])\n+\t\t\tcontinue;\n \n \t\tif (free_sve) {\n \t\t\tstruct cpu_sve_state *sve_state;\n@@ -2354,6 +2356,9 @@ static void __init teardown_hyp_mode(void)\n \t\t\tsve_state = per_cpu_ptr_nvhe_sym(kvm_host_data, cpu)->sve_state;\n \t\t\tfree_pages((unsigned long) sve_state, pkvm_host_sve_state_order());\n \t\t}\n+\n+\t\tfree_pages(kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu], nvhe_percpu_order());\n+\n \t}\n }\n ",
    "stats": {
      "insertions": 6,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "e728e705802fec20f65d974a5d5eb91217ac618d",
    "message": "KVM: arm64: Adjust range correctly during host stage-2 faults\n\nhost_stage2_adjust_range() tries to find the largest block mapping that\nfits within a memory or mmio region (represented by a kvm_mem_range in\nthis function) during host stage-2 faults under pKVM. To do so, it walks\nthe host stage-2 page-table, finds the faulting PTE and its level, and\nthen progressively increments the level until it finds a granule of the\nappropriate size. However, the condition in the loop implementing the\nabove is broken as it checks kvm_level_supports_block_mapping() for the\nnext level instead of the current, so pKVM may attempt to map a region\nlarger than can be covered with a single block.\n\nThis is not a security problem and is quite rare in practice (the\nkvm_mem_range check usually forces host_stage2_adjust_range() to choose a\nsmaller granule), but this is clearly not the expected behaviour.\n\nRefactor the loop to fix the bug and improve readability.\n\nFixes: c4f0935e4d95 (\"KVM: arm64: Optimize host memory aborts\")\nSigned-off-by: Quentin Perret <qperret@google.com>\nLink: https://lore.kernel.org/r/20250625105548.984572-1-qperret@google.com\nSigned-off-by: Marc Zyngier <maz@kernel.org>",
    "author": "Quentin Perret",
    "date": "2025-06-26T08:04:43+01:00",
    "files_changed": [
      "arch/arm64/kvm/hyp/nvhe/mem_protect.c"
    ],
    "diff": "diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c\nindex 95d7534c9679..8957734d6183 100644\n--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c\n+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c\n@@ -479,6 +479,7 @@ static int host_stage2_adjust_range(u64 addr, struct kvm_mem_range *range)\n {\n \tstruct kvm_mem_range cur;\n \tkvm_pte_t pte;\n+\tu64 granule;\n \ts8 level;\n \tint ret;\n \n@@ -496,18 +497,21 @@ static int host_stage2_adjust_range(u64 addr, struct kvm_mem_range *range)\n \t\treturn -EPERM;\n \t}\n \n-\tdo {\n-\t\tu64 granule = kvm_granule_size(level);\n+\tfor (; level <= KVM_PGTABLE_LAST_LEVEL; level++) {\n+\t\tif (!kvm_level_supports_block_mapping(level))\n+\t\t\tcontinue;\n+\t\tgranule = kvm_granule_size(level);\n \t\tcur.start = ALIGN_DOWN(addr, granule);\n \t\tcur.end = cur.start + granule;\n-\t\tlevel++;\n-\t} while ((level <= KVM_PGTABLE_LAST_LEVEL) &&\n-\t\t\t!(kvm_level_supports_block_mapping(level) &&\n-\t\t\t  range_included(&cur, range)));\n+\t\tif (!range_included(&cur, range))\n+\t\t\tcontinue;\n+\t\t*range = cur;\n+\t\treturn 0;\n+\t}\n \n-\t*range = cur;\n+\tWARN_ON(1);\n \n-\treturn 0;\n+\treturn -EINVAL;\n }\n \n int host_stage2_idmap_locked(phys_addr_t addr, u64 size,",
    "stats": {
      "insertions": 12,
      "deletions": 8,
      "files": 1
    }
  },
  {
    "sha": "ee88bddf7f2f5d1f1da87dd7bedc734048b70e88",
    "message": "Merge tag 'bpf-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf\n\nPull bpf fixes from Alexei Starovoitov:\n\n - Fix use-after-free in libbpf when map is resized (Adin Scannell)\n\n - Fix verifier assumptions about 2nd argument of bpf_sysctl_get_name\n   (Jerome Marchand)\n\n - Fix verifier assumption of nullness of d_inode in dentry (Song Liu)\n\n - Fix global starvation of LRU map (Willem de Bruijn)\n\n - Fix potential NULL dereference in btf_dump__free (Yuan Chen)\n\n* tag 'bpf-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf:\n  selftests/bpf: adapt one more case in test_lru_map to the new target_free\n  libbpf: Fix possible use-after-free for externs\n  selftests/bpf: Convert test_sysctl to prog_tests\n  bpf: Specify access type of bpf_sysctl_get_name args\n  libbpf: Fix null pointer dereference in btf_dump__free on allocation failure\n  bpf: Adjust free target to avoid global starvation of LRU map\n  bpf: Mark dentry->d_inode as trusted_or_null",
    "author": "Linus Torvalds",
    "date": "2025-06-25T21:09:02-07:00",
    "files_changed": [
      "kernel/bpf/bpf_lru_list.c",
      "kernel/bpf/bpf_lru_list.h",
      "kernel/bpf/cgroup.c",
      "kernel/bpf/verifier.c",
      "tools/lib/bpf/btf_dump.c",
      "tools/lib/bpf/libbpf.c",
      "tools/testing/selftests/bpf/prog_tests/test_sysctl.c",
      "tools/testing/selftests/bpf/progs/test_global_map_resize.c",
      "tools/testing/selftests/bpf/progs/verifier_vfs_accept.c",
      "tools/testing/selftests/bpf/progs/verifier_vfs_reject.c",
      "tools/testing/selftests/bpf/test_lru_map.c",
      "tools/testing/selftests/bpf/test_sysctl.c"
    ],
    "diff": "diff --git a/Documentation/bpf/map_hash.rst b/Documentation/bpf/map_hash.rst\nindex d2343952f2cb..8606bf958a8c 100644\n--- a/Documentation/bpf/map_hash.rst\n+++ b/Documentation/bpf/map_hash.rst\n@@ -233,10 +233,16 @@ attempts in order to enforce the LRU property which have increasing impacts on\n other CPUs involved in the following operation attempts:\n \n - Attempt to use CPU-local state to batch operations\n-- Attempt to fetch free nodes from global lists\n+- Attempt to fetch ``target_free`` free nodes from global lists\n - Attempt to pull any node from a global list and remove it from the hashmap\n - Attempt to pull any node from any CPU's list and remove it from the hashmap\n \n+The number of nodes to borrow from the global list in a batch, ``target_free``,\n+depends on the size of the map. Larger batch size reduces lock contention, but\n+may also exhaust the global structure. The value is computed at map init to\n+avoid exhaustion, by limiting aggregate reservation by all CPUs to half the map\n+size. With a minimum of a single element and maximum budget of 128 at a time.\n+\n This algorithm is described visually in the following diagram. See the\n description in commit 3a08c2fd7634 (\"bpf: LRU List\") for a full explanation of\n the corresponding operations:\ndiff --git a/Documentation/bpf/map_lru_hash_update.dot b/Documentation/bpf/map_lru_hash_update.dot\nindex a0fee349d29c..ab10058f5b79 100644\n--- a/Documentation/bpf/map_lru_hash_update.dot\n+++ b/Documentation/bpf/map_lru_hash_update.dot\n@@ -35,18 +35,18 @@ digraph {\n   fn_bpf_lru_list_pop_free_to_local [shape=rectangle,fillcolor=2,\n     label=\"Flush local pending,\n     Rotate Global list, move\n-    LOCAL_FREE_TARGET\n+    target_free\n     from global -> local\"]\n   // Also corresponds to:\n   // fn__local_list_flush()\n   // fn_bpf_lru_list_rotate()\n   fn___bpf_lru_node_move_to_free[shape=diamond,fillcolor=2,\n-    label=\"Able to free\\nLOCAL_FREE_TARGET\\nnodes?\"]\n+    label=\"Able to free\\ntarget_free\\nnodes?\"]\n \n   fn___bpf_lru_list_shrink_inactive [shape=rectangle,fillcolor=3,\n     label=\"Shrink inactive list\n       up to remaining\n-      LOCAL_FREE_TARGET\n+      target_free\n       (global LRU -> local)\"]\n   fn___bpf_lru_list_shrink [shape=diamond,fillcolor=2,\n     label=\"> 0 entries in\\nlocal free list?\"]\ndiff --git a/kernel/bpf/bpf_lru_list.c b/kernel/bpf/bpf_lru_list.c\nindex 3dabdd137d10..2d6e1c98d8ad 100644\n--- a/kernel/bpf/bpf_lru_list.c\n+++ b/kernel/bpf/bpf_lru_list.c\n@@ -337,12 +337,12 @@ static void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,\n \t\t\t\t list) {\n \t\t__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),\n \t\t\t\t\t    BPF_LRU_LOCAL_LIST_T_FREE);\n-\t\tif (++nfree == LOCAL_FREE_TARGET)\n+\t\tif (++nfree == lru->target_free)\n \t\t\tbreak;\n \t}\n \n-\tif (nfree < LOCAL_FREE_TARGET)\n-\t\t__bpf_lru_list_shrink(lru, l, LOCAL_FREE_TARGET - nfree,\n+\tif (nfree < lru->target_free)\n+\t\t__bpf_lru_list_shrink(lru, l, lru->target_free - nfree,\n \t\t\t\t      local_free_list(loc_l),\n \t\t\t\t      BPF_LRU_LOCAL_LIST_T_FREE);\n \n@@ -577,6 +577,9 @@ static void bpf_common_lru_populate(struct bpf_lru *lru, void *buf,\n \t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);\n \t\tbuf += elem_size;\n \t}\n+\n+\tlru->target_free = clamp((nr_elems / num_possible_cpus()) / 2,\n+\t\t\t\t 1, LOCAL_FREE_TARGET);\n }\n \n static void bpf_percpu_lru_populate(struct bpf_lru *lru, void *buf,\ndiff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h\nindex cbd8d3720c2b..fe2661a58ea9 100644\n--- a/kernel/bpf/bpf_lru_list.h\n+++ b/kernel/bpf/bpf_lru_list.h\n@@ -58,6 +58,7 @@ struct bpf_lru {\n \tdel_from_htab_func del_from_htab;\n \tvoid *del_arg;\n \tunsigned int hash_offset;\n+\tunsigned int target_free;\n \tunsigned int nr_scans;\n \tbool percpu;\n };\ndiff --git a/kernel/bpf/cgroup.c b/kernel/bpf/cgroup.c\nindex 9122c39870bf..f4885514f007 100644\n--- a/kernel/bpf/cgroup.c\n+++ b/kernel/bpf/cgroup.c\n@@ -2134,7 +2134,7 @@ static const struct bpf_func_proto bpf_sysctl_get_name_proto = {\n \t.gpl_only\t= false,\n \t.ret_type\t= RET_INTEGER,\n \t.arg1_type\t= ARG_PTR_TO_CTX,\n-\t.arg2_type\t= ARG_PTR_TO_MEM,\n+\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_WRITE,\n \t.arg3_type\t= ARG_CONST_SIZE,\n \t.arg4_type\t= ARG_ANYTHING,\n };\ndiff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c\nindex a7d6e0c5928b..169845710c7e 100644\n--- a/kernel/bpf/verifier.c\n+++ b/kernel/bpf/verifier.c\n@@ -7027,8 +7027,7 @@ BTF_TYPE_SAFE_TRUSTED(struct file) {\n \tstruct inode *f_inode;\n };\n \n-BTF_TYPE_SAFE_TRUSTED(struct dentry) {\n-\t/* no negative dentry-s in places where bpf can see it */\n+BTF_TYPE_SAFE_TRUSTED_OR_NULL(struct dentry) {\n \tstruct inode *d_inode;\n };\n \n@@ -7066,7 +7065,6 @@ static bool type_is_trusted(struct bpf_verifier_env *env,\n \tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct bpf_iter__task));\n \tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct linux_binprm));\n \tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct file));\n-\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct dentry));\n \n \treturn btf_nested_type_is_trusted(&env->log, reg, field_name, btf_id, \"__safe_trusted\");\n }\n@@ -7076,6 +7074,7 @@ static bool type_is_trusted_or_null(struct bpf_verifier_env *env,\n \t\t\t\t    const char *field_name, u32 btf_id)\n {\n \tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED_OR_NULL(struct socket));\n+\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED_OR_NULL(struct dentry));\n \n \treturn btf_nested_type_is_trusted(&env->log, reg, field_name, btf_id,\n \t\t\t\t\t  \"__safe_trusted_or_null\");\ndiff --git a/tools/lib/bpf/btf_dump.c b/tools/lib/bpf/btf_dump.c\nindex 460c3e57fadb..0381f209920a 100644\n--- a/tools/lib/bpf/btf_dump.c\n+++ b/tools/lib/bpf/btf_dump.c\n@@ -226,6 +226,9 @@ static void btf_dump_free_names(struct hashmap *map)\n \tsize_t bkt;\n \tstruct hashmap_entry *cur;\n \n+\tif (!map)\n+\t\treturn;\n+\n \thashmap__for_each_entry(map, cur, bkt)\n \t\tfree((void *)cur->pkey);\n \ndiff --git a/tools/lib/bpf/libbpf.c b/tools/lib/bpf/libbpf.c\nindex e9c641a2fb20..52e353368f58 100644\n--- a/tools/lib/bpf/libbpf.c\n+++ b/tools/lib/bpf/libbpf.c\n@@ -597,7 +597,7 @@ struct extern_desc {\n \tint sym_idx;\n \tint btf_id;\n \tint sec_btf_id;\n-\tconst char *name;\n+\tchar *name;\n \tchar *essent_name;\n \tbool is_set;\n \tbool is_weak;\n@@ -4259,7 +4259,9 @@ static int bpf_object__collect_externs(struct bpf_object *obj)\n \t\t\treturn ext->btf_id;\n \t\t}\n \t\tt = btf__type_by_id(obj->btf, ext->btf_id);\n-\t\text->name = btf__name_by_offset(obj->btf, t->name_off);\n+\t\text->name = strdup(btf__name_by_offset(obj->btf, t->name_off));\n+\t\tif (!ext->name)\n+\t\t\treturn -ENOMEM;\n \t\text->sym_idx = i;\n \t\text->is_weak = ELF64_ST_BIND(sym->st_info) == STB_WEAK;\n \n@@ -9138,8 +9140,10 @@ void bpf_object__close(struct bpf_object *obj)\n \tzfree(&obj->btf_custom_path);\n \tzfree(&obj->kconfig);\n \n-\tfor (i = 0; i < obj->nr_extern; i++)\n+\tfor (i = 0; i < obj->nr_extern; i++) {\n+\t\tzfree(&obj->externs[i].name);\n \t\tzfree(&obj->externs[i].essent_name);\n+\t}\n \n \tzfree(&obj->externs);\n \tobj->nr_extern = 0;\ndiff --git a/tools/testing/selftests/bpf/.gitignore b/tools/testing/selftests/bpf/.gitignore\nindex e2a2c46c008b..3d8378972d26 100644\n--- a/tools/testing/selftests/bpf/.gitignore\n+++ b/tools/testing/selftests/bpf/.gitignore\n@@ -21,7 +21,6 @@ test_lirc_mode2_user\n flow_dissector_load\n test_tcpnotify_user\n test_libbpf\n-test_sysctl\n xdping\n test_cpp\n *.d\ndiff --git a/tools/testing/selftests/bpf/Makefile b/tools/testing/selftests/bpf/Makefile\nindex cf5ed3bee573..910d8d6402ef 100644\n--- a/tools/testing/selftests/bpf/Makefile\n+++ b/tools/testing/selftests/bpf/Makefile\n@@ -73,7 +73,7 @@ endif\n # Order correspond to 'make run_tests' order\n TEST_GEN_PROGS = test_verifier test_tag test_maps test_lru_map test_progs \\\n \ttest_sockmap \\\n-\ttest_tcpnotify_user test_sysctl \\\n+\ttest_tcpnotify_user \\\n \ttest_progs-no_alu32\n TEST_INST_SUBDIRS := no_alu32\n \n@@ -220,7 +220,7 @@ ifeq ($(VMLINUX_BTF),)\n $(error Cannot find a vmlinux for VMLINUX_BTF at any of \"$(VMLINUX_BTF_PATHS)\")\n endif\n \n-# Define simple and short `make test_progs`, `make test_sysctl`, etc targets\n+# Define simple and short `make test_progs`, `make test_maps`, etc targets\n # to build individual tests.\n # NOTE: Semicolon at the end is critical to override lib.mk's default static\n # rule for binaries.\n@@ -329,7 +329,6 @@ NETWORK_HELPERS := $(OUTPUT)/network_helpers.o\n $(OUTPUT)/test_sockmap: $(CGROUP_HELPERS) $(TESTING_HELPERS)\n $(OUTPUT)/test_tcpnotify_user: $(CGROUP_HELPERS) $(TESTING_HELPERS) $(TRACE_HELPERS)\n $(OUTPUT)/test_sock_fields: $(CGROUP_HELPERS) $(TESTING_HELPERS)\n-$(OUTPUT)/test_sysctl: $(CGROUP_HELPERS) $(TESTING_HELPERS)\n $(OUTPUT)/test_tag: $(TESTING_HELPERS)\n $(OUTPUT)/test_lirc_mode2_user: $(TESTING_HELPERS)\n $(OUTPUT)/xdping: $(TESTING_HELPERS)\ndiff --git a/tools/testing/selftests/bpf/test_sysctl.c b/tools/testing/selftests/bpf/prog_tests/test_sysctl.c\nsimilarity index 98%\nrename from tools/testing/selftests/bpf/test_sysctl.c\nrename to tools/testing/selftests/bpf/prog_tests/test_sysctl.c\nindex bcdbd27f22f0..273dd41ca09e 100644\n--- a/tools/testing/selftests/bpf/test_sysctl.c\n+++ b/tools/testing/selftests/bpf/prog_tests/test_sysctl.c\n@@ -1,22 +1,8 @@\n // SPDX-License-Identifier: GPL-2.0\n // Copyright (c) 2019 Facebook\n \n-#include <fcntl.h>\n-#include <stdint.h>\n-#include <stdio.h>\n-#include <stdlib.h>\n-#include <string.h>\n-#include <unistd.h>\n-\n-#include <linux/filter.h>\n-\n-#include <bpf/bpf.h>\n-#include <bpf/libbpf.h>\n-\n-#include <bpf/bpf_endian.h>\n-#include \"bpf_util.h\"\n+#include \"test_progs.h\"\n #include \"cgroup_helpers.h\"\n-#include \"testing_helpers.h\"\n \n #define CG_PATH\t\t\t\"/foo\"\n #define MAX_INSNS\t\t512\n@@ -1608,26 +1594,19 @@ static int run_tests(int cgfd)\n \treturn fails ? -1 : 0;\n }\n \n-int main(int argc, char **argv)\n+void test_sysctl(void)\n {\n-\tint cgfd = -1;\n-\tint err = 0;\n+\tint cgfd;\n \n \tcgfd = cgroup_setup_and_join(CG_PATH);\n-\tif (cgfd < 0)\n-\t\tgoto err;\n+\tif (!ASSERT_OK_FD(cgfd < 0, \"create_cgroup\"))\n+\t\tgoto out;\n \n-\t/* Use libbpf 1.0 API mode */\n-\tlibbpf_set_strict_mode(LIBBPF_STRICT_ALL);\n+\tif (!ASSERT_OK(run_tests(cgfd), \"run_tests\"))\n+\t\tgoto out;\n \n-\tif (run_tests(cgfd))\n-\t\tgoto err;\n-\n-\tgoto out;\n-err:\n-\terr = -1;\n out:\n \tclose(cgfd);\n \tcleanup_cgroup_environment();\n-\treturn err;\n+\treturn;\n }\ndiff --git a/tools/testing/selftests/bpf/progs/test_global_map_resize.c b/tools/testing/selftests/bpf/progs/test_global_map_resize.c\nindex a3f220ba7025..ee65bad0436d 100644\n--- a/tools/testing/selftests/bpf/progs/test_global_map_resize.c\n+++ b/tools/testing/selftests/bpf/progs/test_global_map_resize.c\n@@ -32,6 +32,16 @@ int my_int_last SEC(\".data.array_not_last\");\n \n int percpu_arr[1] SEC(\".data.percpu_arr\");\n \n+/* at least one extern is included, to ensure that a specific\n+ * regression is tested whereby resizing resulted in a free-after-use\n+ * bug after type information is invalidated by the resize operation.\n+ *\n+ * There isn't a particularly good API to test for this specific condition,\n+ * but by having externs for the resizing tests it will cover this path.\n+ */\n+extern int LINUX_KERNEL_VERSION __kconfig;\n+long version_sink;\n+\n SEC(\"tp/syscalls/sys_enter_getpid\")\n int bss_array_sum(void *ctx)\n {\n@@ -44,6 +54,9 @@ int bss_array_sum(void *ctx)\n \tfor (size_t i = 0; i < bss_array_len; ++i)\n \t\tsum += array[i];\n \n+\t/* see above; ensure this is not optimized out */\n+\tversion_sink = LINUX_KERNEL_VERSION;\n+\n \treturn 0;\n }\n \n@@ -59,6 +72,9 @@ int data_array_sum(void *ctx)\n \tfor (size_t i = 0; i < data_array_len; ++i)\n \t\tsum += my_array[i];\n \n+\t/* see above; ensure this is not optimized out */\n+\tversion_sink = LINUX_KERNEL_VERSION;\n+\n \treturn 0;\n }\n \ndiff --git a/tools/testing/selftests/bpf/progs/verifier_vfs_accept.c b/tools/testing/selftests/bpf/progs/verifier_vfs_accept.c\nindex a7c0a553aa50..3e2d76ee8050 100644\n--- a/tools/testing/selftests/bpf/progs/verifier_vfs_accept.c\n+++ b/tools/testing/selftests/bpf/progs/verifier_vfs_accept.c\n@@ -2,6 +2,7 @@\n /* Copyright (c) 2024 Google LLC. */\n \n #include <vmlinux.h>\n+#include <errno.h>\n #include <bpf/bpf_helpers.h>\n #include <bpf/bpf_tracing.h>\n \n@@ -82,4 +83,21 @@ int BPF_PROG(path_d_path_from_file_argument, struct file *file)\n \treturn 0;\n }\n \n+SEC(\"lsm.s/inode_rename\")\n+__success\n+int BPF_PROG(inode_rename, struct inode *old_dir, struct dentry *old_dentry,\n+\t     struct inode *new_dir, struct dentry *new_dentry,\n+\t     unsigned int flags)\n+{\n+\tstruct inode *inode = new_dentry->d_inode;\n+\tino_t ino;\n+\n+\tif (!inode)\n+\t\treturn 0;\n+\tino = inode->i_ino;\n+\tif (ino == 0)\n+\t\treturn -EACCES;\n+\treturn 0;\n+}\n+\n char _license[] SEC(\"license\") = \"GPL\";\ndiff --git a/tools/testing/selftests/bpf/progs/verifier_vfs_reject.c b/tools/testing/selftests/bpf/progs/verifier_vfs_reject.c\nindex d6d3f4fcb24c..4b392c6c8fc4 100644\n--- a/tools/testing/selftests/bpf/progs/verifier_vfs_reject.c\n+++ b/tools/testing/selftests/bpf/progs/verifier_vfs_reject.c\n@@ -2,6 +2,7 @@\n /* Copyright (c) 2024 Google LLC. */\n \n #include <vmlinux.h>\n+#include <errno.h>\n #include <bpf/bpf_helpers.h>\n #include <bpf/bpf_tracing.h>\n #include <linux/limits.h>\n@@ -158,4 +159,18 @@ int BPF_PROG(path_d_path_kfunc_non_lsm, struct path *path, struct file *f)\n \treturn 0;\n }\n \n+SEC(\"lsm.s/inode_rename\")\n+__failure __msg(\"invalid mem access 'trusted_ptr_or_null_'\")\n+int BPF_PROG(inode_rename, struct inode *old_dir, struct dentry *old_dentry,\n+\t     struct inode *new_dir, struct dentry *new_dentry,\n+\t     unsigned int flags)\n+{\n+\tstruct inode *inode = new_dentry->d_inode;\n+\tino_t ino;\n+\n+\tino = inode->i_ino;\n+\tif (ino == 0)\n+\t\treturn -EACCES;\n+\treturn 0;\n+}\n char _license[] SEC(\"license\") = \"GPL\";\ndiff --git a/tools/testing/selftests/bpf/test_lru_map.c b/tools/testing/selftests/bpf/test_lru_map.c\nindex fda7589c5023..0921939532c6 100644\n--- a/tools/testing/selftests/bpf/test_lru_map.c\n+++ b/tools/testing/selftests/bpf/test_lru_map.c\n@@ -138,6 +138,18 @@ static int sched_next_online(int pid, int *next_to_try)\n \treturn ret;\n }\n \n+/* Derive target_free from map_size, same as bpf_common_lru_populate */\n+static unsigned int __tgt_size(unsigned int map_size)\n+{\n+\treturn (map_size / nr_cpus) / 2;\n+}\n+\n+/* Inverse of how bpf_common_lru_populate derives target_free from map_size. */\n+static unsigned int __map_size(unsigned int tgt_free)\n+{\n+\treturn tgt_free * nr_cpus * 2;\n+}\n+\n /* Size of the LRU map is 2\n  * Add key=1 (+1 key)\n  * Add key=2 (+1 key)\n@@ -231,11 +243,11 @@ static void test_lru_sanity0(int map_type, int map_flags)\n \tprintf(\"Pass\\n\");\n }\n \n-/* Size of the LRU map is 1.5*tgt_free\n- * Insert 1 to tgt_free (+tgt_free keys)\n- * Lookup 1 to tgt_free/2\n- * Insert 1+tgt_free to 2*tgt_free (+tgt_free keys)\n- * => 1+tgt_free/2 to LOCALFREE_TARGET will be removed by LRU\n+/* Verify that unreferenced elements are recycled before referenced ones.\n+ * Insert elements.\n+ * Reference a subset of these.\n+ * Insert more, enough to trigger recycling.\n+ * Verify that unreferenced are recycled.\n  */\n static void test_lru_sanity1(int map_type, int map_flags, unsigned int tgt_free)\n {\n@@ -257,7 +269,7 @@ static void test_lru_sanity1(int map_type, int map_flags, unsigned int tgt_free)\n \tbatch_size = tgt_free / 2;\n \tassert(batch_size * 2 == tgt_free);\n \n-\tmap_size = tgt_free + batch_size;\n+\tmap_size = __map_size(tgt_free) + batch_size;\n \tlru_map_fd = create_map(map_type, map_flags, map_size);\n \tassert(lru_map_fd != -1);\n \n@@ -266,13 +278,13 @@ static void test_lru_sanity1(int map_type, int map_flags, unsigned int tgt_free)\n \n \tvalue[0] = 1234;\n \n-\t/* Insert 1 to tgt_free (+tgt_free keys) */\n-\tend_key = 1 + tgt_free;\n+\t/* Insert map_size - batch_size keys */\n+\tend_key = 1 + __map_size(tgt_free);\n \tfor (key = 1; key < end_key; key++)\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n \n-\t/* Lookup 1 to tgt_free/2 */\n+\t/* Lookup 1 to batch_size */\n \tend_key = 1 + batch_size;\n \tfor (key = 1; key < end_key; key++) {\n \t\tassert(!bpf_map_lookup_elem_with_ref_bit(lru_map_fd, key, value));\n@@ -280,12 +292,13 @@ static void test_lru_sanity1(int map_type, int map_flags, unsigned int tgt_free)\n \t\t\t\t\t    BPF_NOEXIST));\n \t}\n \n-\t/* Insert 1+tgt_free to 2*tgt_free\n-\t * => 1+tgt_free/2 to LOCALFREE_TARGET will be\n+\t/* Insert another map_size - batch_size keys\n+\t * Map will contain 1 to batch_size plus these latest, i.e.,\n+\t * => previous 1+batch_size to map_size - batch_size will have been\n \t * removed by LRU\n \t */\n-\tkey = 1 + tgt_free;\n-\tend_key = key + tgt_free;\n+\tkey = 1 + __map_size(tgt_free);\n+\tend_key = key + __map_size(tgt_free);\n \tfor (; key < end_key; key++) {\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n@@ -301,17 +314,8 @@ static void test_lru_sanity1(int map_type, int map_flags, unsigned int tgt_free)\n \tprintf(\"Pass\\n\");\n }\n \n-/* Size of the LRU map 1.5 * tgt_free\n- * Insert 1 to tgt_free (+tgt_free keys)\n- * Update 1 to tgt_free/2\n- *   => The original 1 to tgt_free/2 will be removed due to\n- *      the LRU shrink process\n- * Re-insert 1 to tgt_free/2 again and do a lookup immeidately\n- * Insert 1+tgt_free to tgt_free*3/2\n- * Insert 1+tgt_free*3/2 to tgt_free*5/2\n- *   => Key 1+tgt_free to tgt_free*3/2\n- *      will be removed from LRU because it has never\n- *      been lookup and ref bit is not set\n+/* Verify that insertions exceeding map size will recycle the oldest.\n+ * Verify that unreferenced elements are recycled before referenced.\n  */\n static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n {\n@@ -334,7 +338,7 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \tbatch_size = tgt_free / 2;\n \tassert(batch_size * 2 == tgt_free);\n \n-\tmap_size = tgt_free + batch_size;\n+\tmap_size = __map_size(tgt_free) + batch_size;\n \tlru_map_fd = create_map(map_type, map_flags, map_size);\n \tassert(lru_map_fd != -1);\n \n@@ -343,8 +347,8 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \n \tvalue[0] = 1234;\n \n-\t/* Insert 1 to tgt_free (+tgt_free keys) */\n-\tend_key = 1 + tgt_free;\n+\t/* Insert map_size - batch_size keys */\n+\tend_key = 1 + __map_size(tgt_free);\n \tfor (key = 1; key < end_key; key++)\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n@@ -357,8 +361,7 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \t * shrink the inactive list to get tgt_free\n \t * number of free nodes.\n \t *\n-\t * Hence, the oldest key 1 to tgt_free/2\n-\t * are removed from the LRU list.\n+\t * Hence, the oldest key is removed from the LRU list.\n \t */\n \tkey = 1;\n \tif (map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n@@ -370,8 +373,7 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \t\t\t\t\t   BPF_EXIST));\n \t}\n \n-\t/* Re-insert 1 to tgt_free/2 again and do a lookup\n-\t * immeidately.\n+\t/* Re-insert 1 to batch_size again and do a lookup immediately.\n \t */\n \tend_key = 1 + batch_size;\n \tvalue[0] = 4321;\n@@ -387,17 +389,18 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \n \tvalue[0] = 1234;\n \n-\t/* Insert 1+tgt_free to tgt_free*3/2 */\n-\tend_key = 1 + tgt_free + batch_size;\n-\tfor (key = 1 + tgt_free; key < end_key; key++)\n+\t/* Insert batch_size new elements */\n+\tkey = 1 + __map_size(tgt_free);\n+\tend_key = key + batch_size;\n+\tfor (; key < end_key; key++)\n \t\t/* These newly added but not referenced keys will be\n \t\t * gone during the next LRU shrink.\n \t\t */\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n \n-\t/* Insert 1+tgt_free*3/2 to  tgt_free*5/2 */\n-\tend_key = key + tgt_free;\n+\t/* Insert map_size - batch_size elements */\n+\tend_key += __map_size(tgt_free);\n \tfor (; key < end_key; key++) {\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n@@ -413,12 +416,12 @@ static void test_lru_sanity2(int map_type, int map_flags, unsigned int tgt_free)\n \tprintf(\"Pass\\n\");\n }\n \n-/* Size of the LRU map is 2*tgt_free\n- * It is to test the active/inactive list rotation\n- * Insert 1 to 2*tgt_free (+2*tgt_free keys)\n- * Lookup key 1 to tgt_free*3/2\n- * Add 1+2*tgt_free to tgt_free*5/2 (+tgt_free/2 keys)\n- *  => key 1+tgt_free*3/2 to 2*tgt_free are removed from LRU\n+/* Test the active/inactive list rotation\n+ *\n+ * Fill the whole map, deplete the free list.\n+ * Reference all except the last lru->target_free elements.\n+ * Insert lru->target_free new elements. This triggers one shrink.\n+ * Verify that the non-referenced elements are replaced.\n  */\n static void test_lru_sanity3(int map_type, int map_flags, unsigned int tgt_free)\n {\n@@ -437,8 +440,7 @@ static void test_lru_sanity3(int map_type, int map_flags, unsigned int tgt_free)\n \n \tassert(sched_next_online(0, &next_cpu) != -1);\n \n-\tbatch_size = tgt_free / 2;\n-\tassert(batch_size * 2 == tgt_free);\n+\tbatch_size = __tgt_size(tgt_free);\n \n \tmap_size = tgt_free * 2;\n \tlru_map_fd = create_map(map_type, map_flags, map_size);\n@@ -449,23 +451,21 @@ static void test_lru_sanity3(int map_type, int map_flags, unsigned int tgt_free)\n \n \tvalue[0] = 1234;\n \n-\t/* Insert 1 to 2*tgt_free (+2*tgt_free keys) */\n-\tend_key = 1 + (2 * tgt_free);\n+\t/* Fill the map */\n+\tend_key = 1 + map_size;\n \tfor (key = 1; key < end_key; key++)\n \t\tassert(!bpf_map_update_elem(lru_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n \n-\t/* Lookup key 1 to tgt_free*3/2 */\n-\tend_key = tgt_free + batch_size;\n+\t/* Reference all but the last batch_size */\n+\tend_key = 1 + map_size - batch_size;\n \tfor (key = 1; key < end_key; key++) {\n \t\tassert(!bpf_map_lookup_elem_with_ref_bit(lru_map_fd, key, value));\n \t\tassert(!bpf_map_update_elem(expected_map_fd, &key, value,\n \t\t\t\t\t    BPF_NOEXIST));\n \t}\n \n-\t/* Add 1+2*tgt_free to tgt_free*5/2\n-\t * (+tgt_free/2 keys)\n-\t */\n+\t/* Insert new batch_size: replaces the non-referenced elements */\n \tkey = 2 * tgt_free + 1;\n \tend_key = key + batch_size;\n \tfor (; key < end_key; key++) {\n@@ -500,7 +500,8 @@ static void test_lru_sanity4(int map_type, int map_flags, unsigned int tgt_free)\n \t\tlru_map_fd = create_map(map_type, map_flags,\n \t\t\t\t\t3 * tgt_free * nr_cpus);\n \telse\n-\t\tlru_map_fd = create_map(map_type, map_flags, 3 * tgt_free);\n+\t\tlru_map_fd = create_map(map_type, map_flags,\n+\t\t\t\t\t3 * __map_size(tgt_free));\n \tassert(lru_map_fd != -1);\n \n \texpected_map_fd = create_map(BPF_MAP_TYPE_HASH, 0,",
    "stats": {
      "insertions": 1746,
      "deletions": 1703,
      "files": 16
    }
  },
  {
    "sha": "7c942f87cc0be5699b1ce434d369eccd8b5321d4",
    "message": "selftests/mm: fix validate_addr() helper\n\nvalidate_addr() checks whether the address returned by mmap() lies in the\nlow or high VA space, according to whether a high addr hint was passed or\nnot.  The fix commit mentioned below changed the code in such a way that\nthis function will always return failure when passed high_addr == 1; addr\nwill be >= HIGH_ADDR_MARK always, we will fall down to \"if (addr >\nHIGH_ADDR_MARK)\" and return failure.  Fix this.\n\nLink: https://lkml.kernel.org/r/20250620111150.50344-1-dev.jain@arm.com\nFixes: d1d86ce28d0f (\"selftests/mm: virtual_address_range: conform to TAP format output\")\nSigned-off-by: Dev Jain <dev.jain@arm.com>\nReviewed-by: Donet Tom <donettom@linux.ibm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Anshuman Khandual <anshuman.khandual@arm.com>\nCc: Lorenzo Stoakes <lorenzo.stoakes@oracle.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Shuah Khan <shuah@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Dev Jain",
    "date": "2025-06-25T15:55:04-07:00",
    "files_changed": [
      "tools/testing/selftests/mm/virtual_address_range.c"
    ],
    "diff": "diff --git a/tools/testing/selftests/mm/virtual_address_range.c b/tools/testing/selftests/mm/virtual_address_range.c\nindex b380e102b22f..169dbd692bf5 100644\n--- a/tools/testing/selftests/mm/virtual_address_range.c\n+++ b/tools/testing/selftests/mm/virtual_address_range.c\n@@ -77,8 +77,11 @@ static void validate_addr(char *ptr, int high_addr)\n {\n \tunsigned long addr = (unsigned long) ptr;\n \n-\tif (high_addr && addr < HIGH_ADDR_MARK)\n-\t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);\n+\tif (high_addr) {\n+\t\tif (addr < HIGH_ADDR_MARK)\n+\t\t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);\n+\t\treturn;\n+\t}\n \n \tif (addr > HIGH_ADDR_MARK)\n \t\tksft_exit_fail_msg(\"Bad address %lx\\n\", addr);",
    "stats": {
      "insertions": 5,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "4f489fe6afb395dbc79840efa3c05440b760d883",
    "message": "mm/damon/sysfs-schemes: free old damon_sysfs_scheme_filter->memcg_path on write\n\nmemcg_path_store() assigns a newly allocated memory buffer to\nfilter->memcg_path, without deallocating the previously allocated and\nassigned memory buffer.  As a result, users can leak kernel memory by\ncontinuously writing a data to memcg_path DAMOS sysfs file.  Fix the leak\nby deallocating the previously set memory buffer.\n\nLink: https://lkml.kernel.org/r/20250619183608.6647-2-sj@kernel.org\nFixes: 7ee161f18b5d (\"mm/damon/sysfs-schemes: implement filter directory\")\nSigned-off-by: SeongJae Park <sj@kernel.org>\nCc: Shuah Khan <shuah@kernel.org>\nCc: <stable@vger.kernel.org>\t\t[6.3.x]\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "SeongJae Park",
    "date": "2025-06-25T15:55:03-07:00",
    "files_changed": [
      "mm/damon/sysfs-schemes.c"
    ],
    "diff": "diff --git a/mm/damon/sysfs-schemes.c b/mm/damon/sysfs-schemes.c\nindex 0f6c9e1fec0b..30ae7518ffbf 100644\n--- a/mm/damon/sysfs-schemes.c\n+++ b/mm/damon/sysfs-schemes.c\n@@ -472,6 +472,7 @@ static ssize_t memcg_path_store(struct kobject *kobj,\n \t\treturn -ENOMEM;\n \n \tstrscpy(path, buf, count + 1);\n+\tkfree(filter->memcg_path);\n \tfilter->memcg_path = path;\n \treturn count;\n }",
    "stats": {
      "insertions": 1,
      "deletions": 0,
      "files": 1
    }
  }
]
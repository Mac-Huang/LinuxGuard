# LinuxGuard Project Structure

## ğŸ“ Directory Organization

```
LinuxGuard/
â”œâ”€â”€ ğŸ“– README.md                    # Main project documentation
â”œâ”€â”€ ğŸ“‹ PROJECT_STRUCTURE.md         # This file - project organization guide
â”œâ”€â”€ âš™ï¸ setup.py                     # Package setup and dependencies
â”œâ”€â”€ ğŸ”§ config.yaml                  # Configuration settings
â”œâ”€â”€ ğŸ“¦ environment.yml              # Conda environment specification
â”‚
â”œâ”€â”€ ğŸ“‚ src/                         # Source code
â”‚   â”œâ”€â”€ data_collection/            # Phase A: Data collection components
â”‚   â”‚   â”œâ”€â”€ linux_docs_rag.py      # RAG system for Linux documentation
â”‚   â”‚   â”œâ”€â”€ commit_processor.py    # Commit batch processing
â”‚   â”‚   â”œâ”€â”€ large_scale_processor.py # Large-scale dataset processing
â”‚   â”‚   â””â”€â”€ antipattern_filter.py  # Security relevance filtering
â”‚   â”‚
â”‚   â”œâ”€â”€ pattern_analysis/           # Phase B: Pattern analysis components
â”‚   â”‚   â”œâ”€â”€ pattern_derivation.py  # ML clustering + LLM pattern derivation
â”‚   â”‚   â””â”€â”€ clang_generator.py     # Automated static checker generation
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/                 # Validation framework
â”‚   â”‚   â”œâ”€â”€ expert_validation.py   # Expert validation system
â”‚   â”‚   â””â”€â”€ multi_version_validator.py # Cross-version testing
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                 # Performance evaluation
â”‚   â”‚   â””â”€â”€ performance_evaluator.py # Benchmarking and metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ publication/                # Research publication tools
â”‚   â”‚   â””â”€â”€ research_paper_generator.py # Academic paper generation
â”‚   â”‚
â”‚   â””â”€â”€ static_checkers/            # Generated checker templates
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ main.py                     # Main execution script
â”‚   â”œâ”€â”€ phase_b_main.py            # Phase B execution
â”‚   â”œâ”€â”€ large_scale_main.py        # Large-scale processing
â”‚   â”œâ”€â”€ large_scale_demo.py        # Demo script
â”‚   â”œâ”€â”€ evaluation_main.py         # Evaluation execution
â”‚   â”œâ”€â”€ expert_validation_main.py  # Expert validation execution
â”‚   â”œâ”€â”€ evaluation.py              # Evaluation utilities
â”‚   â””â”€â”€ test_simple.py             # Basic testing
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                        # Documentation
â”‚   â””â”€â”€ ENHANCED_ARCHITECTURE.md   # System architecture visualization
â”‚
â”œâ”€â”€ ğŸ“‚ reports/                     # Project reports
â”‚   â”œâ”€â”€ FINAL_PROJECT_SUMMARY.md   # Complete project summary
â”‚   â”œâ”€â”€ FINAL_PHASE_B_REPORT.md    # Phase B completion report
â”‚   â”œâ”€â”€ IMPLEMENTATION_REPORT.md   # Implementation details
â”‚   â”œâ”€â”€ PHASE_B_COMPLETION_REPORT.md # Phase B final report
â”‚   â””â”€â”€ publication/                # Research publication
â”‚       â”œâ”€â”€ LinuxGuard_Research_Paper.md # Complete research paper
â”‚       â”œâ”€â”€ supplementary_materials.md # Additional research materials
â”‚       â”œâ”€â”€ submission_checklist.md # Publication submission guide
â”‚       â””â”€â”€ experimental_data_summary.json # Experimental data
â”‚
â”œâ”€â”€ ğŸ“‚ results/                     # Experimental results
â”‚   â”œâ”€â”€ phase_a_evaluation.json    # Phase A evaluation results
â”‚   â”œâ”€â”€ phase_a_evaluation_report.md # Phase A evaluation report
â”‚   â”‚
â”‚   â”œâ”€â”€ large_scale/                # Large-scale processing results
â”‚   â”‚   â”œâ”€â”€ commits.db             # SQLite database of commits
â”‚   â”‚   â”œâ”€â”€ derived_patterns_large_scale.json # Large-scale derived patterns
â”‚   â”‚   â”œâ”€â”€ demo_results.json      # Demo execution results
â”‚   â”‚   â”œâ”€â”€ large_scale_summary.json # Processing summary
â”‚   â”‚   â””â”€â”€ scale_comparison_report.md # Scale comparison analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ expert_validation/          # Expert validation framework
â”‚   â”‚   â”œâ”€â”€ experts.json           # Expert panel database
â”‚   â”‚   â”œâ”€â”€ questions.json         # Validation questions
â”‚   â”‚   â”œâ”€â”€ expert_profiles.json   # Expert profiles
â”‚   â”‚   â”œâ”€â”€ validation_setup_summary.json # Setup summary
â”‚   â”‚   â””â”€â”€ expert_*/               # Individual expert validation sessions
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                 # Performance evaluation results
â”‚   â”‚   â”œâ”€â”€ evaluation_results.json # Comprehensive evaluation results
â”‚   â”‚   â”œâ”€â”€ evaluation_report.md   # Evaluation report
â”‚   â”‚   â”œâ”€â”€ linux_cves.json        # CVE database for validation
â”‚   â”‚   â””â”€â”€ visualizations/        # Performance charts and graphs
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/                 # Cross-version validation
â”‚   â”‚   â”œâ”€â”€ linux-6.6/             # Linux kernel 6.6 test files
â”‚   â”‚   â”œâ”€â”€ linux-6.7/             # Linux kernel 6.7 test files
â”‚   â”‚   â”œâ”€â”€ linux-6.8/             # Linux kernel 6.8 test files
â”‚   â”‚   â”œâ”€â”€ validation_results.json # Validation results
â”‚   â”‚   â””â”€â”€ validation_report.md   # Validation report
â”‚   â”‚
â”‚   â””â”€â”€ pattern_analysis/           # Pattern derivation results
â”‚       â”œâ”€â”€ commit_analyses.json   # Individual commit analyses
â”‚       â”œâ”€â”€ pattern_clusters.json  # ML clustering results
â”‚       â”œâ”€â”€ derived_patterns.json  # Final derived patterns
â”‚       â””â”€â”€ derivation_report.md   # Pattern derivation report
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/                     # Generated outputs
â”‚   â””â”€â”€ static_checkers/            # Generated Clang static analyzers
â”‚       â”œâ”€â”€ CMakeLists.txt          # Build system
â”‚       â”œâ”€â”€ Registration.cpp       # Checker registration
â”‚       â”œâ”€â”€ checkers/               # Generated checker implementations
â”‚       â”œâ”€â”€ checkers_metadata.json # Checker metadata
â”‚       â”œâ”€â”€ generation_report.md   # Generation report
â”‚       â””â”€â”€ build/                  # Built checker binaries
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # Essential data
â”‚   â”œâ”€â”€ linux_docs/                # Linux documentation corpus
â”‚   â””â”€â”€ vector_db/                  # ChromaDB vector database
â”‚       â””â”€â”€ chroma.sqlite3          # Vector embeddings database
â”‚
â””â”€â”€ ğŸ“‚ archive/                     # Archived materials
    â”œâ”€â”€ test_commits/               # Test commit batches (56 batches)
    â”œâ”€â”€ commits/                    # Original commit processing
    â”œâ”€â”€ results/                    # Legacy result files
    â”œâ”€â”€ experiments/                # Experimental code
    â””â”€â”€ paper/                      # Legacy paper materials
```

## ğŸš€ Quick Start Guide

### 1. Environment Setup
```bash
# Create conda environment
conda env create -f environment.yml
conda activate linuxguard

# Install dependencies
pip install -e .
```

### 2. Core Execution
```bash
# Run Phase A (Data Collection)
python scripts/main.py --phase A

# Run Phase B (Pattern Analysis)
python scripts/phase_b_main.py

# Run Large-Scale Processing
python scripts/large_scale_main.py

# Run Evaluation
python scripts/evaluation_main.py
```

### 3. Key Components

#### Phase A: Anti-Pattern Dataset Creation
- **Data Collection**: `src/data_collection/commit_processor.py`
- **RAG System**: `src/data_collection/linux_docs_rag.py`
- **Security Filtering**: `src/data_collection/antipattern_filter.py`

#### Phase B: Pattern Analysis & Tool Generation
- **Pattern Derivation**: `src/pattern_analysis/pattern_derivation.py`
- **Clang Generation**: `src/pattern_analysis/clang_generator.py`
- **Multi-Version Validation**: `src/validation/multi_version_validator.py`

#### Evaluation & Validation
- **Performance Evaluation**: `src/evaluation/performance_evaluator.py`
- **Expert Validation**: `src/validation/expert_validation.py`

## ğŸ“Š Results Overview

### Generated Artifacts
- **ğŸ“„ Research Paper**: `reports/publication/LinuxGuard_Research_Paper.md` (8,500 words)
- **ğŸ—ï¸ Architecture**: `docs/ENHANCED_ARCHITECTURE.md`
- **ğŸ› ï¸ Static Checkers**: `outputs/static_checkers/`
- **ğŸ“ˆ Evaluation Results**: `results/evaluation/`

### Key Metrics
- **Scale**: 7,200 commits processed (26x improvement)
- **Patterns**: 6 high-confidence anti-patterns derived
- **Performance**: 65% precision, 15.0 files/second analysis speed
- **Validation**: Expert validation framework with 5 security researchers

## ğŸ”§ Configuration

### Main Configuration: `config.yaml`
```yaml
# Processing settings
batch_size: 50
max_workers: 4
api_delay: 2.0

# LLM configuration
llm_model: "gemini-2.0-flash"
embedding_model: "all-MiniLM-L6-v2"

# Paths
data_dir: "data"
results_dir: "results"
outputs_dir: "outputs"
```

## ğŸ“ Development Notes

### Code Organization Principles
1. **Modular Design**: Each component has clear responsibilities
2. **Separation of Concerns**: Data collection, analysis, and evaluation are separate
3. **Extensibility**: Framework can be extended to other repositories/languages
4. **Reproducibility**: All experiments can be reproduced from configuration

### File Naming Conventions
- **Scripts**: `*_main.py` for main execution scripts
- **Results**: `*_results.json` for structured data, `*_report.md` for human-readable reports
- **Generated**: `derived_*` for derived/generated content
- **Validation**: `validation_*` for validation-related files

## ğŸ¯ Project Status

- âœ… **Phase A**: Complete - Dataset creation with 7,200 commits
- âœ… **Phase B**: Complete - Pattern derivation and tool generation
- âœ… **Large-Scale**: Complete - 26x scalability demonstrated
- âœ… **Evaluation**: Complete - Comprehensive performance evaluation
- âœ… **Expert Validation**: Complete - 5 expert validation framework
- âœ… **Publication**: Complete - Research paper ready for submission

**Overall Status**: ğŸ† **PRODUCTION READY & PUBLICATION READY**
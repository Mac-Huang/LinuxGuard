[
  {
    "sha": "7e4a6b57dd7f55cce9ece0778c111905e73db7b1",
    "message": "Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma\n\nPull rdma fixes from Jason Gunthorpe:\n \"Several mlx5 bugs, crashers, and reports:\n\n   - Limit stack usage\n\n   - Fix mis-use of __xa_store/erase() without holding the lock to a\n     locked version\n\n   - Rate limit prints in the gid cache error cases\n\n   - Fully initialize the event object before making it globally visible\n     in an xarray\n\n   - Fix deadlock inside the ODP code if the MMU notifier was called\n     from a reclaim context\n\n   - Include missed counters for some switchdev configurations and\n     mulit-port MPV mode\n\n   - Fix loopback packet support when in mulit-port MPV mode\"\n\n* tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:\n  RDMA/mlx5: Fix vport loopback for MPV device\n  RDMA/mlx5: Fix CC counters query for MPV\n  RDMA/mlx5: Fix HW counters query for non-representor devices\n  IB/core: Annotate umem_mutex acquisition under fs_reclaim for lockdep\n  IB/mlx5: Fix potential deadlock in MR deregistration\n  RDMA/mlx5: Initialize obj_event->obj_sub_list before xa_insert\n  RDMA/core: Rate limit GID cache warning messages\n  RDMA/mlx5: Fix unsafe xarray access in implicit ODP handling\n  RDMA/mlx5: reduce stack usage in mlx5_ib_ufile_hw_cleanup",
    "author": "Linus Torvalds",
    "date": "2025-07-02T09:17:40-07:00",
    "files_changed": [
      "drivers/infiniband/core/cache.c",
      "drivers/infiniband/core/umem_odp.c",
      "drivers/infiniband/hw/mlx5/counters.c",
      "drivers/infiniband/hw/mlx5/devx.c",
      "drivers/infiniband/hw/mlx5/main.c",
      "drivers/infiniband/hw/mlx5/mr.c",
      "drivers/infiniband/hw/mlx5/odp.c"
    ],
    "diff": "diff --git a/drivers/infiniband/core/cache.c b/drivers/infiniband/core/cache.c\nindex 9979a351577f..81cf3c902e81 100644\n--- a/drivers/infiniband/core/cache.c\n+++ b/drivers/infiniband/core/cache.c\n@@ -582,8 +582,8 @@ static int __ib_cache_gid_add(struct ib_device *ib_dev, u32 port,\n out_unlock:\n \tmutex_unlock(&table->lock);\n \tif (ret)\n-\t\tpr_warn(\"%s: unable to add gid %pI6 error=%d\\n\",\n-\t\t\t__func__, gid->raw, ret);\n+\t\tpr_warn_ratelimited(\"%s: unable to add gid %pI6 error=%d\\n\",\n+\t\t\t\t    __func__, gid->raw, ret);\n \treturn ret;\n }\n \ndiff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c\nindex c752ae9fad6c..b1c44ec1a3f3 100644\n--- a/drivers/infiniband/core/umem_odp.c\n+++ b/drivers/infiniband/core/umem_odp.c\n@@ -76,6 +76,17 @@ static int ib_init_umem_odp(struct ib_umem_odp *umem_odp,\n \tend = ALIGN(end, page_size);\n \tif (unlikely(end < page_size))\n \t\treturn -EOVERFLOW;\n+\t/*\n+\t * The mmu notifier can be called within reclaim contexts and takes the\n+\t * umem_mutex. This is rare to trigger in testing, teach lockdep about\n+\t * it.\n+\t */\n+\tif (IS_ENABLED(CONFIG_LOCKDEP)) {\n+\t\tfs_reclaim_acquire(GFP_KERNEL);\n+\t\tmutex_lock(&umem_odp->umem_mutex);\n+\t\tmutex_unlock(&umem_odp->umem_mutex);\n+\t\tfs_reclaim_release(GFP_KERNEL);\n+\t}\n \n \tnr_entries = (end - start) >> PAGE_SHIFT;\n \tif (!(nr_entries * PAGE_SIZE / page_size))\ndiff --git a/drivers/infiniband/hw/mlx5/counters.c b/drivers/infiniband/hw/mlx5/counters.c\nindex b847084dcd99..a506fafd2b15 100644\n--- a/drivers/infiniband/hw/mlx5/counters.c\n+++ b/drivers/infiniband/hw/mlx5/counters.c\n@@ -398,7 +398,7 @@ static int do_get_hw_stats(struct ib_device *ibdev,\n \t\treturn ret;\n \n \t/* We don't expose device counters over Vports */\n-\tif (is_mdev_switchdev_mode(dev->mdev) && port_num != 0)\n+\tif (is_mdev_switchdev_mode(dev->mdev) && dev->is_rep && port_num != 0)\n \t\tgoto done;\n \n \tif (MLX5_CAP_PCAM_FEATURE(dev->mdev, rx_icrc_encapsulated_counter)) {\n@@ -418,7 +418,7 @@ static int do_get_hw_stats(struct ib_device *ibdev,\n \t\t\t */\n \t\t\tgoto done;\n \t\t}\n-\t\tret = mlx5_lag_query_cong_counters(dev->mdev,\n+\t\tret = mlx5_lag_query_cong_counters(mdev,\n \t\t\t\t\t\t   stats->value +\n \t\t\t\t\t\t   cnts->num_q_counters,\n \t\t\t\t\t\t   cnts->num_cong_counters,\ndiff --git a/drivers/infiniband/hw/mlx5/devx.c b/drivers/infiniband/hw/mlx5/devx.c\nindex 2479da8620ca..843dcd312242 100644\n--- a/drivers/infiniband/hw/mlx5/devx.c\n+++ b/drivers/infiniband/hw/mlx5/devx.c\n@@ -1958,6 +1958,7 @@ subscribe_event_xa_alloc(struct mlx5_devx_event_table *devx_event_table,\n \t\t\t/* Level1 is valid for future use, no need to free */\n \t\t\treturn -ENOMEM;\n \n+\t\tINIT_LIST_HEAD(&obj_event->obj_sub_list);\n \t\terr = xa_insert(&event->object_ids,\n \t\t\t\tkey_level2,\n \t\t\t\tobj_event,\n@@ -1966,7 +1967,6 @@ subscribe_event_xa_alloc(struct mlx5_devx_event_table *devx_event_table,\n \t\t\tkfree(obj_event);\n \t\t\treturn err;\n \t\t}\n-\t\tINIT_LIST_HEAD(&obj_event->obj_sub_list);\n \t}\n \n \treturn 0;\n@@ -2669,7 +2669,7 @@ static void devx_wait_async_destroy(struct mlx5_async_cmd *cmd)\n \n void mlx5_ib_ufile_hw_cleanup(struct ib_uverbs_file *ufile)\n {\n-\tstruct mlx5_async_cmd async_cmd[MAX_ASYNC_CMDS];\n+\tstruct mlx5_async_cmd *async_cmd;\n \tstruct ib_ucontext *ucontext = ufile->ucontext;\n \tstruct ib_device *device = ucontext->device;\n \tstruct mlx5_ib_dev *dev = to_mdev(device);\n@@ -2678,6 +2678,10 @@ void mlx5_ib_ufile_hw_cleanup(struct ib_uverbs_file *ufile)\n \tint head = 0;\n \tint tail = 0;\n \n+\tasync_cmd = kcalloc(MAX_ASYNC_CMDS, sizeof(*async_cmd), GFP_KERNEL);\n+\tif (!async_cmd)\n+\t\treturn;\n+\n \tlist_for_each_entry(uobject, &ufile->uobjects, list) {\n \t\tWARN_ON(uverbs_try_lock_object(uobject, UVERBS_LOOKUP_WRITE));\n \n@@ -2713,6 +2717,8 @@ void mlx5_ib_ufile_hw_cleanup(struct ib_uverbs_file *ufile)\n \t\tdevx_wait_async_destroy(&async_cmd[head % MAX_ASYNC_CMDS]);\n \t\thead++;\n \t}\n+\n+\tkfree(async_cmd);\n }\n \n static ssize_t devx_async_cmd_event_read(struct file *filp, char __user *buf,\ndiff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c\nindex ce7610740412..df6557ddbdfc 100644\n--- a/drivers/infiniband/hw/mlx5/main.c\n+++ b/drivers/infiniband/hw/mlx5/main.c\n@@ -1791,6 +1791,33 @@ static void deallocate_uars(struct mlx5_ib_dev *dev,\n \t\t\t\t\t     context->devx_uid);\n }\n \n+static int mlx5_ib_enable_lb_mp(struct mlx5_core_dev *master,\n+\t\t\t\tstruct mlx5_core_dev *slave)\n+{\n+\tint err;\n+\n+\terr = mlx5_nic_vport_update_local_lb(master, true);\n+\tif (err)\n+\t\treturn err;\n+\n+\terr = mlx5_nic_vport_update_local_lb(slave, true);\n+\tif (err)\n+\t\tgoto out;\n+\n+\treturn 0;\n+\n+out:\n+\tmlx5_nic_vport_update_local_lb(master, false);\n+\treturn err;\n+}\n+\n+static void mlx5_ib_disable_lb_mp(struct mlx5_core_dev *master,\n+\t\t\t\t  struct mlx5_core_dev *slave)\n+{\n+\tmlx5_nic_vport_update_local_lb(slave, false);\n+\tmlx5_nic_vport_update_local_lb(master, false);\n+}\n+\n int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)\n {\n \tint err = 0;\n@@ -3495,6 +3522,8 @@ static void mlx5_ib_unbind_slave_port(struct mlx5_ib_dev *ibdev,\n \n \tlockdep_assert_held(&mlx5_ib_multiport_mutex);\n \n+\tmlx5_ib_disable_lb_mp(ibdev->mdev, mpi->mdev);\n+\n \tmlx5_core_mp_event_replay(ibdev->mdev,\n \t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_REMOVED,\n \t\t\t\t  NULL);\n@@ -3590,6 +3619,10 @@ static bool mlx5_ib_bind_slave_port(struct mlx5_ib_dev *ibdev,\n \t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_DONE,\n \t\t\t\t  &key);\n \n+\terr = mlx5_ib_enable_lb_mp(ibdev->mdev, mpi->mdev);\n+\tif (err)\n+\t\tgoto unbind;\n+\n \treturn true;\n \n unbind:\ndiff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c\nindex 57f9bc2a4a3a..bd35e75d9ce5 100644\n--- a/drivers/infiniband/hw/mlx5/mr.c\n+++ b/drivers/infiniband/hw/mlx5/mr.c\n@@ -2027,23 +2027,50 @@ void mlx5_ib_revoke_data_direct_mrs(struct mlx5_ib_dev *dev)\n \t}\n }\n \n-static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n+static int mlx5_umr_revoke_mr_with_lock(struct mlx5_ib_mr *mr)\n {\n-\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n-\tstruct mlx5_cache_ent *ent = mr->mmkey.cache_ent;\n-\tbool is_odp = is_odp_mr(mr);\n \tbool is_odp_dma_buf = is_dmabuf_mr(mr) &&\n-\t\t\t!to_ib_umem_dmabuf(mr->umem)->pinned;\n-\tbool from_cache = !!ent;\n-\tint ret = 0;\n+\t\t\t      !to_ib_umem_dmabuf(mr->umem)->pinned;\n+\tbool is_odp = is_odp_mr(mr);\n+\tint ret;\n \n \tif (is_odp)\n \t\tmutex_lock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n \n \tif (is_odp_dma_buf)\n-\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv, NULL);\n+\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv,\n+\t\t\t      NULL);\n+\n+\tret = mlx5r_umr_revoke_mr(mr);\n+\n+\tif (is_odp) {\n+\t\tif (!ret)\n+\t\t\tto_ib_umem_odp(mr->umem)->private = NULL;\n+\t\tmutex_unlock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n+\t}\n+\n+\tif (is_odp_dma_buf) {\n+\t\tif (!ret)\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->private = NULL;\n+\t\tdma_resv_unlock(\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n+\t}\n \n-\tif (mr->mmkey.cacheable && !mlx5r_umr_revoke_mr(mr) && !cache_ent_find_and_store(dev, mr)) {\n+\treturn ret;\n+}\n+\n+static int mlx5r_handle_mkey_cleanup(struct mlx5_ib_mr *mr)\n+{\n+\tbool is_odp_dma_buf = is_dmabuf_mr(mr) &&\n+\t\t\t      !to_ib_umem_dmabuf(mr->umem)->pinned;\n+\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n+\tstruct mlx5_cache_ent *ent = mr->mmkey.cache_ent;\n+\tbool is_odp = is_odp_mr(mr);\n+\tbool from_cache = !!ent;\n+\tint ret;\n+\n+\tif (mr->mmkey.cacheable && !mlx5_umr_revoke_mr_with_lock(mr) &&\n+\t    !cache_ent_find_and_store(dev, mr)) {\n \t\tent = mr->mmkey.cache_ent;\n \t\t/* upon storing to a clean temp entry - schedule its cleanup */\n \t\tspin_lock_irq(&ent->mkeys_queue.lock);\n@@ -2055,7 +2082,7 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \t\t\tent->tmp_cleanup_scheduled = true;\n \t\t}\n \t\tspin_unlock_irq(&ent->mkeys_queue.lock);\n-\t\tgoto out;\n+\t\treturn 0;\n \t}\n \n \tif (ent) {\n@@ -2064,8 +2091,14 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \t\tmr->mmkey.cache_ent = NULL;\n \t\tspin_unlock_irq(&ent->mkeys_queue.lock);\n \t}\n+\n+\tif (is_odp)\n+\t\tmutex_lock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n+\n+\tif (is_odp_dma_buf)\n+\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv,\n+\t\t\t      NULL);\n \tret = destroy_mkey(dev, mr);\n-out:\n \tif (is_odp) {\n \t\tif (!ret)\n \t\t\tto_ib_umem_odp(mr->umem)->private = NULL;\n@@ -2075,9 +2108,9 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \tif (is_odp_dma_buf) {\n \t\tif (!ret)\n \t\t\tto_ib_umem_dmabuf(mr->umem)->private = NULL;\n-\t\tdma_resv_unlock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n+\t\tdma_resv_unlock(\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n \t}\n-\n \treturn ret;\n }\n \n@@ -2126,7 +2159,7 @@ static int __mlx5_ib_dereg_mr(struct ib_mr *ibmr)\n \t}\n \n \t/* Stop DMA */\n-\trc = mlx5_revoke_mr(mr);\n+\trc = mlx5r_handle_mkey_cleanup(mr);\n \tif (rc)\n \t\treturn rc;\n \ndiff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c\nindex eaa2f9f5f3a9..f6abd64f07f7 100644\n--- a/drivers/infiniband/hw/mlx5/odp.c\n+++ b/drivers/infiniband/hw/mlx5/odp.c\n@@ -259,8 +259,8 @@ static void destroy_unused_implicit_child_mr(struct mlx5_ib_mr *mr)\n \t}\n \n \tif (MLX5_CAP_ODP(mr_to_mdev(mr)->mdev, mem_page_fault))\n-\t\t__xa_erase(&mr_to_mdev(mr)->odp_mkeys,\n-\t\t\t   mlx5_base_mkey(mr->mmkey.key));\n+\t\txa_erase(&mr_to_mdev(mr)->odp_mkeys,\n+\t\t\t mlx5_base_mkey(mr->mmkey.key));\n \txa_unlock(&imr->implicit_children);\n \n \t/* Freeing a MR is a sleeping operation, so bounce to a work queue */\n@@ -532,8 +532,8 @@ static struct mlx5_ib_mr *implicit_get_child_mr(struct mlx5_ib_mr *imr,\n \t}\n \n \tif (MLX5_CAP_ODP(dev->mdev, mem_page_fault)) {\n-\t\tret = __xa_store(&dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),\n-\t\t\t\t &mr->mmkey, GFP_KERNEL);\n+\t\tret = xa_store(&dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),\n+\t\t\t       &mr->mmkey, GFP_KERNEL);\n \t\tif (xa_is_err(ret)) {\n \t\t\tret = ERR_PTR(xa_err(ret));\n \t\t\t__xa_erase(&imr->implicit_children, idx);",
    "stats": {
      "insertions": 107,
      "deletions": 24,
      "files": 7
    }
  },
  {
    "sha": "34a500caf48c47d5171f4aa1f237da39b07c6157",
    "message": "rose: fix dangling neighbour pointers in rose_rt_device_down()\n\nThere are two bugs in rose_rt_device_down() that can cause\nuse-after-free:\n\n1. The loop bound `t->count` is modified within the loop, which can\n   cause the loop to terminate early and miss some entries.\n\n2. When removing an entry from the neighbour array, the subsequent entries\n   are moved up to fill the gap, but the loop index `i` is still\n   incremented, causing the next entry to be skipped.\n\nFor example, if a node has three neighbours (A, A, B) with count=3 and A\nis being removed, the second A is not checked.\n\n    i=0: (A, A, B) -> (A, B) with count=2\n          ^ checked\n    i=1: (A, B)    -> (A, B) with count=2\n             ^ checked (B, not A!)\n    i=2: (doesn't occur because i < count is false)\n\nThis leaves the second A in the array with count=2, but the rose_neigh\nstructure has been freed. Code that accesses these entries assumes that\nthe first `count` entries are valid pointers, causing a use-after-free\nwhen it accesses the dangling pointer.\n\nFix both issues by iterating over the array in reverse order with a fixed\nloop bound. This ensures that all entries are examined and that the removal\nof an entry doesn't affect subsequent iterations.\n\nReported-by: syzbot+e04e2c007ba2c80476cb@syzkaller.appspotmail.com\nCloses: https://syzkaller.appspot.com/bug?extid=e04e2c007ba2c80476cb\nTested-by: syzbot+e04e2c007ba2c80476cb@syzkaller.appspotmail.com\nFixes: 1da177e4c3f4 (\"Linux-2.6.12-rc2\")\nSigned-off-by: Kohei Enju <enjuk@amazon.com>\nReviewed-by: Simon Horman <horms@kernel.org>\nLink: https://patch.msgid.link/20250629030833.6680-1-enjuk@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
    "author": "Kohei Enju",
    "date": "2025-07-01T19:28:48-07:00",
    "files_changed": [
      "net/rose/rose_route.c"
    ],
    "diff": "diff --git a/net/rose/rose_route.c b/net/rose/rose_route.c\nindex 2dd6bd3a3011..b72bf8a08d48 100644\n--- a/net/rose/rose_route.c\n+++ b/net/rose/rose_route.c\n@@ -497,22 +497,15 @@ void rose_rt_device_down(struct net_device *dev)\n \t\t\tt         = rose_node;\n \t\t\trose_node = rose_node->next;\n \n-\t\t\tfor (i = 0; i < t->count; i++) {\n+\t\t\tfor (i = t->count - 1; i >= 0; i--) {\n \t\t\t\tif (t->neighbour[i] != s)\n \t\t\t\t\tcontinue;\n \n \t\t\t\tt->count--;\n \n-\t\t\t\tswitch (i) {\n-\t\t\t\tcase 0:\n-\t\t\t\t\tt->neighbour[0] = t->neighbour[1];\n-\t\t\t\t\tfallthrough;\n-\t\t\t\tcase 1:\n-\t\t\t\t\tt->neighbour[1] = t->neighbour[2];\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase 2:\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\t\tmemmove(&t->neighbour[i], &t->neighbour[i + 1],\n+\t\t\t\t\tsizeof(t->neighbour[0]) *\n+\t\t\t\t\t\t(t->count - i));\n \t\t\t}\n \n \t\t\tif (t->count <= 0)",
    "stats": {
      "insertions": 4,
      "deletions": 11,
      "files": 1
    }
  },
  {
    "sha": "bf906c988d77d9cdfb52a7d19ad9b183991271a3",
    "message": "Merge tag 'amd-drm-fixes-6.16-2025-07-01' of https://gitlab.freedesktop.org/agd5f/linux into drm-fixes\n\namd-drm-fixes-6.16-2025-07-01:\n\namdgpu:\n- SDMA 5.x reset fix\n- Add missing firmware declaration\n- Fix leak in amdgpu_ctx_mgr_entity_fini()\n- Freesync fix\n- OLED backlight fix\n\namdkfd:\n- mtype fix for ext coherent system memory\n- MMU notifier fix\n- gfx7/8 fix\n\nSigned-off-by: Dave Airlie <airlied@redhat.com>\n\nFrom: Alex Deucher <alexander.deucher@amd.com>\nLink: https://lore.kernel.org/r/20250701192642.32490-1-alexander.deucher@amd.com",
    "author": "Dave Airlie",
    "date": "2025-07-02T11:18:24+10:00",
    "files_changed": [
      "drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c",
      "drivers/gpu/drm/amd/amdkfd/kfd_svm.c",
      "drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c",
      "drivers/gpu/drm/amd/display/dc/dc_hw_types.h",
      "drivers/gpu/drm/amd/display/modules/freesync/freesync.c"
    ],
    "diff": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c\nindex ca4a6b82817f..df77558e03ef 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c\n@@ -561,6 +561,13 @@ static uint32_t read_vmid_from_vmfault_reg(struct amdgpu_device *adev)\n \treturn REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);\n }\n \n+static uint32_t kgd_hqd_sdma_get_doorbell(struct amdgpu_device *adev,\n+\t\t\t\t\t  int engine, int queue)\n+\n+{\n+\treturn 0;\n+}\n+\n const struct kfd2kgd_calls gfx_v7_kfd2kgd = {\n \t.program_sh_mem_settings = kgd_program_sh_mem_settings,\n \t.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,\n@@ -578,4 +585,5 @@ const struct kfd2kgd_calls gfx_v7_kfd2kgd = {\n \t.set_scratch_backing_va = set_scratch_backing_va,\n \t.set_vm_context_page_table_base = set_vm_context_page_table_base,\n \t.read_vmid_from_vmfault_reg = read_vmid_from_vmfault_reg,\n+\t.hqd_sdma_get_doorbell = kgd_hqd_sdma_get_doorbell,\n };\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c\nindex 0f3e2944edd7..e68c0fa8d751 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c\n@@ -582,6 +582,13 @@ static void set_vm_context_page_table_base(struct amdgpu_device *adev,\n \t\t\tlower_32_bits(page_table_base));\n }\n \n+static uint32_t kgd_hqd_sdma_get_doorbell(struct amdgpu_device *adev,\n+\t\t\t\t\t  int engine, int queue)\n+\n+{\n+\treturn 0;\n+}\n+\n const struct kfd2kgd_calls gfx_v8_kfd2kgd = {\n \t.program_sh_mem_settings = kgd_program_sh_mem_settings,\n \t.set_pasid_vmid_mapping = kgd_set_pasid_vmid_mapping,\n@@ -599,4 +606,5 @@ const struct kfd2kgd_calls gfx_v8_kfd2kgd = {\n \t\t\tget_atc_vmid_pasid_mapping_info,\n \t.set_scratch_backing_va = set_scratch_backing_va,\n \t.set_vm_context_page_table_base = set_vm_context_page_table_base,\n+\t.hqd_sdma_get_doorbell = kgd_hqd_sdma_get_doorbell,\n };\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c\nindex 85567d0d9545..f5d5c45ddc0d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c\n@@ -944,6 +944,7 @@ static void amdgpu_ctx_mgr_entity_fini(struct amdgpu_ctx_mgr *mgr)\n \t\t\t\tdrm_sched_entity_fini(entity);\n \t\t\t}\n \t\t}\n+\t\tkref_put(&ctx->refcount, amdgpu_ctx_fini);\n \t}\n }\n \ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c b/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\nindex cef68df4c663..bb82c652e4c0 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\n@@ -45,6 +45,7 @@\n #include \"amdgpu_ras.h\"\n \n MODULE_FIRMWARE(\"amdgpu/sdma_4_4_2.bin\");\n+MODULE_FIRMWARE(\"amdgpu/sdma_4_4_4.bin\");\n MODULE_FIRMWARE(\"amdgpu/sdma_4_4_5.bin\");\n \n static const struct amdgpu_hwip_reg_entry sdma_reg_list_4_4_2[] = {\ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c b/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\nindex 1813c3ed0aa6..37f4b5b4a098 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\n@@ -1543,8 +1543,13 @@ static int sdma_v5_0_reset_queue(struct amdgpu_ring *ring, unsigned int vmid)\n {\n \tstruct amdgpu_device *adev = ring->adev;\n \tu32 inst_id = ring->me;\n+\tint r;\n+\n+\tamdgpu_amdkfd_suspend(adev, true);\n+\tr = amdgpu_sdma_reset_engine(adev, inst_id);\n+\tamdgpu_amdkfd_resume(adev, true);\n \n-\treturn amdgpu_sdma_reset_engine(adev, inst_id);\n+\treturn r;\n }\n \n static int sdma_v5_0_stop_queue(struct amdgpu_ring *ring)\ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c b/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\nindex 23f97da62808..0b40411b92a0 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\n@@ -1456,8 +1456,13 @@ static int sdma_v5_2_reset_queue(struct amdgpu_ring *ring, unsigned int vmid)\n {\n \tstruct amdgpu_device *adev = ring->adev;\n \tu32 inst_id = ring->me;\n+\tint r;\n+\n+\tamdgpu_amdkfd_suspend(adev, true);\n+\tr = amdgpu_sdma_reset_engine(adev, inst_id);\n+\tamdgpu_amdkfd_resume(adev, true);\n \n-\treturn amdgpu_sdma_reset_engine(adev, inst_id);\n+\treturn r;\n }\n \n static int sdma_v5_2_stop_queue(struct amdgpu_ring *ring)\ndiff --git a/drivers/gpu/drm/amd/amdkfd/kfd_svm.c b/drivers/gpu/drm/amd/amdkfd/kfd_svm.c\nindex 865dca2547de..a0f22ea6d15a 100644\n--- a/drivers/gpu/drm/amd/amdkfd/kfd_svm.c\n+++ b/drivers/gpu/drm/amd/amdkfd/kfd_svm.c\n@@ -1171,13 +1171,12 @@ svm_range_split_head(struct svm_range *prange, uint64_t new_start,\n }\n \n static void\n-svm_range_add_child(struct svm_range *prange, struct mm_struct *mm,\n-\t\t    struct svm_range *pchild, enum svm_work_list_ops op)\n+svm_range_add_child(struct svm_range *prange, struct svm_range *pchild, enum svm_work_list_ops op)\n {\n \tpr_debug(\"add child 0x%p [0x%lx 0x%lx] to prange 0x%p child list %d\\n\",\n \t\t pchild, pchild->start, pchild->last, prange, op);\n \n-\tpchild->work_item.mm = mm;\n+\tpchild->work_item.mm = NULL;\n \tpchild->work_item.op = op;\n \tlist_add_tail(&pchild->child_list, &prange->child_list);\n }\n@@ -1278,7 +1277,7 @@ svm_range_get_pte_flags(struct kfd_node *node,\n \t\t\t\tmapping_flags |= ext_coherent ? AMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n \t\t/* system memory accessed by the dGPU */\n \t\t} else {\n-\t\t\tif (gc_ip_version < IP_VERSION(9, 5, 0))\n+\t\t\tif (gc_ip_version < IP_VERSION(9, 5, 0) || ext_coherent)\n \t\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_UC;\n \t\t\telse\n \t\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_NC;\n@@ -2394,15 +2393,17 @@ svm_range_add_list_work(struct svm_range_list *svms, struct svm_range *prange,\n \t\t    prange->work_item.op != SVM_OP_UNMAP_RANGE)\n \t\t\tprange->work_item.op = op;\n \t} else {\n-\t\tprange->work_item.op = op;\n-\n-\t\t/* Pairs with mmput in deferred_list_work */\n-\t\tmmget(mm);\n-\t\tprange->work_item.mm = mm;\n-\t\tlist_add_tail(&prange->deferred_list,\n-\t\t\t      &prange->svms->deferred_range_list);\n-\t\tpr_debug(\"add prange 0x%p [0x%lx 0x%lx] to work list op %d\\n\",\n-\t\t\t prange, prange->start, prange->last, op);\n+\t\t/* Pairs with mmput in deferred_list_work.\n+\t\t * If process is exiting and mm is gone, don't update mmu notifier.\n+\t\t */\n+\t\tif (mmget_not_zero(mm)) {\n+\t\t\tprange->work_item.mm = mm;\n+\t\t\tprange->work_item.op = op;\n+\t\t\tlist_add_tail(&prange->deferred_list,\n+\t\t\t\t      &prange->svms->deferred_range_list);\n+\t\t\tpr_debug(\"add prange 0x%p [0x%lx 0x%lx] to work list op %d\\n\",\n+\t\t\t\t prange, prange->start, prange->last, op);\n+\t\t}\n \t}\n \tspin_unlock(&svms->deferred_list_lock);\n }\n@@ -2416,8 +2417,7 @@ void schedule_deferred_list_work(struct svm_range_list *svms)\n }\n \n static void\n-svm_range_unmap_split(struct mm_struct *mm, struct svm_range *parent,\n-\t\t      struct svm_range *prange, unsigned long start,\n+svm_range_unmap_split(struct svm_range *parent, struct svm_range *prange, unsigned long start,\n \t\t      unsigned long last)\n {\n \tstruct svm_range *head;\n@@ -2438,12 +2438,12 @@ svm_range_unmap_split(struct mm_struct *mm, struct svm_range *parent,\n \t\tsvm_range_split(tail, last + 1, tail->last, &head);\n \n \tif (head != prange && tail != prange) {\n-\t\tsvm_range_add_child(parent, mm, head, SVM_OP_UNMAP_RANGE);\n-\t\tsvm_range_add_child(parent, mm, tail, SVM_OP_ADD_RANGE);\n+\t\tsvm_range_add_child(parent, head, SVM_OP_UNMAP_RANGE);\n+\t\tsvm_range_add_child(parent, tail, SVM_OP_ADD_RANGE);\n \t} else if (tail != prange) {\n-\t\tsvm_range_add_child(parent, mm, tail, SVM_OP_UNMAP_RANGE);\n+\t\tsvm_range_add_child(parent, tail, SVM_OP_UNMAP_RANGE);\n \t} else if (head != prange) {\n-\t\tsvm_range_add_child(parent, mm, head, SVM_OP_UNMAP_RANGE);\n+\t\tsvm_range_add_child(parent, head, SVM_OP_UNMAP_RANGE);\n \t} else if (parent != prange) {\n \t\tprange->work_item.op = SVM_OP_UNMAP_RANGE;\n \t}\n@@ -2520,14 +2520,14 @@ svm_range_unmap_from_cpu(struct mm_struct *mm, struct svm_range *prange,\n \t\tl = min(last, pchild->last);\n \t\tif (l >= s)\n \t\t\tsvm_range_unmap_from_gpus(pchild, s, l, trigger);\n-\t\tsvm_range_unmap_split(mm, prange, pchild, start, last);\n+\t\tsvm_range_unmap_split(prange, pchild, start, last);\n \t\tmutex_unlock(&pchild->lock);\n \t}\n \ts = max(start, prange->start);\n \tl = min(last, prange->last);\n \tif (l >= s)\n \t\tsvm_range_unmap_from_gpus(prange, s, l, trigger);\n-\tsvm_range_unmap_split(mm, prange, prange, start, last);\n+\tsvm_range_unmap_split(prange, prange, start, last);\n \n \tif (unmap_parent)\n \t\tsvm_range_add_list_work(svms, prange, mm, SVM_OP_UNMAP_RANGE);\n@@ -2570,8 +2570,6 @@ svm_range_cpu_invalidate_pagetables(struct mmu_interval_notifier *mni,\n \n \tif (range->event == MMU_NOTIFY_RELEASE)\n \t\treturn true;\n-\tif (!mmget_not_zero(mni->mm))\n-\t\treturn true;\n \n \tstart = mni->interval_tree.start;\n \tlast = mni->interval_tree.last;\n@@ -2598,7 +2596,6 @@ svm_range_cpu_invalidate_pagetables(struct mmu_interval_notifier *mni,\n \t}\n \n \tsvm_range_unlock(prange);\n-\tmmput(mni->mm);\n \n \treturn true;\n }\ndiff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\nindex 0b8ac9edc070..f58fa5da7fe5 100644\n--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\n+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\n@@ -3610,13 +3610,15 @@ static void update_connector_ext_caps(struct amdgpu_dm_connector *aconnector)\n \n \tluminance_range = &conn_base->display_info.luminance_range;\n \n-\tif (luminance_range->max_luminance) {\n-\t\tcaps->aux_min_input_signal = luminance_range->min_luminance;\n+\tif (luminance_range->max_luminance)\n \t\tcaps->aux_max_input_signal = luminance_range->max_luminance;\n-\t} else {\n-\t\tcaps->aux_min_input_signal = 0;\n+\telse\n \t\tcaps->aux_max_input_signal = 512;\n-\t}\n+\n+\tif (luminance_range->min_luminance)\n+\t\tcaps->aux_min_input_signal = luminance_range->min_luminance;\n+\telse\n+\t\tcaps->aux_min_input_signal = 1;\n \n \tmin_input_signal_override = drm_get_panel_min_brightness_quirk(aconnector->drm_edid);\n \tif (min_input_signal_override >= 0)\ndiff --git a/drivers/gpu/drm/amd/display/dc/dc_hw_types.h b/drivers/gpu/drm/amd/display/dc/dc_hw_types.h\nindex d562ddeca512..c9f6c6275ca1 100644\n--- a/drivers/gpu/drm/amd/display/dc/dc_hw_types.h\n+++ b/drivers/gpu/drm/amd/display/dc/dc_hw_types.h\n@@ -974,6 +974,7 @@ struct dc_crtc_timing {\n \tuint32_t pix_clk_100hz;\n \n \tuint32_t min_refresh_in_uhz;\n+\tuint32_t max_refresh_in_uhz;\n \n \tuint32_t vic;\n \tuint32_t hdmi_vic;\ndiff --git a/drivers/gpu/drm/amd/display/modules/freesync/freesync.c b/drivers/gpu/drm/amd/display/modules/freesync/freesync.c\nindex 3ba9b62ba70b..250f09922d2f 100644\n--- a/drivers/gpu/drm/amd/display/modules/freesync/freesync.c\n+++ b/drivers/gpu/drm/amd/display/modules/freesync/freesync.c\n@@ -155,6 +155,14 @@ unsigned int mod_freesync_calc_v_total_from_refresh(\n \t\tv_total = div64_u64(div64_u64(((unsigned long long)(\n \t\t\t\tframe_duration_in_ns) * (stream->timing.pix_clk_100hz / 10)),\n \t\t\t\tstream->timing.h_total), 1000000);\n+\t} else if (refresh_in_uhz >= stream->timing.max_refresh_in_uhz) {\n+\t\t/* When the target refresh rate is the maximum panel refresh rate\n+\t\t * round up the vtotal value to prevent off-by-one error causing\n+\t\t * v_total_min to be below the panel's lower bound\n+\t\t */\n+\t\tv_total = div64_u64(div64_u64(((unsigned long long)(\n+\t\t\t\tframe_duration_in_ns) * (stream->timing.pix_clk_100hz / 10)),\n+\t\t\t\tstream->timing.h_total) + (1000000 - 1), 1000000);\n \t} else {\n \t\tv_total = div64_u64(div64_u64(((unsigned long long)(\n \t\t\t\tframe_duration_in_ns) * (stream->timing.pix_clk_100hz / 10)),",
    "stats": {
      "insertions": 67,
      "deletions": 31,
      "files": 10
    }
  },
  {
    "sha": "c6e8d51b37d2ca37dee63753fd240bcbc6402ad3",
    "message": "bcachefs: Work around deadlock to btree node rewrites in journal replay\n\nDon't mark btree nodes for rewrites, if they are or would be degraded,\nif journal replay hasn't finished, to avoid a deadlock.\n\nThis is because btree node rewrites generate more updates for the\ninterior updates (alloc, backpointers), and if those updates touch\nnew nodes and generate more rewrites - we can only have so many interior\nbtree updates in flight before we deadlock on open_buckets.\n\nThe biggest cause is that we don't use the btree write buffer (for\nthe backpointer updates - this needs some real thought on locking in\norder to fix.\n\nThe problem with this workaround (not doing the rewrite for degraded\nnodes in journal replay) is that those degraded nodes persist, and we\ndon't want that (this is a real bug when a btree node write completes\nwith fewer replicas than we wanted and leaves a degraded node due to\ndevice _removal_, i.e. the device went away mid write).\n\nIt's less of a bug here, but still a problem because we don't yet\nhave a way of tracking degraded data - we another index (all\nextents/btree nodes, by replicas entry) in order to fix properly\n(re-replicate degraded data at the earliest possible time).\n\nSigned-off-by: Kent Overstreet <kent.overstreet@linux.dev>",
    "author": "Kent Overstreet",
    "date": "2025-07-01T19:33:46-04:00",
    "files_changed": [
      "fs/bcachefs/btree_io.c"
    ],
    "diff": "diff --git a/fs/bcachefs/btree_io.c b/fs/bcachefs/btree_io.c\nindex 08b22bddd747..e874a4357f64 100644\n--- a/fs/bcachefs/btree_io.c\n+++ b/fs/bcachefs/btree_io.c\n@@ -1337,15 +1337,42 @@ int bch2_btree_node_read_done(struct bch_fs *c, struct bch_dev *ca,\n \n \tbtree_node_reset_sib_u64s(b);\n \n-\tscoped_guard(rcu)\n-\t\tbkey_for_each_ptr(bch2_bkey_ptrs(bkey_i_to_s(&b->key)), ptr) {\n-\t\t\tstruct bch_dev *ca2 = bch2_dev_rcu(c, ptr->dev);\n-\n-\t\t\tif (!ca2 || ca2->mi.state != BCH_MEMBER_STATE_rw) {\n-\t\t\t\tset_btree_node_need_rewrite(b);\n-\t\t\t\tset_btree_node_need_rewrite_degraded(b);\n+\t/*\n+\t * XXX:\n+\t *\n+\t * We deadlock if too many btree updates require node rewrites while\n+\t * we're still in journal replay.\n+\t *\n+\t * This is because btree node rewrites generate more updates for the\n+\t * interior updates (alloc, backpointers), and if those updates touch\n+\t * new nodes and generate more rewrites - well, you see the problem.\n+\t *\n+\t * The biggest cause is that we don't use the btree write buffer (for\n+\t * the backpointer updates - this needs some real thought on locking in\n+\t * order to fix.\n+\t *\n+\t * The problem with this workaround (not doing the rewrite for degraded\n+\t * nodes in journal replay) is that those degraded nodes persist, and we\n+\t * don't want that (this is a real bug when a btree node write completes\n+\t * with fewer replicas than we wanted and leaves a degraded node due to\n+\t * device _removal_, i.e. the device went away mid write).\n+\t *\n+\t * It's less of a bug here, but still a problem because we don't yet\n+\t * have a way of tracking degraded data - we another index (all\n+\t * extents/btree nodes, by replicas entry) in order to fix properly\n+\t * (re-replicate degraded data at the earliest possible time).\n+\t */\n+\tif (c->recovery.passes_complete & BIT_ULL(BCH_RECOVERY_PASS_journal_replay)) {\n+\t\tscoped_guard(rcu)\n+\t\t\tbkey_for_each_ptr(bch2_bkey_ptrs(bkey_i_to_s(&b->key)), ptr) {\n+\t\t\t\tstruct bch_dev *ca2 = bch2_dev_rcu(c, ptr->dev);\n+\n+\t\t\t\tif (!ca2 || ca2->mi.state != BCH_MEMBER_STATE_rw) {\n+\t\t\t\t\tset_btree_node_need_rewrite(b);\n+\t\t\t\t\tset_btree_node_need_rewrite_degraded(b);\n+\t\t\t\t}\n \t\t\t}\n-\t\t}\n+\t}\n \n \tif (!ptr_written) {\n \t\tset_btree_node_need_rewrite(b);",
    "stats": {
      "insertions": 35,
      "deletions": 8,
      "files": 1
    }
  },
  {
    "sha": "d5cb81ba929c1b0d02dadd4be27fc1440dd2e014",
    "message": "Merge patch series \"netfs, cifs: Fixes to retry-related code\"\n\nDavid Howells <dhowells@redhat.com> says:\n\nHere are some miscellaneous fixes and changes for netfslib and cifs, if you\ncould consider pulling them.\n\nMany of these were found because a bug in Samba was causing smbd to crash\nand restart after about 1-2s and this was vigorously and abruptly\nexercising the netfslib retry paths.\n\nSubsequent testing of the cifs RDMA support showed up some more bugs, but\nthe fixes for those went via the cifs tree and have been removed from this set\nas they're now upstream.\n\nFirst, there are some netfs fixes:\n\n (1) Fix a hang due to missing case in final DIO read result collection\n     not breaking out of a loop if the request finished, but there were no\n     subrequests being processed and NETFS_RREQ_ALL_QUEUED wasn't yet set.\n\n (2) Fix a double put of the netfs_io_request struct if completion happened\n     in the pause loop.\n\n (3) Provide some helpers to abstract out NETFS_RREQ_IN_PROGRESS flag\n     wrangling.\n\n (4) Fix infinite looping in netfs_wait_for_pause/request() which wa caused\n     by a loop waiting for NETFS_RREQ_ALL_QUEUED to get set - but which\n     wouldn't get set until the looping function returned.  This uses patch\n     (3) above.\n\n (5) Fix a ref leak on an extra subrequest inserted into a request's list\n     of subreqs because more subreq records were needed for retrying than\n     were needed for the original request (say, for instance, that the\n     amount of cifs credit available was reduced and, subsequently, the ops\n     had to be smaller).\n\nThen a bunch of cifs fixes, some of which are from other people:\n\n (6-8) cifs: Fix various RPC callbacks to set NETFS_SREQ_NEED_RETRY if a\n     subrequest fails retriably.\n\n(10) Fix a warning in the workqueue code when reconnecting a channel.\n\nFollowed by some patches to deal with i_size handling:\n\n(11) Fix the updating of i_size to use a lock to avoid a race between\n     testing if we should have extended the file with a DIO write and\n     changing i_size.\n\n(12) A follow-up patch to (11) to merge the places in netfslib that update\n     i_size on write.\n\nAnd finally a couple of patches to improve tracing output, but that should\notherwise not affect functionality:\n\n(13) Renumber the NETFS_RREQ_* flags to make the hex values easier to\n     interpret by eye, including moving the main status flags down to the\n     lowest bits, with IN_PROGRESS in bit 0.\n\n(14) Update the tracepoints in a number of ways, including adding more\n     tracepoints into the cifs read/write RPC callback so that differend\n     MID_RESPONSE_* values can be differentiated.\n\n* patches from https://lore.kernel.org/20250701163852.2171681-1-dhowells@redhat.com:\n  netfs: Update tracepoints in a number of ways\n  netfs: Renumber the NETFS_RREQ_* flags to make traces easier to read\n  netfs: Merge i_size update functions\n  netfs: Fix i_size updating\n  smb: client: set missing retry flag in cifs_writev_callback()\n  smb: client: set missing retry flag in cifs_readv_callback()\n  smb: client: set missing retry flag in smb2_writev_callback()\n  netfs: Fix ref leak on inserted extra subreq in write retry\n  netfs: Fix looping in wait functions\n  netfs: Provide helpers to perform NETFS_RREQ_IN_PROGRESS flag wangling\n  netfs: Fix double put of request\n  netfs: Fix hang due to missing case in final DIO read result collection\n\nLink: https://lore.kernel.org/20250701163852.2171681-1-dhowells@redhat.com\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
    "author": "Christian Brauner",
    "date": "2025-07-01T22:37:20+02:00",
    "files_changed": [
      "fs/netfs/buffered_write.c",
      "fs/netfs/direct_write.c",
      "fs/netfs/internal.h",
      "fs/netfs/main.c",
      "fs/netfs/misc.c",
      "fs/netfs/read_collect.c",
      "fs/netfs/write_collect.c",
      "fs/netfs/write_retry.c",
      "fs/smb/client/cifssmb.c",
      "fs/smb/client/smb2pdu.c",
      "include/linux/netfs.h",
      "include/trace/events/netfs.h"
    ],
    "diff": "diff --git a/fs/netfs/buffered_write.c b/fs/netfs/buffered_write.c\nindex 72a3e6db2524..f27ea5099a68 100644\n--- a/fs/netfs/buffered_write.c\n+++ b/fs/netfs/buffered_write.c\n@@ -53,30 +53,40 @@ static struct folio *netfs_grab_folio_for_write(struct address_space *mapping,\n  * data written into the pagecache until we can find out from the server what\n  * the values actually are.\n  */\n-static void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n-\t\t\t\tloff_t i_size, loff_t pos, size_t copied)\n+void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n+\t\t\t loff_t pos, size_t copied)\n {\n+\tloff_t i_size, end = pos + copied;\n \tblkcnt_t add;\n \tsize_t gap;\n \n+\tif (end <= i_size_read(inode))\n+\t\treturn;\n+\n \tif (ctx->ops->update_i_size) {\n-\t\tctx->ops->update_i_size(inode, pos);\n+\t\tctx->ops->update_i_size(inode, end);\n \t\treturn;\n \t}\n \n-\ti_size_write(inode, pos);\n+\tspin_lock(&inode->i_lock);\n+\n+\ti_size = i_size_read(inode);\n+\tif (end > i_size) {\n+\t\ti_size_write(inode, end);\n #if IS_ENABLED(CONFIG_FSCACHE)\n-\tfscache_update_cookie(ctx->cache, NULL, &pos);\n+\t\tfscache_update_cookie(ctx->cache, NULL, &end);\n #endif\n \n-\tgap = SECTOR_SIZE - (i_size & (SECTOR_SIZE - 1));\n-\tif (copied > gap) {\n-\t\tadd = DIV_ROUND_UP(copied - gap, SECTOR_SIZE);\n+\t\tgap = SECTOR_SIZE - (i_size & (SECTOR_SIZE - 1));\n+\t\tif (copied > gap) {\n+\t\t\tadd = DIV_ROUND_UP(copied - gap, SECTOR_SIZE);\n \n-\t\tinode->i_blocks = min_t(blkcnt_t,\n-\t\t\t\t\tDIV_ROUND_UP(pos, SECTOR_SIZE),\n-\t\t\t\t\tinode->i_blocks + add);\n+\t\t\tinode->i_blocks = min_t(blkcnt_t,\n+\t\t\t\t\t\tDIV_ROUND_UP(end, SECTOR_SIZE),\n+\t\t\t\t\t\tinode->i_blocks + add);\n+\t\t}\n \t}\n+\tspin_unlock(&inode->i_lock);\n }\n \n /**\n@@ -111,7 +121,7 @@ ssize_t netfs_perform_write(struct kiocb *iocb, struct iov_iter *iter,\n \tstruct folio *folio = NULL, *writethrough = NULL;\n \tunsigned int bdp_flags = (iocb->ki_flags & IOCB_NOWAIT) ? BDP_ASYNC : 0;\n \tssize_t written = 0, ret, ret2;\n-\tloff_t i_size, pos = iocb->ki_pos;\n+\tloff_t pos = iocb->ki_pos;\n \tsize_t max_chunk = mapping_max_folio_size(mapping);\n \tbool maybe_trouble = false;\n \n@@ -344,10 +354,8 @@ ssize_t netfs_perform_write(struct kiocb *iocb, struct iov_iter *iter,\n \t\tflush_dcache_folio(folio);\n \n \t\t/* Update the inode size if we moved the EOF marker */\n+\t\tnetfs_update_i_size(ctx, inode, pos, copied);\n \t\tpos += copied;\n-\t\ti_size = i_size_read(inode);\n-\t\tif (pos > i_size)\n-\t\t\tnetfs_update_i_size(ctx, inode, i_size, pos, copied);\n \t\twritten += copied;\n \n \t\tif (likely(!wreq)) {\ndiff --git a/fs/netfs/direct_write.c b/fs/netfs/direct_write.c\nindex fa9a5bf3c6d5..a16660ab7f83 100644\n--- a/fs/netfs/direct_write.c\n+++ b/fs/netfs/direct_write.c\n@@ -9,20 +9,6 @@\n #include <linux/uio.h>\n #include \"internal.h\"\n \n-static void netfs_cleanup_dio_write(struct netfs_io_request *wreq)\n-{\n-\tstruct inode *inode = wreq->inode;\n-\tunsigned long long end = wreq->start + wreq->transferred;\n-\n-\tif (!wreq->error &&\n-\t    i_size_read(inode) < end) {\n-\t\tif (wreq->netfs_ops->update_i_size)\n-\t\t\twreq->netfs_ops->update_i_size(inode, end);\n-\t\telse\n-\t\t\ti_size_write(inode, end);\n-\t}\n-}\n-\n /*\n  * Perform an unbuffered write where we may have to do an RMW operation on an\n  * encrypted file.  This can also be used for direct I/O writes.\n@@ -98,7 +84,6 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \tif (async)\n \t\twreq->iocb = iocb;\n \twreq->len = iov_iter_count(&wreq->buffer.iter);\n-\twreq->cleanup = netfs_cleanup_dio_write;\n \tret = netfs_unbuffered_write(wreq, is_sync_kiocb(iocb), wreq->len);\n \tif (ret < 0) {\n \t\t_debug(\"begin = %zd\", ret);\n@@ -106,7 +91,6 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t}\n \n \tif (!async) {\n-\t\ttrace_netfs_rreq(wreq, netfs_rreq_trace_wait_ip);\n \t\tret = netfs_wait_for_write(wreq);\n \t\tif (ret > 0)\n \t\t\tiocb->ki_pos += ret;\ndiff --git a/fs/netfs/internal.h b/fs/netfs/internal.h\nindex e2ee9183392b..d4f16fefd965 100644\n--- a/fs/netfs/internal.h\n+++ b/fs/netfs/internal.h\n@@ -27,6 +27,12 @@ void netfs_cache_read_terminated(void *priv, ssize_t transferred_or_error);\n int netfs_prefetch_for_write(struct file *file, struct folio *folio,\n \t\t\t     size_t offset, size_t len);\n \n+/*\n+ * buffered_write.c\n+ */\n+void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n+\t\t\t loff_t pos, size_t copied);\n+\n /*\n  * main.c\n  */\n@@ -267,13 +273,31 @@ static inline void netfs_wake_rreq_flag(struct netfs_io_request *rreq,\n \t\t\t\t\tenum netfs_rreq_trace trace)\n {\n \tif (test_bit(rreq_flag, &rreq->flags)) {\n-\t\ttrace_netfs_rreq(rreq, trace);\n \t\tclear_bit_unlock(rreq_flag, &rreq->flags);\n \t\tsmp_mb__after_atomic(); /* Set flag before task state */\n+\t\ttrace_netfs_rreq(rreq, trace);\n \t\twake_up(&rreq->waitq);\n \t}\n }\n \n+/*\n+ * Test the NETFS_RREQ_IN_PROGRESS flag, inserting an appropriate barrier.\n+ */\n+static inline bool netfs_check_rreq_in_progress(const struct netfs_io_request *rreq)\n+{\n+\t/* Order read of flags before read of anything else, such as error. */\n+\treturn test_bit_acquire(NETFS_RREQ_IN_PROGRESS, &rreq->flags);\n+}\n+\n+/*\n+ * Test the NETFS_SREQ_IN_PROGRESS flag, inserting an appropriate barrier.\n+ */\n+static inline bool netfs_check_subreq_in_progress(const struct netfs_io_subrequest *subreq)\n+{\n+\t/* Order read of flags before read of anything else, such as error. */\n+\treturn test_bit_acquire(NETFS_SREQ_IN_PROGRESS, &subreq->flags);\n+}\n+\n /*\n  * fscache-cache.c\n  */\ndiff --git a/fs/netfs/main.c b/fs/netfs/main.c\nindex 3db401d269e7..73da6c9f5777 100644\n--- a/fs/netfs/main.c\n+++ b/fs/netfs/main.c\n@@ -58,15 +58,15 @@ static int netfs_requests_seq_show(struct seq_file *m, void *v)\n \n \tif (v == &netfs_io_requests) {\n \t\tseq_puts(m,\n-\t\t\t \"REQUEST  OR REF FL ERR  OPS COVERAGE\\n\"\n-\t\t\t \"======== == === == ==== === =========\\n\"\n+\t\t\t \"REQUEST  OR REF FLAG ERR  OPS COVERAGE\\n\"\n+\t\t\t \"======== == === ==== ==== === =========\\n\"\n \t\t\t );\n \t\treturn 0;\n \t}\n \n \trreq = list_entry(v, struct netfs_io_request, proc_link);\n \tseq_printf(m,\n-\t\t   \"%08x %s %3d %2lx %4ld %3d @%04llx %llx/%llx\",\n+\t\t   \"%08x %s %3d %4lx %4ld %3d @%04llx %llx/%llx\",\n \t\t   rreq->debug_id,\n \t\t   netfs_origins[rreq->origin],\n \t\t   refcount_read(&rreq->ref),\ndiff --git a/fs/netfs/misc.c b/fs/netfs/misc.c\nindex 43b67a28a8fa..20748bcfbf59 100644\n--- a/fs/netfs/misc.c\n+++ b/fs/netfs/misc.c\n@@ -356,22 +356,22 @@ void netfs_wait_for_in_progress_stream(struct netfs_io_request *rreq,\n \tDEFINE_WAIT(myself);\n \n \tlist_for_each_entry(subreq, &stream->subrequests, rreq_link) {\n-\t\tif (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags))\n+\t\tif (!netfs_check_subreq_in_progress(subreq))\n \t\t\tcontinue;\n \n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_quiesce);\n \t\tfor (;;) {\n \t\t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n-\t\t\tif (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags))\n+\t\t\tif (!netfs_check_subreq_in_progress(subreq))\n \t\t\t\tbreak;\n \n \t\t\ttrace_netfs_sreq(subreq, netfs_sreq_trace_wait_for);\n \t\t\tschedule();\n-\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t\t}\n \t}\n \n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_quiesce);\n \tfinish_wait(&rreq->waitq, &myself);\n }\n \n@@ -381,7 +381,12 @@ void netfs_wait_for_in_progress_stream(struct netfs_io_request *rreq,\n static int netfs_collect_in_app(struct netfs_io_request *rreq,\n \t\t\t\tbool (*collector)(struct netfs_io_request *rreq))\n {\n-\tbool need_collect = false, inactive = true;\n+\tbool need_collect = false, inactive = true, done = true;\n+\n+\tif (!netfs_check_rreq_in_progress(rreq)) {\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_recollect);\n+\t\treturn 1; /* Done */\n+\t}\n \n \tfor (int i = 0; i < NR_IO_STREAMS; i++) {\n \t\tstruct netfs_io_subrequest *subreq;\n@@ -395,14 +400,16 @@ static int netfs_collect_in_app(struct netfs_io_request *rreq,\n \t\t\t\t\t\t  struct netfs_io_subrequest,\n \t\t\t\t\t\t  rreq_link);\n \t\tif (subreq &&\n-\t\t    (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags) ||\n+\t\t    (!netfs_check_subreq_in_progress(subreq) ||\n \t\t     test_bit(NETFS_SREQ_MADE_PROGRESS, &subreq->flags))) {\n \t\t\tneed_collect = true;\n \t\t\tbreak;\n \t\t}\n+\t\tif (subreq || !test_bit(NETFS_RREQ_ALL_QUEUED, &rreq->flags))\n+\t\t\tdone = false;\n \t}\n \n-\tif (!need_collect && !inactive)\n+\tif (!need_collect && !inactive && !done)\n \t\treturn 0; /* Sleep */\n \n \t__set_current_state(TASK_RUNNING);\n@@ -423,14 +430,13 @@ static int netfs_collect_in_app(struct netfs_io_request *rreq,\n /*\n  * Wait for a request to complete, successfully or otherwise.\n  */\n-static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n-\t\t\t\t      bool (*collector)(struct netfs_io_request *rreq))\n+static ssize_t netfs_wait_for_in_progress(struct netfs_io_request *rreq,\n+\t\t\t\t\t  bool (*collector)(struct netfs_io_request *rreq))\n {\n \tDEFINE_WAIT(myself);\n \tssize_t ret;\n \n \tfor (;;) {\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n \t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n \t\tif (!test_bit(NETFS_RREQ_OFFLOAD_COLLECTION, &rreq->flags)) {\n@@ -440,18 +446,22 @@ static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n \t\t\tcase 1:\n \t\t\t\tgoto all_collected;\n \t\t\tcase 2:\n+\t\t\t\tif (!netfs_check_rreq_in_progress(rreq))\n+\t\t\t\t\tbreak;\n+\t\t\t\tcond_resched();\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n \n-\t\tif (!test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags))\n+\t\tif (!netfs_check_rreq_in_progress(rreq))\n \t\t\tbreak;\n \n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_ip);\n \t\tschedule();\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t}\n \n all_collected:\n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_ip);\n \tfinish_wait(&rreq->waitq, &myself);\n \n \tret = rreq->error;\n@@ -478,12 +488,12 @@ static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n \n ssize_t netfs_wait_for_read(struct netfs_io_request *rreq)\n {\n-\treturn netfs_wait_for_request(rreq, netfs_read_collection);\n+\treturn netfs_wait_for_in_progress(rreq, netfs_read_collection);\n }\n \n ssize_t netfs_wait_for_write(struct netfs_io_request *rreq)\n {\n-\treturn netfs_wait_for_request(rreq, netfs_write_collection);\n+\treturn netfs_wait_for_in_progress(rreq, netfs_write_collection);\n }\n \n /*\n@@ -494,10 +504,8 @@ static void netfs_wait_for_pause(struct netfs_io_request *rreq,\n {\n \tDEFINE_WAIT(myself);\n \n-\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_pause);\n-\n \tfor (;;) {\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_pause);\n \t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n \t\tif (!test_bit(NETFS_RREQ_OFFLOAD_COLLECTION, &rreq->flags)) {\n@@ -507,19 +515,23 @@ static void netfs_wait_for_pause(struct netfs_io_request *rreq,\n \t\t\tcase 1:\n \t\t\t\tgoto all_collected;\n \t\t\tcase 2:\n+\t\t\t\tif (!netfs_check_rreq_in_progress(rreq) ||\n+\t\t\t\t    !test_bit(NETFS_RREQ_PAUSE, &rreq->flags))\n+\t\t\t\t\tbreak;\n+\t\t\t\tcond_resched();\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n \n-\t\tif (!test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags) ||\n+\t\tif (!netfs_check_rreq_in_progress(rreq) ||\n \t\t    !test_bit(NETFS_RREQ_PAUSE, &rreq->flags))\n \t\t\tbreak;\n \n \t\tschedule();\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t}\n \n all_collected:\n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_pause);\n \tfinish_wait(&rreq->waitq, &myself);\n }\n \ndiff --git a/fs/netfs/read_collect.c b/fs/netfs/read_collect.c\nindex 96ee18af28ef..3e804da1e1eb 100644\n--- a/fs/netfs/read_collect.c\n+++ b/fs/netfs/read_collect.c\n@@ -218,7 +218,7 @@ static void netfs_collect_read_results(struct netfs_io_request *rreq)\n \t\t\tstream->collected_to = front->start;\n \t\t}\n \n-\t\tif (test_bit(NETFS_SREQ_IN_PROGRESS, &front->flags))\n+\t\tif (netfs_check_subreq_in_progress(front))\n \t\t\tnotes |= HIT_PENDING;\n \t\tsmp_rmb(); /* Read counters after IN_PROGRESS flag. */\n \t\ttransferred = READ_ONCE(front->transferred);\n@@ -293,7 +293,9 @@ static void netfs_collect_read_results(struct netfs_io_request *rreq)\n \t\tspin_lock(&rreq->lock);\n \n \t\tremove = front;\n-\t\ttrace_netfs_sreq(front, netfs_sreq_trace_discard);\n+\t\ttrace_netfs_sreq(front,\n+\t\t\t\t notes & ABANDON_SREQ ?\n+\t\t\t\t netfs_sreq_trace_abandoned : netfs_sreq_trace_consumed);\n \t\tlist_del_init(&front->rreq_link);\n \t\tfront = list_first_entry_or_null(&stream->subrequests,\n \t\t\t\t\t\t struct netfs_io_subrequest, rreq_link);\n@@ -353,9 +355,11 @@ static void netfs_rreq_assess_dio(struct netfs_io_request *rreq)\n \n \tif (rreq->iocb) {\n \t\trreq->iocb->ki_pos += rreq->transferred;\n-\t\tif (rreq->iocb->ki_complete)\n+\t\tif (rreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_ki_complete);\n \t\t\trreq->iocb->ki_complete(\n \t\t\t\trreq->iocb, rreq->error ? rreq->error : rreq->transferred);\n+\t\t}\n \t}\n \tif (rreq->netfs_ops->done)\n \t\trreq->netfs_ops->done(rreq);\n@@ -379,9 +383,11 @@ static void netfs_rreq_assess_single(struct netfs_io_request *rreq)\n \n \tif (rreq->iocb) {\n \t\trreq->iocb->ki_pos += rreq->transferred;\n-\t\tif (rreq->iocb->ki_complete)\n+\t\tif (rreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_ki_complete);\n \t\t\trreq->iocb->ki_complete(\n \t\t\t\trreq->iocb, rreq->error ? rreq->error : rreq->transferred);\n+\t\t}\n \t}\n \tif (rreq->netfs_ops->done)\n \t\trreq->netfs_ops->done(rreq);\n@@ -445,7 +451,7 @@ void netfs_read_collection_worker(struct work_struct *work)\n \tstruct netfs_io_request *rreq = container_of(work, struct netfs_io_request, work);\n \n \tnetfs_see_request(rreq, netfs_rreq_trace_see_work);\n-\tif (test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags)) {\n+\tif (netfs_check_rreq_in_progress(rreq)) {\n \t\tif (netfs_read_collection(rreq))\n \t\t\t/* Drop the ref from the IN_PROGRESS flag. */\n \t\t\tnetfs_put_request(rreq, netfs_rreq_trace_put_work_ip);\ndiff --git a/fs/netfs/write_collect.c b/fs/netfs/write_collect.c\nindex e2b102ffb768..0f3a36852a4d 100644\n--- a/fs/netfs/write_collect.c\n+++ b/fs/netfs/write_collect.c\n@@ -240,7 +240,7 @@ static void netfs_collect_write_results(struct netfs_io_request *wreq)\n \t\t\t}\n \n \t\t\t/* Stall if the front is still undergoing I/O. */\n-\t\t\tif (test_bit(NETFS_SREQ_IN_PROGRESS, &front->flags)) {\n+\t\t\tif (netfs_check_subreq_in_progress(front)) {\n \t\t\t\tnotes |= HIT_PENDING;\n \t\t\t\tbreak;\n \t\t\t}\n@@ -393,8 +393,10 @@ bool netfs_write_collection(struct netfs_io_request *wreq)\n \t\tictx->ops->invalidate_cache(wreq);\n \t}\n \n-\tif (wreq->cleanup)\n-\t\twreq->cleanup(wreq);\n+\tif ((wreq->origin == NETFS_UNBUFFERED_WRITE ||\n+\t     wreq->origin == NETFS_DIO_WRITE) &&\n+\t    !wreq->error)\n+\t\tnetfs_update_i_size(ictx, &ictx->inode, wreq->start, wreq->transferred);\n \n \tif (wreq->origin == NETFS_DIO_WRITE &&\n \t    wreq->mapping->nrpages) {\n@@ -419,9 +421,11 @@ bool netfs_write_collection(struct netfs_io_request *wreq)\n \tif (wreq->iocb) {\n \t\tsize_t written = min(wreq->transferred, wreq->len);\n \t\twreq->iocb->ki_pos += written;\n-\t\tif (wreq->iocb->ki_complete)\n+\t\tif (wreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(wreq, netfs_rreq_trace_ki_complete);\n \t\t\twreq->iocb->ki_complete(\n \t\t\t\twreq->iocb, wreq->error ? wreq->error : written);\n+\t\t}\n \t\twreq->iocb = VFS_PTR_POISON;\n \t}\n \n@@ -434,7 +438,7 @@ void netfs_write_collection_worker(struct work_struct *work)\n \tstruct netfs_io_request *rreq = container_of(work, struct netfs_io_request, work);\n \n \tnetfs_see_request(rreq, netfs_rreq_trace_see_work);\n-\tif (test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags)) {\n+\tif (netfs_check_rreq_in_progress(rreq)) {\n \t\tif (netfs_write_collection(rreq))\n \t\t\t/* Drop the ref from the IN_PROGRESS flag. */\n \t\t\tnetfs_put_request(rreq, netfs_rreq_trace_put_work_ip);\ndiff --git a/fs/netfs/write_retry.c b/fs/netfs/write_retry.c\nindex 9d1d8a8bab72..fc9c3e0d34d8 100644\n--- a/fs/netfs/write_retry.c\n+++ b/fs/netfs/write_retry.c\n@@ -146,14 +146,13 @@ static void netfs_retry_write_stream(struct netfs_io_request *wreq,\n \t\t\tsubreq = netfs_alloc_subrequest(wreq);\n \t\t\tsubreq->source\t\t= to->source;\n \t\t\tsubreq->start\t\t= start;\n-\t\t\tsubreq->debug_index\t= atomic_inc_return(&wreq->subreq_counter);\n \t\t\tsubreq->stream_nr\t= to->stream_nr;\n \t\t\tsubreq->retry_count\t= 1;\n \n \t\t\ttrace_netfs_sreq_ref(wreq->debug_id, subreq->debug_index,\n \t\t\t\t\t     refcount_read(&subreq->ref),\n \t\t\t\t\t     netfs_sreq_trace_new);\n-\t\t\tnetfs_get_subrequest(subreq, netfs_sreq_trace_get_resubmit);\n+\t\t\ttrace_netfs_sreq(subreq, netfs_sreq_trace_split);\n \n \t\t\tlist_add(&subreq->rreq_link, &to->rreq_link);\n \t\t\tto = list_next_entry(to, rreq_link);\ndiff --git a/fs/smb/client/cifssmb.c b/fs/smb/client/cifssmb.c\nindex 7216fcec79e8..75142f49d65d 100644\n--- a/fs/smb/client/cifssmb.c\n+++ b/fs/smb/client/cifssmb.c\n@@ -1334,7 +1334,12 @@ cifs_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\tgoto do_retry;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_retry_needed);\n+do_retry:\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &rdata->subreq.flags);\n \t\trdata->result = -EAGAIN;\n \t\tif (server->sign && rdata->got_bytes)\n \t\t\t/* reset bytes number since we can not check a sign */\n@@ -1343,8 +1348,14 @@ cifs_readv_callback(struct mid_q_entry *mid)\n \t\ttask_io_account_read(rdata->got_bytes);\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n+\tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_malformed);\n+\t\trdata->result = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_unknown);\n \t\trdata->result = -EIO;\n+\t\tbreak;\n \t}\n \n \tif (rdata->result == -ENODATA) {\n@@ -1713,10 +1724,21 @@ cifs_writev_callback(struct mid_q_entry *mid)\n \t\t}\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n+\t\tresult = -EAGAIN;\n+\t\tbreak;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_retry_needed);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n \t\tresult = -EAGAIN;\n \t\tbreak;\n+\tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_malformed);\n+\t\tresult = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_unknown);\n \t\tresult = -EIO;\n \t\tbreak;\n \t}\ndiff --git a/fs/smb/client/smb2pdu.c b/fs/smb/client/smb2pdu.c\nindex a717be1626a3..7f6186c2e60d 100644\n--- a/fs/smb/client/smb2pdu.c\n+++ b/fs/smb/client/smb2pdu.c\n@@ -4567,7 +4567,11 @@ smb2_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\tgoto do_retry;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_retry_needed);\n+do_retry:\n \t\t__set_bit(NETFS_SREQ_NEED_RETRY, &rdata->subreq.flags);\n \t\trdata->result = -EAGAIN;\n \t\tif (server->sign && rdata->got_bytes)\n@@ -4578,11 +4582,15 @@ smb2_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_malformed);\n \t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n-\t\tfallthrough;\n+\t\trdata->result = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_unknown);\n \t\trdata->result = -EIO;\n+\t\tbreak;\n \t}\n #ifdef CONFIG_CIFS_SMB_DIRECT\n \t/*\n@@ -4835,11 +4843,14 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \n \tswitch (mid->mid_state) {\n \tcase MID_RESPONSE_RECEIVED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_progress);\n \t\tcredits.value = le16_to_cpu(rsp->hdr.CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n \t\tresult = smb2_check_receive(mid, server, 0);\n-\t\tif (result != 0)\n+\t\tif (result != 0) {\n+\t\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_bad);\n \t\t\tbreak;\n+\t\t}\n \n \t\twritten = le32_to_cpu(rsp->DataLength);\n \t\t/*\n@@ -4861,14 +4872,23 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \t\t}\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n+\t\tresult = -EAGAIN;\n+\t\tbreak;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_retry_needed);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n \t\tresult = -EAGAIN;\n \t\tbreak;\n \tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_malformed);\n \t\tcredits.value = le16_to_cpu(rsp->hdr.CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n-\t\tfallthrough;\n+\t\tresult = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_unknown);\n \t\tresult = -EIO;\n \t\tbreak;\n \t}\n@@ -4908,7 +4928,6 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \t\t\t      server->credits, server->in_flight,\n \t\t\t      0, cifs_trace_rw_credits_write_response_clear);\n \twdata->credits.value = 0;\n-\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_progress);\n \tcifs_write_subrequest_terminated(wdata, result ?: written);\n \trelease_mid(mid);\n \ttrace_smb3_rw_credits(rreq_debug_id, subreq_debug_index, 0,\ndiff --git a/include/linux/netfs.h b/include/linux/netfs.h\nindex 065c17385e53..f43f075852c0 100644\n--- a/include/linux/netfs.h\n+++ b/include/linux/netfs.h\n@@ -265,21 +265,20 @@ struct netfs_io_request {\n \tbool\t\t\tdirect_bv_unpin; /* T if direct_bv[] must be unpinned */\n \trefcount_t\t\tref;\n \tunsigned long\t\tflags;\n-#define NETFS_RREQ_OFFLOAD_COLLECTION\t0\t/* Offload collection to workqueue */\n-#define NETFS_RREQ_NO_UNLOCK_FOLIO\t2\t/* Don't unlock no_unlock_folio on completion */\n-#define NETFS_RREQ_FAILED\t\t4\t/* The request failed */\n-#define NETFS_RREQ_IN_PROGRESS\t\t5\t/* Unlocked when the request completes (has ref) */\n-#define NETFS_RREQ_FOLIO_COPY_TO_CACHE\t6\t/* Copy current folio to cache from read */\n-#define NETFS_RREQ_UPLOAD_TO_SERVER\t8\t/* Need to write to the server */\n-#define NETFS_RREQ_PAUSE\t\t11\t/* Pause subrequest generation */\n+#define NETFS_RREQ_IN_PROGRESS\t\t0\t/* Unlocked when the request completes (has ref) */\n+#define NETFS_RREQ_ALL_QUEUED\t\t1\t/* All subreqs are now queued */\n+#define NETFS_RREQ_PAUSE\t\t2\t/* Pause subrequest generation */\n+#define NETFS_RREQ_FAILED\t\t3\t/* The request failed */\n+#define NETFS_RREQ_RETRYING\t\t4\t/* Set if we're in the retry path */\n+#define NETFS_RREQ_SHORT_TRANSFER\t5\t/* Set if we have a short transfer */\n+#define NETFS_RREQ_OFFLOAD_COLLECTION\t8\t/* Offload collection to workqueue */\n+#define NETFS_RREQ_NO_UNLOCK_FOLIO\t9\t/* Don't unlock no_unlock_folio on completion */\n+#define NETFS_RREQ_FOLIO_COPY_TO_CACHE\t10\t/* Copy current folio to cache from read */\n+#define NETFS_RREQ_UPLOAD_TO_SERVER\t11\t/* Need to write to the server */\n #define NETFS_RREQ_USE_IO_ITER\t\t12\t/* Use ->io_iter rather than ->i_pages */\n-#define NETFS_RREQ_ALL_QUEUED\t\t13\t/* All subreqs are now queued */\n-#define NETFS_RREQ_RETRYING\t\t14\t/* Set if we're in the retry path */\n-#define NETFS_RREQ_SHORT_TRANSFER\t15\t/* Set if we have a short transfer */\n #define NETFS_RREQ_USE_PGPRIV2\t\t31\t/* [DEPRECATED] Use PG_private_2 to mark\n \t\t\t\t\t\t * write to cache on read */\n \tconst struct netfs_request_ops *netfs_ops;\n-\tvoid (*cleanup)(struct netfs_io_request *req);\n };\n \n /*\ndiff --git a/include/trace/events/netfs.h b/include/trace/events/netfs.h\nindex 333d2e38dd2c..73e96ccbe830 100644\n--- a/include/trace/events/netfs.h\n+++ b/include/trace/events/netfs.h\n@@ -50,12 +50,14 @@\n \n #define netfs_rreq_traces\t\t\t\t\t\\\n \tEM(netfs_rreq_trace_assess,\t\t\"ASSESS \")\t\\\n-\tEM(netfs_rreq_trace_copy,\t\t\"COPY   \")\t\\\n \tEM(netfs_rreq_trace_collect,\t\t\"COLLECT\")\t\\\n \tEM(netfs_rreq_trace_complete,\t\t\"COMPLET\")\t\\\n+\tEM(netfs_rreq_trace_copy,\t\t\"COPY   \")\t\\\n \tEM(netfs_rreq_trace_dirty,\t\t\"DIRTY  \")\t\\\n \tEM(netfs_rreq_trace_done,\t\t\"DONE   \")\t\\\n \tEM(netfs_rreq_trace_free,\t\t\"FREE   \")\t\\\n+\tEM(netfs_rreq_trace_ki_complete,\t\"KI-CMPL\")\t\\\n+\tEM(netfs_rreq_trace_recollect,\t\t\"RECLLCT\")\t\\\n \tEM(netfs_rreq_trace_redirty,\t\t\"REDIRTY\")\t\\\n \tEM(netfs_rreq_trace_resubmit,\t\t\"RESUBMT\")\t\\\n \tEM(netfs_rreq_trace_set_abandon,\t\"S-ABNDN\")\t\\\n@@ -63,13 +65,15 @@\n \tEM(netfs_rreq_trace_unlock,\t\t\"UNLOCK \")\t\\\n \tEM(netfs_rreq_trace_unlock_pgpriv2,\t\"UNLCK-2\")\t\\\n \tEM(netfs_rreq_trace_unmark,\t\t\"UNMARK \")\t\\\n+\tEM(netfs_rreq_trace_unpause,\t\t\"UNPAUSE\")\t\\\n \tEM(netfs_rreq_trace_wait_ip,\t\t\"WAIT-IP\")\t\\\n-\tEM(netfs_rreq_trace_wait_pause,\t\t\"WT-PAUS\")\t\\\n-\tEM(netfs_rreq_trace_wait_queue,\t\t\"WAIT-Q \")\t\\\n+\tEM(netfs_rreq_trace_wait_pause,\t\t\"--PAUSED--\")\t\\\n+\tEM(netfs_rreq_trace_wait_quiesce,\t\"WAIT-QUIESCE\")\t\\\n+\tEM(netfs_rreq_trace_waited_ip,\t\t\"DONE-IP\")\t\\\n+\tEM(netfs_rreq_trace_waited_pause,\t\"--UNPAUSED--\")\t\\\n+\tEM(netfs_rreq_trace_waited_quiesce,\t\"DONE-QUIESCE\")\t\\\n \tEM(netfs_rreq_trace_wake_ip,\t\t\"WAKE-IP\")\t\\\n \tEM(netfs_rreq_trace_wake_queue,\t\t\"WAKE-Q \")\t\\\n-\tEM(netfs_rreq_trace_woke_queue,\t\t\"WOKE-Q \")\t\\\n-\tEM(netfs_rreq_trace_unpause,\t\t\"UNPAUSE\")\t\\\n \tE_(netfs_rreq_trace_write_done,\t\t\"WR-DONE\")\n \n #define netfs_sreq_sources\t\t\t\t\t\\\n@@ -82,6 +86,7 @@\n \tE_(NETFS_WRITE_TO_CACHE,\t\t\"WRIT\")\n \n #define netfs_sreq_traces\t\t\t\t\t\\\n+\tEM(netfs_sreq_trace_abandoned,\t\t\"ABNDN\")\t\\\n \tEM(netfs_sreq_trace_add_donations,\t\"+DON \")\t\\\n \tEM(netfs_sreq_trace_added,\t\t\"ADD  \")\t\\\n \tEM(netfs_sreq_trace_cache_nowrite,\t\"CA-NW\")\t\\\n@@ -89,6 +94,7 @@\n \tEM(netfs_sreq_trace_cache_write,\t\"CA-WR\")\t\\\n \tEM(netfs_sreq_trace_cancel,\t\t\"CANCL\")\t\\\n \tEM(netfs_sreq_trace_clear,\t\t\"CLEAR\")\t\\\n+\tEM(netfs_sreq_trace_consumed,\t\t\"CONSM\")\t\\\n \tEM(netfs_sreq_trace_discard,\t\t\"DSCRD\")\t\\\n \tEM(netfs_sreq_trace_donate_to_prev,\t\"DON-P\")\t\\\n \tEM(netfs_sreq_trace_donate_to_next,\t\"DON-N\")\t\\\n@@ -96,7 +102,12 @@\n \tEM(netfs_sreq_trace_fail,\t\t\"FAIL \")\t\\\n \tEM(netfs_sreq_trace_free,\t\t\"FREE \")\t\\\n \tEM(netfs_sreq_trace_hit_eof,\t\t\"EOF  \")\t\\\n-\tEM(netfs_sreq_trace_io_progress,\t\"IO   \")\t\\\n+\tEM(netfs_sreq_trace_io_bad,\t\t\"I-BAD\")\t\\\n+\tEM(netfs_sreq_trace_io_malformed,\t\"I-MLF\")\t\\\n+\tEM(netfs_sreq_trace_io_unknown,\t\t\"I-UNK\")\t\\\n+\tEM(netfs_sreq_trace_io_progress,\t\"I-OK \")\t\\\n+\tEM(netfs_sreq_trace_io_req_submitted,\t\"I-RSB\")\t\\\n+\tEM(netfs_sreq_trace_io_retry_needed,\t\"I-RTR\")\t\\\n \tEM(netfs_sreq_trace_limited,\t\t\"LIMIT\")\t\\\n \tEM(netfs_sreq_trace_need_clear,\t\t\"N-CLR\")\t\\\n \tEM(netfs_sreq_trace_partial_read,\t\"PARTR\")\t\\\n@@ -142,8 +153,8 @@\n \n #define netfs_sreq_ref_traces\t\t\t\t\t\\\n \tEM(netfs_sreq_trace_get_copy_to_cache,\t\"GET COPY2C \")\t\\\n-\tEM(netfs_sreq_trace_get_resubmit,\t\"GET RESUBMIT\")\t\\\n-\tEM(netfs_sreq_trace_get_submit,\t\t\"GET SUBMIT\")\t\\\n+\tEM(netfs_sreq_trace_get_resubmit,\t\"GET RESUBMT\")\t\\\n+\tEM(netfs_sreq_trace_get_submit,\t\t\"GET SUBMIT \")\t\\\n \tEM(netfs_sreq_trace_get_short_read,\t\"GET SHORTRD\")\t\\\n \tEM(netfs_sreq_trace_new,\t\t\"NEW        \")\t\\\n \tEM(netfs_sreq_trace_put_abandon,\t\"PUT ABANDON\")\t\\\n@@ -366,7 +377,7 @@ TRACE_EVENT(netfs_sreq,\n \t\t    __entry->slot\t= sreq->io_iter.folioq_slot;\n \t\t\t   ),\n \n-\t    TP_printk(\"R=%08x[%x] %s %s f=%02x s=%llx %zx/%zx s=%u e=%d\",\n+\t    TP_printk(\"R=%08x[%x] %s %s f=%03x s=%llx %zx/%zx s=%u e=%d\",\n \t\t      __entry->rreq, __entry->index,\n \t\t      __print_symbolic(__entry->source, netfs_sreq_sources),\n \t\t      __print_symbolic(__entry->what, netfs_sreq_traces),",
    "stats": {
      "insertions": 178,
      "deletions": 90,
      "files": 12
    }
  }
]
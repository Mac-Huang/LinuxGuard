[
  {
    "sha": "48fd7ebe00c1cdc782b42576548b25185902f64c",
    "message": "Revert \"bcache: remove heap-related macros and switch to generic min_heap\"\n\nThis reverts commit 866898efbb25bb44fd42848318e46db9e785973a.\n\nThe generic bottom-up min_heap implementation causes performance\nregression in invalidate_buckets_lru(), a hot path in bcache.  Before the\ncache is fully populated, new_bucket_prio() often returns zero, leading to\nmany equal comparisons.  In such cases, bottom-up sift_down performs up to\n2 * log2(n) comparisons, while the original top-down approach completes\nwith just O() comparisons, resulting in a measurable performance gap.\n\nThe performance degradation is further worsened by the non-inlined\nmin_heap API functions introduced in commit 92a8b224b833 (\"lib/min_heap:\nintroduce non-inline versions of min heap API functions\"), adding function\ncall overhead to this critical path.\n\nAs reported by Robert, bcache now suffers from latency spikes, with P100\n(max) latency increasing from 600 ms to 2.4 seconds every 5 minutes. \nThese regressions degrade bcache's effectiveness as a low-latency cache\nlayer and lead to frequent timeouts and application stalls in production\nenvironments.\n\nThis revert aims to restore bcache's original low-latency behavior.\n\nLink: https://lore.kernel.org/lkml/CAJhEC05+0S69z+3+FB2Cd0hD+pCRyWTKLEOsc8BOmH73p1m+KQ@mail.gmail.com\nLink: https://lkml.kernel.org/r/20250614202353.1632957-3-visitorckw@gmail.com\nFixes: 866898efbb25 (\"bcache: remove heap-related macros and switch to generic min_heap\")\nFixes: 92a8b224b833 (\"lib/min_heap: introduce non-inline versions of min heap API functions\")\nSigned-off-by: Kuan-Wei Chiu <visitorckw@gmail.com>\nReported-by: Robert Pang <robertpang@google.com>\nCloses: https://lore.kernel.org/linux-bcache/CAJhEC06F_AtrPgw2-7CvCqZgeStgCtitbD-ryuPpXQA-JG5XXw@mail.gmail.com\nAcked-by: Coly Li <colyli@kernel.org>\nCc: Ching-Chun (Jim) Huang <jserv@ccns.ncku.edu.tw>\nCc: Kent Overstreet <kent.overstreet@linux.dev>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Kuan-Wei Chiu",
    "date": "2025-06-19T20:48:03-07:00",
    "files_changed": [
      "drivers/md/bcache/alloc.c",
      "drivers/md/bcache/bcache.h",
      "drivers/md/bcache/bset.c",
      "drivers/md/bcache/bset.h",
      "drivers/md/bcache/btree.c",
      "drivers/md/bcache/extents.c",
      "drivers/md/bcache/movinggc.c",
      "drivers/md/bcache/super.c",
      "drivers/md/bcache/sysfs.c",
      "drivers/md/bcache/util.h",
      "drivers/md/bcache/writeback.c"
    ],
    "diff": "diff --git a/drivers/md/bcache/alloc.c b/drivers/md/bcache/alloc.c\nindex da50f6661bae..48ce750bf70a 100644\n--- a/drivers/md/bcache/alloc.c\n+++ b/drivers/md/bcache/alloc.c\n@@ -164,68 +164,40 @@ static void bch_invalidate_one_bucket(struct cache *ca, struct bucket *b)\n  * prio is worth 1/8th of what INITIAL_PRIO is worth.\n  */\n \n-static inline unsigned int new_bucket_prio(struct cache *ca, struct bucket *b)\n-{\n-\tunsigned int min_prio = (INITIAL_PRIO - ca->set->min_prio) / 8;\n-\n-\treturn (b->prio - ca->set->min_prio + min_prio) * GC_SECTORS_USED(b);\n-}\n-\n-static inline bool new_bucket_max_cmp(const void *l, const void *r, void *args)\n-{\n-\tstruct bucket **lhs = (struct bucket **)l;\n-\tstruct bucket **rhs = (struct bucket **)r;\n-\tstruct cache *ca = args;\n-\n-\treturn new_bucket_prio(ca, *lhs) > new_bucket_prio(ca, *rhs);\n-}\n-\n-static inline bool new_bucket_min_cmp(const void *l, const void *r, void *args)\n-{\n-\tstruct bucket **lhs = (struct bucket **)l;\n-\tstruct bucket **rhs = (struct bucket **)r;\n-\tstruct cache *ca = args;\n-\n-\treturn new_bucket_prio(ca, *lhs) < new_bucket_prio(ca, *rhs);\n-}\n-\n-static inline void new_bucket_swap(void *l, void *r, void __always_unused *args)\n-{\n-\tstruct bucket **lhs = l, **rhs = r;\n+#define bucket_prio(b)\t\t\t\t\t\t\t\\\n+({\t\t\t\t\t\t\t\t\t\\\n+\tunsigned int min_prio = (INITIAL_PRIO - ca->set->min_prio) / 8;\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t(b->prio - ca->set->min_prio + min_prio) * GC_SECTORS_USED(b);\t\\\n+})\n \n-\tswap(*lhs, *rhs);\n-}\n+#define bucket_max_cmp(l, r)\t(bucket_prio(l) < bucket_prio(r))\n+#define bucket_min_cmp(l, r)\t(bucket_prio(l) > bucket_prio(r))\n \n static void invalidate_buckets_lru(struct cache *ca)\n {\n \tstruct bucket *b;\n-\tconst struct min_heap_callbacks bucket_max_cmp_callback = {\n-\t\t.less = new_bucket_max_cmp,\n-\t\t.swp = new_bucket_swap,\n-\t};\n-\tconst struct min_heap_callbacks bucket_min_cmp_callback = {\n-\t\t.less = new_bucket_min_cmp,\n-\t\t.swp = new_bucket_swap,\n-\t};\n+\tssize_t i;\n \n-\tca->heap.nr = 0;\n+\tca->heap.used = 0;\n \n \tfor_each_bucket(b, ca) {\n \t\tif (!bch_can_invalidate_bucket(ca, b))\n \t\t\tcontinue;\n \n-\t\tif (!min_heap_full(&ca->heap))\n-\t\t\tmin_heap_push(&ca->heap, &b, &bucket_max_cmp_callback, ca);\n-\t\telse if (!new_bucket_max_cmp(&b, min_heap_peek(&ca->heap), ca)) {\n+\t\tif (!heap_full(&ca->heap))\n+\t\t\theap_add(&ca->heap, b, bucket_max_cmp);\n+\t\telse if (bucket_max_cmp(b, heap_peek(&ca->heap))) {\n \t\t\tca->heap.data[0] = b;\n-\t\t\tmin_heap_sift_down(&ca->heap, 0, &bucket_max_cmp_callback, ca);\n+\t\t\theap_sift(&ca->heap, 0, bucket_max_cmp);\n \t\t}\n \t}\n \n-\tmin_heapify_all(&ca->heap, &bucket_min_cmp_callback, ca);\n+\tfor (i = ca->heap.used / 2 - 1; i >= 0; --i)\n+\t\theap_sift(&ca->heap, i, bucket_min_cmp);\n \n \twhile (!fifo_full(&ca->free_inc)) {\n-\t\tif (!ca->heap.nr) {\n+\t\tif (!heap_pop(&ca->heap, b, bucket_min_cmp)) {\n \t\t\t/*\n \t\t\t * We don't want to be calling invalidate_buckets()\n \t\t\t * multiple times when it can't do anything\n@@ -234,8 +206,6 @@ static void invalidate_buckets_lru(struct cache *ca)\n \t\t\twake_up_gc(ca->set);\n \t\t\treturn;\n \t\t}\n-\t\tb = min_heap_peek(&ca->heap)[0];\n-\t\tmin_heap_pop(&ca->heap, &bucket_min_cmp_callback, ca);\n \n \t\tbch_invalidate_one_bucket(ca, b);\n \t}\ndiff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h\nindex 785b0d9008fa..1d33e40d26ea 100644\n--- a/drivers/md/bcache/bcache.h\n+++ b/drivers/md/bcache/bcache.h\n@@ -458,7 +458,7 @@ struct cache {\n \t/* Allocation stuff: */\n \tstruct bucket\t\t*buckets;\n \n-\tDEFINE_MIN_HEAP(struct bucket *, cache_heap) heap;\n+\tDECLARE_HEAP(struct bucket *, heap);\n \n \t/*\n \t * If nonzero, we know we aren't going to find any buckets to invalidate\ndiff --git a/drivers/md/bcache/bset.c b/drivers/md/bcache/bset.c\nindex bd97d8626887..463eb13bd0b2 100644\n--- a/drivers/md/bcache/bset.c\n+++ b/drivers/md/bcache/bset.c\n@@ -54,11 +54,9 @@ void bch_dump_bucket(struct btree_keys *b)\n int __bch_count_data(struct btree_keys *b)\n {\n \tunsigned int ret = 0;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct bkey *k;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \tif (b->ops->is_extents)\n \t\tfor_each_key(b, k, &iter)\n \t\t\tret += KEY_SIZE(k);\n@@ -69,11 +67,9 @@ void __bch_check_keys(struct btree_keys *b, const char *fmt, ...)\n {\n \tva_list args;\n \tstruct bkey *k, *p = NULL;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tconst char *err;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \tfor_each_key(b, k, &iter) {\n \t\tif (b->ops->is_extents) {\n \t\t\terr = \"Keys out of order\";\n@@ -114,9 +110,9 @@ void __bch_check_keys(struct btree_keys *b, const char *fmt, ...)\n \n static void bch_btree_iter_next_check(struct btree_iter *iter)\n {\n-\tstruct bkey *k = iter->heap.data->k, *next = bkey_next(k);\n+\tstruct bkey *k = iter->data->k, *next = bkey_next(k);\n \n-\tif (next < iter->heap.data->end &&\n+\tif (next < iter->data->end &&\n \t    bkey_cmp(k, iter->b->ops->is_extents ?\n \t\t     &START_KEY(next) : next) > 0) {\n \t\tbch_dump_bucket(iter->b);\n@@ -883,14 +879,12 @@ unsigned int bch_btree_insert_key(struct btree_keys *b, struct bkey *k,\n \tunsigned int status = BTREE_INSERT_STATUS_NO_INSERT;\n \tstruct bset *i = bset_tree_last(b)->data;\n \tstruct bkey *m, *prev = NULL;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct bkey preceding_key_on_stack = ZERO_KEY;\n \tstruct bkey *preceding_key_p = &preceding_key_on_stack;\n \n \tBUG_ON(b->ops->is_extents && !KEY_SIZE(k));\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \t/*\n \t * If k has preceding key, preceding_key_p will be set to address\n \t *  of k's preceding key; otherwise preceding_key_p will be set\n@@ -901,9 +895,9 @@ unsigned int bch_btree_insert_key(struct btree_keys *b, struct bkey *k,\n \telse\n \t\tpreceding_key(k, &preceding_key_p);\n \n-\tm = bch_btree_iter_init(b, &iter, preceding_key_p);\n+\tm = bch_btree_iter_stack_init(b, &iter, preceding_key_p);\n \n-\tif (b->ops->insert_fixup(b, k, &iter, replace_key))\n+\tif (b->ops->insert_fixup(b, k, &iter.iter, replace_key))\n \t\treturn status;\n \n \tstatus = BTREE_INSERT_STATUS_INSERT;\n@@ -1083,102 +1077,79 @@ struct bkey *__bch_bset_search(struct btree_keys *b, struct bset_tree *t,\n \n /* Btree iterator */\n \n-typedef bool (new_btree_iter_cmp_fn)(const void *, const void *, void *);\n-\n-static inline bool new_btree_iter_cmp(const void *l, const void *r, void __always_unused *args)\n-{\n-\tconst struct btree_iter_set *_l = l;\n-\tconst struct btree_iter_set *_r = r;\n-\n-\treturn bkey_cmp(_l->k, _r->k) <= 0;\n-}\n+typedef bool (btree_iter_cmp_fn)(struct btree_iter_set,\n+\t\t\t\t struct btree_iter_set);\n \n-static inline void new_btree_iter_swap(void *iter1, void *iter2, void __always_unused *args)\n+static inline bool btree_iter_cmp(struct btree_iter_set l,\n+\t\t\t\t  struct btree_iter_set r)\n {\n-\tstruct btree_iter_set *_iter1 = iter1;\n-\tstruct btree_iter_set *_iter2 = iter2;\n-\n-\tswap(*_iter1, *_iter2);\n+\treturn bkey_cmp(l.k, r.k) > 0;\n }\n \n static inline bool btree_iter_end(struct btree_iter *iter)\n {\n-\treturn !iter->heap.nr;\n+\treturn !iter->used;\n }\n \n void bch_btree_iter_push(struct btree_iter *iter, struct bkey *k,\n \t\t\t struct bkey *end)\n {\n-\tconst struct min_heap_callbacks callbacks = {\n-\t\t.less = new_btree_iter_cmp,\n-\t\t.swp = new_btree_iter_swap,\n-\t};\n-\n \tif (k != end)\n-\t\tBUG_ON(!min_heap_push(&iter->heap,\n-\t\t\t\t &((struct btree_iter_set) { k, end }),\n-\t\t\t\t &callbacks,\n-\t\t\t\t NULL));\n+\t\tBUG_ON(!heap_add(iter,\n+\t\t\t\t ((struct btree_iter_set) { k, end }),\n+\t\t\t\t btree_iter_cmp));\n }\n \n-static struct bkey *__bch_btree_iter_init(struct btree_keys *b,\n-\t\t\t\t\t  struct btree_iter *iter,\n-\t\t\t\t\t  struct bkey *search,\n-\t\t\t\t\t  struct bset_tree *start)\n+static struct bkey *__bch_btree_iter_stack_init(struct btree_keys *b,\n+\t\t\t\t\t\tstruct btree_iter_stack *iter,\n+\t\t\t\t\t\tstruct bkey *search,\n+\t\t\t\t\t\tstruct bset_tree *start)\n {\n \tstruct bkey *ret = NULL;\n \n-\titer->heap.size = ARRAY_SIZE(iter->heap.preallocated);\n-\titer->heap.nr = 0;\n+\titer->iter.size = ARRAY_SIZE(iter->stack_data);\n+\titer->iter.used = 0;\n \n #ifdef CONFIG_BCACHE_DEBUG\n-\titer->b = b;\n+\titer->iter.b = b;\n #endif\n \n \tfor (; start <= bset_tree_last(b); start++) {\n \t\tret = bch_bset_search(b, start, search);\n-\t\tbch_btree_iter_push(iter, ret, bset_bkey_last(start->data));\n+\t\tbch_btree_iter_push(&iter->iter, ret, bset_bkey_last(start->data));\n \t}\n \n \treturn ret;\n }\n \n-struct bkey *bch_btree_iter_init(struct btree_keys *b,\n-\t\t\t\t struct btree_iter *iter,\n+struct bkey *bch_btree_iter_stack_init(struct btree_keys *b,\n+\t\t\t\t struct btree_iter_stack *iter,\n \t\t\t\t struct bkey *search)\n {\n-\treturn __bch_btree_iter_init(b, iter, search, b->set);\n+\treturn __bch_btree_iter_stack_init(b, iter, search, b->set);\n }\n \n static inline struct bkey *__bch_btree_iter_next(struct btree_iter *iter,\n-\t\t\t\t\t\t new_btree_iter_cmp_fn *cmp)\n+\t\t\t\t\t\t btree_iter_cmp_fn *cmp)\n {\n \tstruct btree_iter_set b __maybe_unused;\n \tstruct bkey *ret = NULL;\n-\tconst struct min_heap_callbacks callbacks = {\n-\t\t.less = cmp,\n-\t\t.swp = new_btree_iter_swap,\n-\t};\n \n \tif (!btree_iter_end(iter)) {\n \t\tbch_btree_iter_next_check(iter);\n \n-\t\tret = iter->heap.data->k;\n-\t\titer->heap.data->k = bkey_next(iter->heap.data->k);\n+\t\tret = iter->data->k;\n+\t\titer->data->k = bkey_next(iter->data->k);\n \n-\t\tif (iter->heap.data->k > iter->heap.data->end) {\n+\t\tif (iter->data->k > iter->data->end) {\n \t\t\tWARN_ONCE(1, \"bset was corrupt!\\n\");\n-\t\t\titer->heap.data->k = iter->heap.data->end;\n+\t\t\titer->data->k = iter->data->end;\n \t\t}\n \n-\t\tif (iter->heap.data->k == iter->heap.data->end) {\n-\t\t\tif (iter->heap.nr) {\n-\t\t\t\tb = min_heap_peek(&iter->heap)[0];\n-\t\t\t\tmin_heap_pop(&iter->heap, &callbacks, NULL);\n-\t\t\t}\n-\t\t}\n+\t\tif (iter->data->k == iter->data->end)\n+\t\t\theap_pop(iter, b, cmp);\n \t\telse\n-\t\t\tmin_heap_sift_down(&iter->heap, 0, &callbacks, NULL);\n+\t\t\theap_sift(iter, 0, cmp);\n \t}\n \n \treturn ret;\n@@ -1186,7 +1157,7 @@ static inline struct bkey *__bch_btree_iter_next(struct btree_iter *iter,\n \n struct bkey *bch_btree_iter_next(struct btree_iter *iter)\n {\n-\treturn __bch_btree_iter_next(iter, new_btree_iter_cmp);\n+\treturn __bch_btree_iter_next(iter, btree_iter_cmp);\n \n }\n \n@@ -1224,18 +1195,16 @@ static void btree_mergesort(struct btree_keys *b, struct bset *out,\n \t\t\t    struct btree_iter *iter,\n \t\t\t    bool fixup, bool remove_stale)\n {\n+\tint i;\n \tstruct bkey *k, *last = NULL;\n \tBKEY_PADDED(k) tmp;\n \tbool (*bad)(struct btree_keys *, const struct bkey *) = remove_stale\n \t\t? bch_ptr_bad\n \t\t: bch_ptr_invalid;\n-\tconst struct min_heap_callbacks callbacks = {\n-\t\t.less = b->ops->sort_cmp,\n-\t\t.swp = new_btree_iter_swap,\n-\t};\n \n \t/* Heapify the iterator, using our comparison function */\n-\tmin_heapify_all(&iter->heap, &callbacks, NULL);\n+\tfor (i = iter->used / 2 - 1; i >= 0; --i)\n+\t\theap_sift(iter, i, b->ops->sort_cmp);\n \n \twhile (!btree_iter_end(iter)) {\n \t\tif (b->ops->sort_fixup && fixup)\n@@ -1324,11 +1293,10 @@ void bch_btree_sort_partial(struct btree_keys *b, unsigned int start,\n \t\t\t    struct bset_sort_state *state)\n {\n \tsize_t order = b->page_order, keys = 0;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tint oldsize = bch_count_data(b);\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\t__bch_btree_iter_init(b, &iter, NULL, &b->set[start]);\n+\t__bch_btree_iter_stack_init(b, &iter, NULL, &b->set[start]);\n \n \tif (start) {\n \t\tunsigned int i;\n@@ -1339,7 +1307,7 @@ void bch_btree_sort_partial(struct btree_keys *b, unsigned int start,\n \t\torder = get_order(__set_bytes(b->set->data, keys));\n \t}\n \n-\t__btree_sort(b, &iter, start, order, false, state);\n+\t__btree_sort(b, &iter.iter, start, order, false, state);\n \n \tEBUG_ON(oldsize >= 0 && bch_count_data(b) != oldsize);\n }\n@@ -1355,13 +1323,11 @@ void bch_btree_sort_into(struct btree_keys *b, struct btree_keys *new,\n \t\t\t struct bset_sort_state *state)\n {\n \tuint64_t start_time = local_clock();\n-\tstruct btree_iter iter;\n-\n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n+\tstruct btree_iter_stack iter;\n \n-\tbch_btree_iter_init(b, &iter, NULL);\n+\tbch_btree_iter_stack_init(b, &iter, NULL);\n \n-\tbtree_mergesort(b, new->set->data, &iter, false, true);\n+\tbtree_mergesort(b, new->set->data, &iter.iter, false, true);\n \n \tbch_time_stats_update(&state->time, start_time);\n \ndiff --git a/drivers/md/bcache/bset.h b/drivers/md/bcache/bset.h\nindex f79441acd4c1..011f6062c4c0 100644\n--- a/drivers/md/bcache/bset.h\n+++ b/drivers/md/bcache/bset.h\n@@ -187,9 +187,8 @@ struct bset_tree {\n };\n \n struct btree_keys_ops {\n-\tbool\t\t(*sort_cmp)(const void *l,\n-\t\t\t\t    const void *r,\n-\t\t\t\t\tvoid *args);\n+\tbool\t\t(*sort_cmp)(struct btree_iter_set l,\n+\t\t\t\t    struct btree_iter_set r);\n \tstruct bkey\t*(*sort_fixup)(struct btree_iter *iter,\n \t\t\t\t       struct bkey *tmp);\n \tbool\t\t(*insert_fixup)(struct btree_keys *b,\n@@ -313,17 +312,23 @@ enum {\n \tBTREE_INSERT_STATUS_FRONT_MERGE,\n };\n \n-struct btree_iter_set {\n-\tstruct bkey *k, *end;\n-};\n-\n /* Btree key iteration */\n \n struct btree_iter {\n+\tsize_t size, used;\n #ifdef CONFIG_BCACHE_DEBUG\n \tstruct btree_keys *b;\n #endif\n-\tMIN_HEAP_PREALLOCATED(struct btree_iter_set, btree_iter_heap, MAX_BSETS) heap;\n+\tstruct btree_iter_set {\n+\t\tstruct bkey *k, *end;\n+\t} data[];\n+};\n+\n+/* Fixed-size btree_iter that can be allocated on the stack */\n+\n+struct btree_iter_stack {\n+\tstruct btree_iter iter;\n+\tstruct btree_iter_set stack_data[MAX_BSETS];\n };\n \n typedef bool (*ptr_filter_fn)(struct btree_keys *b, const struct bkey *k);\n@@ -335,9 +340,9 @@ struct bkey *bch_btree_iter_next_filter(struct btree_iter *iter,\n \n void bch_btree_iter_push(struct btree_iter *iter, struct bkey *k,\n \t\t\t struct bkey *end);\n-struct bkey *bch_btree_iter_init(struct btree_keys *b,\n-\t\t\t\t struct btree_iter *iter,\n-\t\t\t\t struct bkey *search);\n+struct bkey *bch_btree_iter_stack_init(struct btree_keys *b,\n+\t\t\t\t       struct btree_iter_stack *iter,\n+\t\t\t\t       struct bkey *search);\n \n struct bkey *__bch_bset_search(struct btree_keys *b, struct bset_tree *t,\n \t\t\t       const struct bkey *search);\n@@ -352,13 +357,14 @@ static inline struct bkey *bch_bset_search(struct btree_keys *b,\n \treturn search ? __bch_bset_search(b, t, search) : t->data->start;\n }\n \n-#define for_each_key_filter(b, k, iter, filter)\t\t\t\t\\\n-\tfor (bch_btree_iter_init((b), (iter), NULL);\t\t\t\\\n-\t     ((k) = bch_btree_iter_next_filter((iter), (b), filter));)\n+#define for_each_key_filter(b, k, stack_iter, filter)                      \\\n+\tfor (bch_btree_iter_stack_init((b), (stack_iter), NULL);           \\\n+\t     ((k) = bch_btree_iter_next_filter(&((stack_iter)->iter), (b), \\\n+\t\t\t\t\t       filter));)\n \n-#define for_each_key(b, k, iter)\t\t\t\t\t\\\n-\tfor (bch_btree_iter_init((b), (iter), NULL);\t\t\t\\\n-\t     ((k) = bch_btree_iter_next(iter));)\n+#define for_each_key(b, k, stack_iter)                           \\\n+\tfor (bch_btree_iter_stack_init((b), (stack_iter), NULL); \\\n+\t     ((k) = bch_btree_iter_next(&((stack_iter)->iter)));)\n \n /* Sorting */\n \ndiff --git a/drivers/md/bcache/btree.c b/drivers/md/bcache/btree.c\nindex 1d0100677357..210b59007d98 100644\n--- a/drivers/md/bcache/btree.c\n+++ b/drivers/md/bcache/btree.c\n@@ -148,19 +148,19 @@ void bch_btree_node_read_done(struct btree *b)\n {\n \tconst char *err = \"bad btree header\";\n \tstruct bset *i = btree_bset_first(b);\n-\tstruct btree_iter iter;\n+\tstruct btree_iter *iter;\n \n \t/*\n \t * c->fill_iter can allocate an iterator with more memory space\n \t * than static MAX_BSETS.\n \t * See the comment arount cache_set->fill_iter.\n \t */\n-\titer.heap.data = mempool_alloc(&b->c->fill_iter, GFP_NOIO);\n-\titer.heap.size = b->c->cache->sb.bucket_size / b->c->cache->sb.block_size;\n-\titer.heap.nr = 0;\n+\titer = mempool_alloc(&b->c->fill_iter, GFP_NOIO);\n+\titer->size = b->c->cache->sb.bucket_size / b->c->cache->sb.block_size;\n+\titer->used = 0;\n \n #ifdef CONFIG_BCACHE_DEBUG\n-\titer.b = &b->keys;\n+\titer->b = &b->keys;\n #endif\n \n \tif (!i->seq)\n@@ -198,7 +198,7 @@ void bch_btree_node_read_done(struct btree *b)\n \t\tif (i != b->keys.set[0].data && !i->keys)\n \t\t\tgoto err;\n \n-\t\tbch_btree_iter_push(&iter, i->start, bset_bkey_last(i));\n+\t\tbch_btree_iter_push(iter, i->start, bset_bkey_last(i));\n \n \t\tb->written += set_blocks(i, block_bytes(b->c->cache));\n \t}\n@@ -210,7 +210,7 @@ void bch_btree_node_read_done(struct btree *b)\n \t\tif (i->seq == b->keys.set[0].data->seq)\n \t\t\tgoto err;\n \n-\tbch_btree_sort_and_fix_extents(&b->keys, &iter, &b->c->sort);\n+\tbch_btree_sort_and_fix_extents(&b->keys, iter, &b->c->sort);\n \n \ti = b->keys.set[0].data;\n \terr = \"short btree key\";\n@@ -222,7 +222,7 @@ void bch_btree_node_read_done(struct btree *b)\n \t\tbch_bset_init_next(&b->keys, write_block(b),\n \t\t\t\t   bset_magic(&b->c->cache->sb));\n out:\n-\tmempool_free(iter.heap.data, &b->c->fill_iter);\n+\tmempool_free(iter, &b->c->fill_iter);\n \treturn;\n err:\n \tset_btree_node_io_error(b);\n@@ -1306,11 +1306,9 @@ static bool btree_gc_mark_node(struct btree *b, struct gc_stat *gc)\n \tuint8_t stale = 0;\n \tunsigned int keys = 0, good_keys = 0;\n \tstruct bkey *k;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct bset_tree *t;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \tgc->nodes++;\n \n \tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_invalid) {\n@@ -1569,11 +1567,9 @@ static int btree_gc_rewrite_node(struct btree *b, struct btree_op *op,\n static unsigned int btree_gc_count_keys(struct btree *b)\n {\n \tstruct bkey *k;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tunsigned int ret = 0;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_bad)\n \t\tret += bkey_u64s(k);\n \n@@ -1612,18 +1608,18 @@ static int btree_gc_recurse(struct btree *b, struct btree_op *op,\n \tint ret = 0;\n \tbool should_rewrite;\n \tstruct bkey *k;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct gc_merge_info r[GC_MERGE_NODES];\n \tstruct gc_merge_info *i, *last = r + ARRAY_SIZE(r) - 1;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\tbch_btree_iter_init(&b->keys, &iter, &b->c->gc_done);\n+\tbch_btree_iter_stack_init(&b->keys, &iter, &b->c->gc_done);\n \n \tfor (i = r; i < r + ARRAY_SIZE(r); i++)\n \t\ti->b = ERR_PTR(-EINTR);\n \n \twhile (1) {\n-\t\tk = bch_btree_iter_next_filter(&iter, &b->keys, bch_ptr_bad);\n+\t\tk = bch_btree_iter_next_filter(&iter.iter, &b->keys,\n+\t\t\t\t\t       bch_ptr_bad);\n \t\tif (k) {\n \t\t\tr->b = bch_btree_node_get(b->c, op, k, b->level - 1,\n \t\t\t\t\t\t  true, b);\n@@ -1918,9 +1914,7 @@ static int bch_btree_check_recurse(struct btree *b, struct btree_op *op)\n {\n \tint ret = 0;\n \tstruct bkey *k, *p = NULL;\n-\tstruct btree_iter iter;\n-\n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n+\tstruct btree_iter_stack iter;\n \n \tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_invalid)\n \t\tbch_initial_mark_key(b->c, b->level, k);\n@@ -1928,10 +1922,10 @@ static int bch_btree_check_recurse(struct btree *b, struct btree_op *op)\n \tbch_initial_mark_key(b->c, b->level + 1, &b->key);\n \n \tif (b->level) {\n-\t\tbch_btree_iter_init(&b->keys, &iter, NULL);\n+\t\tbch_btree_iter_stack_init(&b->keys, &iter, NULL);\n \n \t\tdo {\n-\t\t\tk = bch_btree_iter_next_filter(&iter, &b->keys,\n+\t\t\tk = bch_btree_iter_next_filter(&iter.iter, &b->keys,\n \t\t\t\t\t\t       bch_ptr_bad);\n \t\t\tif (k) {\n \t\t\t\tbtree_node_prefetch(b, k);\n@@ -1959,7 +1953,7 @@ static int bch_btree_check_thread(void *arg)\n \tstruct btree_check_info *info = arg;\n \tstruct btree_check_state *check_state = info->state;\n \tstruct cache_set *c = check_state->c;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct bkey *k, *p;\n \tint cur_idx, prev_idx, skip_nr;\n \n@@ -1967,11 +1961,9 @@ static int bch_btree_check_thread(void *arg)\n \tcur_idx = prev_idx = 0;\n \tret = 0;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \t/* root node keys are checked before thread created */\n-\tbch_btree_iter_init(&c->root->keys, &iter, NULL);\n-\tk = bch_btree_iter_next_filter(&iter, &c->root->keys, bch_ptr_bad);\n+\tbch_btree_iter_stack_init(&c->root->keys, &iter, NULL);\n+\tk = bch_btree_iter_next_filter(&iter.iter, &c->root->keys, bch_ptr_bad);\n \tBUG_ON(!k);\n \n \tp = k;\n@@ -1989,7 +1981,7 @@ static int bch_btree_check_thread(void *arg)\n \t\tskip_nr = cur_idx - prev_idx;\n \n \t\twhile (skip_nr) {\n-\t\t\tk = bch_btree_iter_next_filter(&iter,\n+\t\t\tk = bch_btree_iter_next_filter(&iter.iter,\n \t\t\t\t\t\t       &c->root->keys,\n \t\t\t\t\t\t       bch_ptr_bad);\n \t\t\tif (k)\n@@ -2062,11 +2054,9 @@ int bch_btree_check(struct cache_set *c)\n \tint ret = 0;\n \tint i;\n \tstruct bkey *k = NULL;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct btree_check_state check_state;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n \t/* check and mark root node keys */\n \tfor_each_key_filter(&c->root->keys, k, &iter, bch_ptr_invalid)\n \t\tbch_initial_mark_key(c, c->root->level, k);\n@@ -2560,12 +2550,11 @@ static int bch_btree_map_nodes_recurse(struct btree *b, struct btree_op *op,\n \n \tif (b->level) {\n \t\tstruct bkey *k;\n-\t\tstruct btree_iter iter;\n+\t\tstruct btree_iter_stack iter;\n \n-\t\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\t\tbch_btree_iter_init(&b->keys, &iter, from);\n+\t\tbch_btree_iter_stack_init(&b->keys, &iter, from);\n \n-\t\twhile ((k = bch_btree_iter_next_filter(&iter, &b->keys,\n+\t\twhile ((k = bch_btree_iter_next_filter(&iter.iter, &b->keys,\n \t\t\t\t\t\t       bch_ptr_bad))) {\n \t\t\tret = bcache_btree(map_nodes_recurse, k, b,\n \t\t\t\t    op, from, fn, flags);\n@@ -2594,12 +2583,12 @@ int bch_btree_map_keys_recurse(struct btree *b, struct btree_op *op,\n {\n \tint ret = MAP_CONTINUE;\n \tstruct bkey *k;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\tbch_btree_iter_init(&b->keys, &iter, from);\n+\tbch_btree_iter_stack_init(&b->keys, &iter, from);\n \n-\twhile ((k = bch_btree_iter_next_filter(&iter, &b->keys, bch_ptr_bad))) {\n+\twhile ((k = bch_btree_iter_next_filter(&iter.iter, &b->keys,\n+\t\t\t\t\t       bch_ptr_bad))) {\n \t\tret = !b->level\n \t\t\t? fn(op, b, k)\n \t\t\t: bcache_btree(map_keys_recurse, k,\ndiff --git a/drivers/md/bcache/extents.c b/drivers/md/bcache/extents.c\nindex a7221e5dbe81..d626ffcbecb9 100644\n--- a/drivers/md/bcache/extents.c\n+++ b/drivers/md/bcache/extents.c\n@@ -33,16 +33,15 @@ static void sort_key_next(struct btree_iter *iter,\n \ti->k = bkey_next(i->k);\n \n \tif (i->k == i->end)\n-\t\t*i = iter->heap.data[--iter->heap.nr];\n+\t\t*i = iter->data[--iter->used];\n }\n \n-static bool new_bch_key_sort_cmp(const void *l, const void *r, void *args)\n+static bool bch_key_sort_cmp(struct btree_iter_set l,\n+\t\t\t     struct btree_iter_set r)\n {\n-\tstruct btree_iter_set *_l = (struct btree_iter_set *)l;\n-\tstruct btree_iter_set *_r = (struct btree_iter_set *)r;\n-\tint64_t c = bkey_cmp(_l->k, _r->k);\n+\tint64_t c = bkey_cmp(l.k, r.k);\n \n-\treturn !(c ? c > 0 : _l->k < _r->k);\n+\treturn c ? c > 0 : l.k < r.k;\n }\n \n static bool __ptr_invalid(struct cache_set *c, const struct bkey *k)\n@@ -239,7 +238,7 @@ static bool bch_btree_ptr_insert_fixup(struct btree_keys *bk,\n }\n \n const struct btree_keys_ops bch_btree_keys_ops = {\n-\t.sort_cmp\t= new_bch_key_sort_cmp,\n+\t.sort_cmp\t= bch_key_sort_cmp,\n \t.insert_fixup\t= bch_btree_ptr_insert_fixup,\n \t.key_invalid\t= bch_btree_ptr_invalid,\n \t.key_bad\t= bch_btree_ptr_bad,\n@@ -256,36 +255,22 @@ const struct btree_keys_ops bch_btree_keys_ops = {\n  * Necessary for btree_sort_fixup() - if there are multiple keys that compare\n  * equal in different sets, we have to process them newest to oldest.\n  */\n-\n-static bool new_bch_extent_sort_cmp(const void *l, const void *r, void __always_unused *args)\n-{\n-\tstruct btree_iter_set *_l = (struct btree_iter_set *)l;\n-\tstruct btree_iter_set *_r = (struct btree_iter_set *)r;\n-\tint64_t c = bkey_cmp(&START_KEY(_l->k), &START_KEY(_r->k));\n-\n-\treturn !(c ? c > 0 : _l->k < _r->k);\n-}\n-\n-static inline void new_btree_iter_swap(void *iter1, void *iter2, void __always_unused *args)\n+static bool bch_extent_sort_cmp(struct btree_iter_set l,\n+\t\t\t\tstruct btree_iter_set r)\n {\n-\tstruct btree_iter_set *_iter1 = iter1;\n-\tstruct btree_iter_set *_iter2 = iter2;\n+\tint64_t c = bkey_cmp(&START_KEY(l.k), &START_KEY(r.k));\n \n-\tswap(*_iter1, *_iter2);\n+\treturn c ? c > 0 : l.k < r.k;\n }\n \n static struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\n \t\t\t\t\t  struct bkey *tmp)\n {\n-\tconst struct min_heap_callbacks callbacks = {\n-\t\t.less = new_bch_extent_sort_cmp,\n-\t\t.swp = new_btree_iter_swap,\n-\t};\n-\twhile (iter->heap.nr > 1) {\n-\t\tstruct btree_iter_set *top = iter->heap.data, *i = top + 1;\n-\n-\t\tif (iter->heap.nr > 2 &&\n-\t\t    !new_bch_extent_sort_cmp(&i[0], &i[1], NULL))\n+\twhile (iter->used > 1) {\n+\t\tstruct btree_iter_set *top = iter->data, *i = top + 1;\n+\n+\t\tif (iter->used > 2 &&\n+\t\t    bch_extent_sort_cmp(i[0], i[1]))\n \t\t\ti++;\n \n \t\tif (bkey_cmp(top->k, &START_KEY(i->k)) <= 0)\n@@ -293,7 +278,7 @@ static struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\n \n \t\tif (!KEY_SIZE(i->k)) {\n \t\t\tsort_key_next(iter, i);\n-\t\t\tmin_heap_sift_down(&iter->heap, i - top, &callbacks, NULL);\n+\t\t\theap_sift(iter, i - top, bch_extent_sort_cmp);\n \t\t\tcontinue;\n \t\t}\n \n@@ -303,7 +288,7 @@ static struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\n \t\t\telse\n \t\t\t\tbch_cut_front(top->k, i->k);\n \n-\t\t\tmin_heap_sift_down(&iter->heap, i - top, &callbacks, NULL);\n+\t\t\theap_sift(iter, i - top, bch_extent_sort_cmp);\n \t\t} else {\n \t\t\t/* can't happen because of comparison func */\n \t\t\tBUG_ON(!bkey_cmp(&START_KEY(top->k), &START_KEY(i->k)));\n@@ -313,7 +298,7 @@ static struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\n \n \t\t\t\tbch_cut_back(&START_KEY(i->k), tmp);\n \t\t\t\tbch_cut_front(i->k, top->k);\n-\t\t\t\tmin_heap_sift_down(&iter->heap, 0, &callbacks, NULL);\n+\t\t\t\theap_sift(iter, 0, bch_extent_sort_cmp);\n \n \t\t\t\treturn tmp;\n \t\t\t} else {\n@@ -633,7 +618,7 @@ static bool bch_extent_merge(struct btree_keys *bk,\n }\n \n const struct btree_keys_ops bch_extent_keys_ops = {\n-\t.sort_cmp\t= new_bch_extent_sort_cmp,\n+\t.sort_cmp\t= bch_extent_sort_cmp,\n \t.sort_fixup\t= bch_extent_sort_fixup,\n \t.insert_fixup\t= bch_extent_insert_fixup,\n \t.key_invalid\t= bch_extent_invalid,\ndiff --git a/drivers/md/bcache/movinggc.c b/drivers/md/bcache/movinggc.c\nindex d6c73dd8eb2b..26a6a535ec32 100644\n--- a/drivers/md/bcache/movinggc.c\n+++ b/drivers/md/bcache/movinggc.c\n@@ -182,27 +182,16 @@ err:\t\tif (!IS_ERR_OR_NULL(w->private))\n \tclosure_sync(&cl);\n }\n \n-static bool new_bucket_cmp(const void *l, const void *r, void __always_unused *args)\n+static bool bucket_cmp(struct bucket *l, struct bucket *r)\n {\n-\tstruct bucket **_l = (struct bucket **)l;\n-\tstruct bucket **_r = (struct bucket **)r;\n-\n-\treturn GC_SECTORS_USED(*_l) >= GC_SECTORS_USED(*_r);\n-}\n-\n-static void new_bucket_swap(void *l, void *r, void __always_unused *args)\n-{\n-\tstruct bucket **_l = l;\n-\tstruct bucket **_r = r;\n-\n-\tswap(*_l, *_r);\n+\treturn GC_SECTORS_USED(l) < GC_SECTORS_USED(r);\n }\n \n static unsigned int bucket_heap_top(struct cache *ca)\n {\n \tstruct bucket *b;\n \n-\treturn (b = min_heap_peek(&ca->heap)[0]) ? GC_SECTORS_USED(b) : 0;\n+\treturn (b = heap_peek(&ca->heap)) ? GC_SECTORS_USED(b) : 0;\n }\n \n void bch_moving_gc(struct cache_set *c)\n@@ -210,10 +199,6 @@ void bch_moving_gc(struct cache_set *c)\n \tstruct cache *ca = c->cache;\n \tstruct bucket *b;\n \tunsigned long sectors_to_move, reserve_sectors;\n-\tconst struct min_heap_callbacks callbacks = {\n-\t\t.less = new_bucket_cmp,\n-\t\t.swp = new_bucket_swap,\n-\t};\n \n \tif (!c->copy_gc_enabled)\n \t\treturn;\n@@ -224,7 +209,7 @@ void bch_moving_gc(struct cache_set *c)\n \treserve_sectors = ca->sb.bucket_size *\n \t\t\t     fifo_used(&ca->free[RESERVE_MOVINGGC]);\n \n-\tca->heap.nr = 0;\n+\tca->heap.used = 0;\n \n \tfor_each_bucket(b, ca) {\n \t\tif (GC_MARK(b) == GC_MARK_METADATA ||\n@@ -233,31 +218,25 @@ void bch_moving_gc(struct cache_set *c)\n \t\t    atomic_read(&b->pin))\n \t\t\tcontinue;\n \n-\t\tif (!min_heap_full(&ca->heap)) {\n+\t\tif (!heap_full(&ca->heap)) {\n \t\t\tsectors_to_move += GC_SECTORS_USED(b);\n-\t\t\tmin_heap_push(&ca->heap, &b, &callbacks, NULL);\n-\t\t} else if (!new_bucket_cmp(&b, min_heap_peek(&ca->heap), ca)) {\n+\t\t\theap_add(&ca->heap, b, bucket_cmp);\n+\t\t} else if (bucket_cmp(b, heap_peek(&ca->heap))) {\n \t\t\tsectors_to_move -= bucket_heap_top(ca);\n \t\t\tsectors_to_move += GC_SECTORS_USED(b);\n \n \t\t\tca->heap.data[0] = b;\n-\t\t\tmin_heap_sift_down(&ca->heap, 0, &callbacks, NULL);\n+\t\t\theap_sift(&ca->heap, 0, bucket_cmp);\n \t\t}\n \t}\n \n \twhile (sectors_to_move > reserve_sectors) {\n-\t\tif (ca->heap.nr) {\n-\t\t\tb = min_heap_peek(&ca->heap)[0];\n-\t\t\tmin_heap_pop(&ca->heap, &callbacks, NULL);\n-\t\t}\n+\t\theap_pop(&ca->heap, b, bucket_cmp);\n \t\tsectors_to_move -= GC_SECTORS_USED(b);\n \t}\n \n-\twhile (ca->heap.nr) {\n-\t\tb = min_heap_peek(&ca->heap)[0];\n-\t\tmin_heap_pop(&ca->heap, &callbacks, NULL);\n+\twhile (heap_pop(&ca->heap, b, bucket_cmp))\n \t\tSET_GC_MOVE(b, 1);\n-\t}\n \n \tmutex_unlock(&c->bucket_lock);\n \ndiff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c\nindex 1efb768b2890..2ea490b9d370 100644\n--- a/drivers/md/bcache/super.c\n+++ b/drivers/md/bcache/super.c\n@@ -1912,7 +1912,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)\n \tINIT_LIST_HEAD(&c->btree_cache_freed);\n \tINIT_LIST_HEAD(&c->data_buckets);\n \n-\titer_size = ((meta_bucket_pages(sb) * PAGE_SECTORS) / sb->block_size) *\n+\titer_size = sizeof(struct btree_iter) +\n+\t\t    ((meta_bucket_pages(sb) * PAGE_SECTORS) / sb->block_size) *\n \t\t\t    sizeof(struct btree_iter_set);\n \n \tc->devices = kcalloc(c->nr_uuids, sizeof(void *), GFP_KERNEL);\ndiff --git a/drivers/md/bcache/sysfs.c b/drivers/md/bcache/sysfs.c\nindex e8f696cb58c0..826b14cae4e5 100644\n--- a/drivers/md/bcache/sysfs.c\n+++ b/drivers/md/bcache/sysfs.c\n@@ -660,9 +660,7 @@ static unsigned int bch_root_usage(struct cache_set *c)\n \tunsigned int bytes = 0;\n \tstruct bkey *k;\n \tstruct btree *b;\n-\tstruct btree_iter iter;\n-\n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n+\tstruct btree_iter_stack iter;\n \n \tgoto lock_root;\n \ndiff --git a/drivers/md/bcache/util.h b/drivers/md/bcache/util.h\nindex 539454d8e2d0..f61ab1bada6c 100644\n--- a/drivers/md/bcache/util.h\n+++ b/drivers/md/bcache/util.h\n@@ -9,7 +9,6 @@\n #include <linux/kernel.h>\n #include <linux/sched/clock.h>\n #include <linux/llist.h>\n-#include <linux/min_heap.h>\n #include <linux/ratelimit.h>\n #include <linux/vmalloc.h>\n #include <linux/workqueue.h>\n@@ -31,10 +30,16 @@ struct closure;\n \n #endif\n \n+#define DECLARE_HEAP(type, name)\t\t\t\t\t\\\n+\tstruct {\t\t\t\t\t\t\t\\\n+\t\tsize_t size, used;\t\t\t\t\t\\\n+\t\ttype *data;\t\t\t\t\t\t\\\n+\t} name\n+\n #define init_heap(heap, _size, gfp)\t\t\t\t\t\\\n ({\t\t\t\t\t\t\t\t\t\\\n \tsize_t _bytes;\t\t\t\t\t\t\t\\\n-\t(heap)->nr = 0;\t\t\t\t\t\t\\\n+\t(heap)->used = 0;\t\t\t\t\t\t\\\n \t(heap)->size = (_size);\t\t\t\t\t\t\\\n \t_bytes = (heap)->size * sizeof(*(heap)->data);\t\t\t\\\n \t(heap)->data = kvmalloc(_bytes, (gfp) & GFP_KERNEL);\t\t\\\n@@ -47,6 +52,64 @@ do {\t\t\t\t\t\t\t\t\t\\\n \t(heap)->data = NULL;\t\t\t\t\t\t\\\n } while (0)\n \n+#define heap_swap(h, i, j)\tswap((h)->data[i], (h)->data[j])\n+\n+#define heap_sift(h, i, cmp)\t\t\t\t\t\t\\\n+do {\t\t\t\t\t\t\t\t\t\\\n+\tsize_t _r, _j = i;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tfor (; _j * 2 + 1 < (h)->used; _j = _r) {\t\t\t\\\n+\t\t_r = _j * 2 + 1;\t\t\t\t\t\\\n+\t\tif (_r + 1 < (h)->used &&\t\t\t\t\\\n+\t\t    cmp((h)->data[_r], (h)->data[_r + 1]))\t\t\\\n+\t\t\t_r++;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\tif (cmp((h)->data[_r], (h)->data[_j]))\t\t\t\\\n+\t\t\tbreak;\t\t\t\t\t\t\\\n+\t\theap_swap(h, _r, _j);\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+} while (0)\n+\n+#define heap_sift_down(h, i, cmp)\t\t\t\t\t\\\n+do {\t\t\t\t\t\t\t\t\t\\\n+\twhile (i) {\t\t\t\t\t\t\t\\\n+\t\tsize_t p = (i - 1) / 2;\t\t\t\t\t\\\n+\t\tif (cmp((h)->data[i], (h)->data[p]))\t\t\t\\\n+\t\t\tbreak;\t\t\t\t\t\t\\\n+\t\theap_swap(h, i, p);\t\t\t\t\t\\\n+\t\ti = p;\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+} while (0)\n+\n+#define heap_add(h, d, cmp)\t\t\t\t\t\t\\\n+({\t\t\t\t\t\t\t\t\t\\\n+\tbool _r = !heap_full(h);\t\t\t\t\t\\\n+\tif (_r) {\t\t\t\t\t\t\t\\\n+\t\tsize_t _i = (h)->used++;\t\t\t\t\\\n+\t\t(h)->data[_i] = d;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\theap_sift_down(h, _i, cmp);\t\t\t\t\\\n+\t\theap_sift(h, _i, cmp);\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\t_r;\t\t\t\t\t\t\t\t\\\n+})\n+\n+#define heap_pop(h, d, cmp)\t\t\t\t\t\t\\\n+({\t\t\t\t\t\t\t\t\t\\\n+\tbool _r = (h)->used;\t\t\t\t\t\t\\\n+\tif (_r) {\t\t\t\t\t\t\t\\\n+\t\t(d) = (h)->data[0];\t\t\t\t\t\\\n+\t\t(h)->used--;\t\t\t\t\t\t\\\n+\t\theap_swap(h, 0, (h)->used);\t\t\t\t\\\n+\t\theap_sift(h, 0, cmp);\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\t_r;\t\t\t\t\t\t\t\t\\\n+})\n+\n+#define heap_peek(h)\t((h)->used ? (h)->data[0] : NULL)\n+\n+#define heap_full(h)\t((h)->used == (h)->size)\n+\n #define DECLARE_FIFO(type, name)\t\t\t\t\t\\\n \tstruct {\t\t\t\t\t\t\t\\\n \t\tsize_t front, back, size, mask;\t\t\t\t\\\ndiff --git a/drivers/md/bcache/writeback.c b/drivers/md/bcache/writeback.c\nindex 453efbbdc8ee..302e75f1fc4b 100644\n--- a/drivers/md/bcache/writeback.c\n+++ b/drivers/md/bcache/writeback.c\n@@ -908,16 +908,15 @@ static int bch_dirty_init_thread(void *arg)\n \tstruct dirty_init_thrd_info *info = arg;\n \tstruct bch_dirty_init_state *state = info->state;\n \tstruct cache_set *c = state->c;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct bkey *k, *p;\n \tint cur_idx, prev_idx, skip_nr;\n \n \tk = p = NULL;\n \tprev_idx = 0;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\tbch_btree_iter_init(&c->root->keys, &iter, NULL);\n-\tk = bch_btree_iter_next_filter(&iter, &c->root->keys, bch_ptr_bad);\n+\tbch_btree_iter_stack_init(&c->root->keys, &iter, NULL);\n+\tk = bch_btree_iter_next_filter(&iter.iter, &c->root->keys, bch_ptr_bad);\n \tBUG_ON(!k);\n \n \tp = k;\n@@ -931,7 +930,7 @@ static int bch_dirty_init_thread(void *arg)\n \t\tskip_nr = cur_idx - prev_idx;\n \n \t\twhile (skip_nr) {\n-\t\t\tk = bch_btree_iter_next_filter(&iter,\n+\t\t\tk = bch_btree_iter_next_filter(&iter.iter,\n \t\t\t\t\t\t       &c->root->keys,\n \t\t\t\t\t\t       bch_ptr_bad);\n \t\t\tif (k)\n@@ -980,13 +979,11 @@ void bch_sectors_dirty_init(struct bcache_device *d)\n \tint i;\n \tstruct btree *b = NULL;\n \tstruct bkey *k = NULL;\n-\tstruct btree_iter iter;\n+\tstruct btree_iter_stack iter;\n \tstruct sectors_dirty_init op;\n \tstruct cache_set *c = d->c;\n \tstruct bch_dirty_init_state state;\n \n-\tmin_heap_init(&iter.heap, NULL, MAX_BSETS);\n-\n retry_lock:\n \tb = c->root;\n \trw_lock(0, b, b->level);",
    "stats": {
      "insertions": 217,
      "deletions": 263,
      "files": 11
    }
  },
  {
    "sha": "517f496e1e61bd169d585dab4dd77e7147506322",
    "message": "mm/gup: revert \"mm: gup: fix infinite loop within __get_longterm_locked\"\n\nAfter commit 1aaf8c122918 (\"mm: gup: fix infinite loop within\n__get_longterm_locked\") we are able to longterm pin folios that are not\nsupposed to get longterm pinned, simply because they temporarily have the\nLRU flag cleared (esp.  temporarily isolated).\n\nFor example, two __get_longterm_locked() callers can race, or\n__get_longterm_locked() can race with anything else that temporarily\nisolates folios.\n\nThe introducing commit mentions the use case of a driver that uses\nvm_ops->fault to insert pages allocated through cma_alloc() into the page\ntables, assuming they can later get longterm pinned.  These pages/ folios\nwould never have the LRU flag set and consequently cannot get isolated. \nThere is no known in-tree user making use of that so far, fortunately.\n\nTo handle that in the future -- and avoid retrying forever to\nisolate/migrate them -- we will need a different mechanism for the CMA\narea *owner* to indicate that it actually already allocated the page and\nis fine with longterm pinning it.  The LRU flag is not suitable for that.\n\nProbably we can lookup the relevant CMA area and query the bitmap; we only\nhave have to care about some races, probably.  If already allocated, we\ncould just allow longterm pinning)\n\nAnyhow, let's fix the \"must not be longterm pinned\" problem first by\nreverting the original commit.\n\nLink: https://lkml.kernel.org/r/20250611131314.594529-1-david@redhat.com\nFixes: 1aaf8c122918 (\"mm: gup: fix infinite loop within __get_longterm_locked\")\nSigned-off-by: David Hildenbrand <david@redhat.com>\nCloses: https://lore.kernel.org/all/20250522092755.GA3277597@tiffany/\nReported-by: Hyesoo Yu <hyesoo.yu@samsung.com>\nReviewed-by: John Hubbard <jhubbard@nvidia.com>\nCc: Jason Gunthorpe <jgg@ziepe.ca>\nCc: Peter Xu <peterx@redhat.com>\nCc: Zhaoyang Huang <zhaoyang.huang@unisoc.com>\nCc: Aijun Sun <aijun.sun@unisoc.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "David Hildenbrand",
    "date": "2025-06-19T20:48:01-07:00",
    "files_changed": [
      "mm/gup.c"
    ],
    "diff": "diff --git a/mm/gup.c b/mm/gup.c\nindex e065a49842a8..3c39cbbeebef 100644\n--- a/mm/gup.c\n+++ b/mm/gup.c\n@@ -2303,13 +2303,13 @@ static void pofs_unpin(struct pages_or_folios *pofs)\n /*\n  * Returns the number of collected folios. Return value is always >= 0.\n  */\n-static void collect_longterm_unpinnable_folios(\n+static unsigned long collect_longterm_unpinnable_folios(\n \t\tstruct list_head *movable_folio_list,\n \t\tstruct pages_or_folios *pofs)\n {\n+\tunsigned long i, collected = 0;\n \tstruct folio *prev_folio = NULL;\n \tbool drain_allow = true;\n-\tunsigned long i;\n \n \tfor (i = 0; i < pofs->nr_entries; i++) {\n \t\tstruct folio *folio = pofs_get_folio(pofs, i);\n@@ -2321,6 +2321,8 @@ static void collect_longterm_unpinnable_folios(\n \t\tif (folio_is_longterm_pinnable(folio))\n \t\t\tcontinue;\n \n+\t\tcollected++;\n+\n \t\tif (folio_is_device_coherent(folio))\n \t\t\tcontinue;\n \n@@ -2342,6 +2344,8 @@ static void collect_longterm_unpinnable_folios(\n \t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n \t\t\t\t    folio_nr_pages(folio));\n \t}\n+\n+\treturn collected;\n }\n \n /*\n@@ -2418,9 +2422,11 @@ static long\n check_and_migrate_movable_pages_or_folios(struct pages_or_folios *pofs)\n {\n \tLIST_HEAD(movable_folio_list);\n+\tunsigned long collected;\n \n-\tcollect_longterm_unpinnable_folios(&movable_folio_list, pofs);\n-\tif (list_empty(&movable_folio_list))\n+\tcollected = collect_longterm_unpinnable_folios(&movable_folio_list,\n+\t\t\t\t\t\t       pofs);\n+\tif (!collected)\n \t\treturn 0;\n \n \treturn migrate_longterm_unpinnable_folios(&movable_folio_list, pofs);",
    "stats": {
      "insertions": 10,
      "deletions": 4,
      "files": 1
    }
  },
  {
    "sha": "85d6fbc47c3087c5d048e6734926b0c36af34fe9",
    "message": "scsi: fnic: Fix missing DMA mapping error in fnic_send_frame()\n\ndma_map_XXX() can fail and should be tested for errors with\ndma_mapping_error().\n\nFixes: a63e78eb2b0f (\"scsi: fnic: Add support for fabric based solicited requests and responses\")\nSigned-off-by: Thomas Fourier <fourier.thomas@gmail.com>\nLink: https://lore.kernel.org/r/20250618065715.14740-2-fourier.thomas@gmail.com\nReviewed-by: Karan Tilak Kumar <kartilak@cisco.com>\nReviewed-by: John Menghini <jmeneghi@redhat.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
    "author": "Thomas Fourier",
    "date": "2025-06-19T23:07:57-04:00",
    "files_changed": [
      "drivers/scsi/fnic/fnic_fcs.c"
    ],
    "diff": "diff --git a/drivers/scsi/fnic/fnic_fcs.c b/drivers/scsi/fnic/fnic_fcs.c\nindex 1e8cd64f9a5c..103ab6f1f7cd 100644\n--- a/drivers/scsi/fnic/fnic_fcs.c\n+++ b/drivers/scsi/fnic/fnic_fcs.c\n@@ -636,6 +636,8 @@ static int fnic_send_frame(struct fnic *fnic, void *frame, int frame_len)\n \tunsigned long flags;\n \n \tpa = dma_map_single(&fnic->pdev->dev, frame, frame_len, DMA_TO_DEVICE);\n+\tif (dma_mapping_error(&fnic->pdev->dev, pa))\n+\t\treturn -ENOMEM;\n \n \tif ((fnic_fc_trace_set_data(fnic->fnic_num,\n \t\t\t\tFNIC_FC_SEND | 0x80, (char *) frame,",
    "stats": {
      "insertions": 2,
      "deletions": 0,
      "files": 1
    }
  },
  {
    "sha": "74f46a0524f8d2f01dc7ca95bb5fc463a8603e72",
    "message": "scsi: fnic: Turn off FDMI ACTIVE flags on link down\n\nWhen the link goes down and comes up, FDMI requests are not sent out\nanymore.\n\nFix bug by turning off FNIC_FDMI_ACTIVE when the link goes down.\n\nFixes: 09c1e6ab4ab2 (\"scsi: fnic: Add and integrate support for FDMI\")\nReviewed-by: Sesidhar Baddela <sebaddel@cisco.com>\nReviewed-by: Arulprabhu Ponnusamy <arulponn@cisco.com>\nReviewed-by: Gian Carlo Boffa <gcboffa@cisco.com>\nReviewed-by: Arun Easi <aeasi@cisco.com>\nTested-by: Karan Tilak Kumar <kartilak@cisco.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Karan Tilak Kumar <kartilak@cisco.com>\nLink: https://lore.kernel.org/r/20250618003431.6314-2-kartilak@cisco.com\nReviewed-by: John Meneghini <jmeneghi@redhat.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
    "author": "Karan Tilak Kumar",
    "date": "2025-06-19T23:06:27-04:00",
    "files_changed": [
      "drivers/scsi/fnic/fdls_disc.c",
      "drivers/scsi/fnic/fnic.h"
    ],
    "diff": "diff --git a/drivers/scsi/fnic/fdls_disc.c b/drivers/scsi/fnic/fdls_disc.c\nindex 36b498ad55b4..fa9cf0b37d72 100644\n--- a/drivers/scsi/fnic/fdls_disc.c\n+++ b/drivers/scsi/fnic/fdls_disc.c\n@@ -5029,9 +5029,12 @@ void fnic_fdls_link_down(struct fnic_iport_s *iport)\n \t\tfdls_delete_tport(iport, tport);\n \t}\n \n-\tif ((fnic_fdmi_support == 1) && (iport->fabric.fdmi_pending > 0)) {\n-\t\ttimer_delete_sync(&iport->fabric.fdmi_timer);\n-\t\tiport->fabric.fdmi_pending = 0;\n+\tif (fnic_fdmi_support == 1) {\n+\t\tif (iport->fabric.fdmi_pending > 0) {\n+\t\t\ttimer_delete_sync(&iport->fabric.fdmi_timer);\n+\t\t\tiport->fabric.fdmi_pending = 0;\n+\t\t}\n+\t\tiport->flags &= ~FNIC_FDMI_ACTIVE;\n \t}\n \n \tFNIC_FCS_DBG(KERN_INFO, fnic->host, fnic->fnic_num,\ndiff --git a/drivers/scsi/fnic/fnic.h b/drivers/scsi/fnic/fnic.h\nindex 86e293ce530d..c2fdc6553e62 100644\n--- a/drivers/scsi/fnic/fnic.h\n+++ b/drivers/scsi/fnic/fnic.h\n@@ -30,7 +30,7 @@\n \n #define DRV_NAME\t\t\"fnic\"\n #define DRV_DESCRIPTION\t\t\"Cisco FCoE HBA Driver\"\n-#define DRV_VERSION\t\t\"1.8.0.1\"\n+#define DRV_VERSION\t\t\"1.8.0.2\"\n #define PFX\t\t\tDRV_NAME \": \"\n #define DFX                     DRV_NAME \"%d: \"\n ",
    "stats": {
      "insertions": 7,
      "deletions": 4,
      "files": 2
    }
  },
  {
    "sha": "a35b29bdedb4d2ae3160d4d6684a6f1ecd9ca7c2",
    "message": "scsi: fnic: Fix crash in fnic_wq_cmpl_handler when FDMI times out\n\nWhen both the RHBA and RPA FDMI requests time out, fnic reuses a frame to\nsend ABTS for each of them. On send completion, this causes an attempt to\nfree the same frame twice that leads to a crash.\n\nFix crash by allocating separate frames for RHBA and RPA, and modify ABTS\nlogic accordingly.\n\nTested by checking MDS for FDMI information.\n\nTested by using instrumented driver to:\n\n - Drop PLOGI response\n - Drop RHBA response\n - Drop RPA response\n - Drop RHBA and RPA response\n - Drop PLOGI response + ABTS response\n - Drop RHBA response + ABTS response\n - Drop RPA response + ABTS response\n - Drop RHBA and RPA response + ABTS response for both of them\n\nFixes: 09c1e6ab4ab2 (\"scsi: fnic: Add and integrate support for FDMI\")\nReviewed-by: Sesidhar Baddela <sebaddel@cisco.com>\nReviewed-by: Arulprabhu Ponnusamy <arulponn@cisco.com>\nReviewed-by: Gian Carlo Boffa <gcboffa@cisco.com>\nTested-by: Arun Easi <aeasi@cisco.com>\nCo-developed-by: Arun Easi <aeasi@cisco.com>\nSigned-off-by: Arun Easi <aeasi@cisco.com>\nTested-by: Karan Tilak Kumar <kartilak@cisco.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Karan Tilak Kumar <kartilak@cisco.com>\nLink: https://lore.kernel.org/r/20250618003431.6314-1-kartilak@cisco.com\nReviewed-by: John Meneghini <jmeneghi@redhat.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
    "author": "Karan Tilak Kumar",
    "date": "2025-06-19T23:06:27-04:00",
    "files_changed": [
      "drivers/scsi/fnic/fdls_disc.c",
      "drivers/scsi/fnic/fnic.h",
      "drivers/scsi/fnic/fnic_fdls.h"
    ],
    "diff": "diff --git a/drivers/scsi/fnic/fdls_disc.c b/drivers/scsi/fnic/fdls_disc.c\nindex f8ab69c51dab..36b498ad55b4 100644\n--- a/drivers/scsi/fnic/fdls_disc.c\n+++ b/drivers/scsi/fnic/fdls_disc.c\n@@ -763,47 +763,69 @@ static void fdls_send_fabric_abts(struct fnic_iport_s *iport)\n \tiport->fabric.timer_pending = 1;\n }\n \n-static void fdls_send_fdmi_abts(struct fnic_iport_s *iport)\n+static uint8_t *fdls_alloc_init_fdmi_abts_frame(struct fnic_iport_s *iport,\n+\t\tuint16_t oxid)\n {\n-\tuint8_t *frame;\n+\tstruct fc_frame_header *pfdmi_abts;\n \tuint8_t d_id[3];\n+\tuint8_t *frame;\n \tstruct fnic *fnic = iport->fnic;\n-\tstruct fc_frame_header *pfabric_abts;\n-\tunsigned long fdmi_tov;\n-\tuint16_t oxid;\n-\tuint16_t frame_size = FNIC_ETH_FCOE_HDRS_OFFSET +\n-\t\t\tsizeof(struct fc_frame_header);\n \n \tframe = fdls_alloc_frame(iport);\n \tif (frame == NULL) {\n \t\tFNIC_FCS_DBG(KERN_ERR, fnic->host, fnic->fnic_num,\n \t\t\t\t\"Failed to allocate frame to send FDMI ABTS\");\n-\t\treturn;\n+\t\treturn NULL;\n \t}\n \n-\tpfabric_abts = (struct fc_frame_header *) (frame + FNIC_ETH_FCOE_HDRS_OFFSET);\n+\tpfdmi_abts = (struct fc_frame_header *) (frame + FNIC_ETH_FCOE_HDRS_OFFSET);\n \tfdls_init_fabric_abts_frame(frame, iport);\n \n \thton24(d_id, FC_FID_MGMT_SERV);\n-\tFNIC_STD_SET_D_ID(*pfabric_abts, d_id);\n+\tFNIC_STD_SET_D_ID(*pfdmi_abts, d_id);\n+\tFNIC_STD_SET_OX_ID(*pfdmi_abts, oxid);\n+\n+\treturn frame;\n+}\n+\n+static void fdls_send_fdmi_abts(struct fnic_iport_s *iport)\n+{\n+\tuint8_t *frame;\n+\tunsigned long fdmi_tov;\n+\tuint16_t frame_size = FNIC_ETH_FCOE_HDRS_OFFSET +\n+\t\t\tsizeof(struct fc_frame_header);\n \n \tif (iport->fabric.fdmi_pending & FDLS_FDMI_PLOGI_PENDING) {\n-\t\toxid = iport->active_oxid_fdmi_plogi;\n-\t\tFNIC_STD_SET_OX_ID(*pfabric_abts, oxid);\n+\t\tframe = fdls_alloc_init_fdmi_abts_frame(iport,\n+\t\t\t\t\t\tiport->active_oxid_fdmi_plogi);\n+\t\tif (frame == NULL)\n+\t\t\treturn;\n+\n \t\tfnic_send_fcoe_frame(iport, frame, frame_size);\n \t} else {\n \t\tif (iport->fabric.fdmi_pending & FDLS_FDMI_REG_HBA_PENDING) {\n-\t\t\toxid = iport->active_oxid_fdmi_rhba;\n-\t\t\tFNIC_STD_SET_OX_ID(*pfabric_abts, oxid);\n+\t\t\tframe = fdls_alloc_init_fdmi_abts_frame(iport,\n+\t\t\t\t\t\tiport->active_oxid_fdmi_rhba);\n+\t\t\tif (frame == NULL)\n+\t\t\t\treturn;\n+\n \t\t\tfnic_send_fcoe_frame(iport, frame, frame_size);\n \t\t}\n \t\tif (iport->fabric.fdmi_pending & FDLS_FDMI_RPA_PENDING) {\n-\t\t\toxid = iport->active_oxid_fdmi_rpa;\n-\t\t\tFNIC_STD_SET_OX_ID(*pfabric_abts, oxid);\n+\t\t\tframe = fdls_alloc_init_fdmi_abts_frame(iport,\n+\t\t\t\t\t\tiport->active_oxid_fdmi_rpa);\n+\t\t\tif (frame == NULL) {\n+\t\t\t\tif (iport->fabric.fdmi_pending & FDLS_FDMI_REG_HBA_PENDING)\n+\t\t\t\t\tgoto arm_timer;\n+\t\t\t\telse\n+\t\t\t\t\treturn;\n+\t\t\t}\n+\n \t\t\tfnic_send_fcoe_frame(iport, frame, frame_size);\n \t\t}\n \t}\n \n+arm_timer:\n \tfdmi_tov = jiffies + msecs_to_jiffies(2 * iport->e_d_tov);\n \tmod_timer(&iport->fabric.fdmi_timer, round_jiffies(fdmi_tov));\n \tiport->fabric.fdmi_pending |= FDLS_FDMI_ABORT_PENDING;\n@@ -2245,6 +2267,21 @@ void fdls_fabric_timer_callback(struct timer_list *t)\n \tspin_unlock_irqrestore(&fnic->fnic_lock, flags);\n }\n \n+void fdls_fdmi_retry_plogi(struct fnic_iport_s *iport)\n+{\n+\tstruct fnic *fnic = iport->fnic;\n+\n+\tiport->fabric.fdmi_pending = 0;\n+\t/* If max retries not exhausted, start over from fdmi plogi */\n+\tif (iport->fabric.fdmi_retry < FDLS_FDMI_MAX_RETRY) {\n+\t\tiport->fabric.fdmi_retry++;\n+\t\tFNIC_FCS_DBG(KERN_INFO, fnic->host, fnic->fnic_num,\n+\t\t\t\t\t \"Retry FDMI PLOGI. FDMI retry: %d\",\n+\t\t\t\t\t iport->fabric.fdmi_retry);\n+\t\tfdls_send_fdmi_plogi(iport);\n+\t}\n+}\n+\n void fdls_fdmi_timer_callback(struct timer_list *t)\n {\n \tstruct fnic_fdls_fabric_s *fabric = timer_container_of(fabric, t,\n@@ -2291,14 +2328,7 @@ void fdls_fdmi_timer_callback(struct timer_list *t)\n \tFNIC_FCS_DBG(KERN_INFO, fnic->host, fnic->fnic_num,\n \t\t\"fdmi timer callback : 0x%x\\n\", iport->fabric.fdmi_pending);\n \n-\tiport->fabric.fdmi_pending = 0;\n-\t/* If max retries not exhaused, start over from fdmi plogi */\n-\tif (iport->fabric.fdmi_retry < FDLS_FDMI_MAX_RETRY) {\n-\t\tiport->fabric.fdmi_retry++;\n-\t\tFNIC_FCS_DBG(KERN_INFO, fnic->host, fnic->fnic_num,\n-\t\t\t\t\t \"retry fdmi timer %d\", iport->fabric.fdmi_retry);\n-\t\tfdls_send_fdmi_plogi(iport);\n-\t}\n+\tfdls_fdmi_retry_plogi(iport);\n \tFNIC_FCS_DBG(KERN_INFO, fnic->host, fnic->fnic_num,\n \t\t\"fdmi timer callback : 0x%x\\n\", iport->fabric.fdmi_pending);\n \tspin_unlock_irqrestore(&fnic->fnic_lock, flags);\n@@ -3716,11 +3746,32 @@ static void fdls_process_fdmi_abts_rsp(struct fnic_iport_s *iport,\n \tswitch (FNIC_FRAME_TYPE(oxid)) {\n \tcase FNIC_FRAME_TYPE_FDMI_PLOGI:\n \t\tfdls_free_oxid(iport, oxid, &iport->active_oxid_fdmi_plogi);\n+\n+\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_PLOGI_PENDING;\n+\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_ABORT_PENDING;\n \t\tbreak;\n \tcase FNIC_FRAME_TYPE_FDMI_RHBA:\n+\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_REG_HBA_PENDING;\n+\n+\t\t/* If RPA is still pending, don't turn off ABORT PENDING.\n+\t\t * We count on the timer to detect the ABTS timeout and take\n+\t\t * corrective action.\n+\t\t */\n+\t\tif (!(iport->fabric.fdmi_pending & FDLS_FDMI_RPA_PENDING))\n+\t\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_ABORT_PENDING;\n+\n \t\tfdls_free_oxid(iport, oxid, &iport->active_oxid_fdmi_rhba);\n \t\tbreak;\n \tcase FNIC_FRAME_TYPE_FDMI_RPA:\n+\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_RPA_PENDING;\n+\n+\t\t/* If RHBA is still pending, don't turn off ABORT PENDING.\n+\t\t * We count on the timer to detect the ABTS timeout and take\n+\t\t * corrective action.\n+\t\t */\n+\t\tif (!(iport->fabric.fdmi_pending & FDLS_FDMI_REG_HBA_PENDING))\n+\t\t\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_ABORT_PENDING;\n+\n \t\tfdls_free_oxid(iport, oxid, &iport->active_oxid_fdmi_rpa);\n \t\tbreak;\n \tdefault:\n@@ -3730,10 +3781,16 @@ static void fdls_process_fdmi_abts_rsp(struct fnic_iport_s *iport,\n \t\tbreak;\n \t}\n \n-\ttimer_delete_sync(&iport->fabric.fdmi_timer);\n-\tiport->fabric.fdmi_pending &= ~FDLS_FDMI_ABORT_PENDING;\n-\n-\tfdls_send_fdmi_plogi(iport);\n+\t/*\n+\t * Only if ABORT PENDING is off, delete the timer, and if no other\n+\t * operations are pending, retry FDMI.\n+\t * Otherwise, let the timer pop and take the appropriate action.\n+\t */\n+\tif (!(iport->fabric.fdmi_pending & FDLS_FDMI_ABORT_PENDING)) {\n+\t\ttimer_delete_sync(&iport->fabric.fdmi_timer);\n+\t\tif (!iport->fabric.fdmi_pending)\n+\t\t\tfdls_fdmi_retry_plogi(iport);\n+\t}\n }\n \n static void\ndiff --git a/drivers/scsi/fnic/fnic.h b/drivers/scsi/fnic/fnic.h\nindex 6c5f6046b1f5..86e293ce530d 100644\n--- a/drivers/scsi/fnic/fnic.h\n+++ b/drivers/scsi/fnic/fnic.h\n@@ -30,7 +30,7 @@\n \n #define DRV_NAME\t\t\"fnic\"\n #define DRV_DESCRIPTION\t\t\"Cisco FCoE HBA Driver\"\n-#define DRV_VERSION\t\t\"1.8.0.0\"\n+#define DRV_VERSION\t\t\"1.8.0.1\"\n #define PFX\t\t\tDRV_NAME \": \"\n #define DFX                     DRV_NAME \"%d: \"\n \ndiff --git a/drivers/scsi/fnic/fnic_fdls.h b/drivers/scsi/fnic/fnic_fdls.h\nindex 8e610b65ad57..531d0b37e450 100644\n--- a/drivers/scsi/fnic/fnic_fdls.h\n+++ b/drivers/scsi/fnic/fnic_fdls.h\n@@ -394,6 +394,7 @@ void fdls_send_tport_abts(struct fnic_iport_s *iport,\n bool fdls_delete_tport(struct fnic_iport_s *iport,\n \t\t       struct fnic_tport_s *tport);\n void fdls_fdmi_timer_callback(struct timer_list *t);\n+void fdls_fdmi_retry_plogi(struct fnic_iport_s *iport);\n \n /* fnic_fcs.c */\n void fnic_fdls_init(struct fnic *fnic, int usefip);",
    "stats": {
      "insertions": 87,
      "deletions": 29,
      "files": 3
    }
  }
]
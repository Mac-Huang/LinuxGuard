[
  {
    "sha": "62dba28275a9a3104d4e33595c7b3328d4032d8d",
    "message": "atm: clip: Fix memory leak of struct clip_vcc.\n\nioctl(ATMARP_MKIP) allocates struct clip_vcc and set it to\nvcc->user_back.\n\nThe code assumes that vcc_destroy_socket() passes NULL skb\nto vcc->push() when the socket is close()d, and then clip_push()\nfrees clip_vcc.\n\nHowever, ioctl(ATMARPD_CTRL) sets NULL to vcc->push() in\natm_init_atmarp(), resulting in memory leak.\n\nLet's serialise two ioctl() by lock_sock() and check vcc->push()\nin atm_init_atmarp() to prevent memleak.\n\nFixes: 1da177e4c3f4 (\"Linux-2.6.12-rc2\")\nSigned-off-by: Kuniyuki Iwashima <kuniyu@google.com>\nReviewed-by: Simon Horman <horms@kernel.org>\nLink: https://patch.msgid.link/20250704062416.1613927-3-kuniyu@google.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
    "author": "Kuniyuki Iwashima",
    "date": "2025-07-09T17:52:26-07:00",
    "files_changed": [
      "net/atm/clip.c"
    ],
    "diff": "diff --git a/net/atm/clip.c b/net/atm/clip.c\nindex f36f2c7d8714..9c9c6c3d9886 100644\n--- a/net/atm/clip.c\n+++ b/net/atm/clip.c\n@@ -645,6 +645,9 @@ static struct atm_dev atmarpd_dev = {\n \n static int atm_init_atmarp(struct atm_vcc *vcc)\n {\n+\tif (vcc->push == clip_push)\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&atmarpd_lock);\n \tif (atmarpd) {\n \t\tmutex_unlock(&atmarpd_lock);\n@@ -669,6 +672,7 @@ static int atm_init_atmarp(struct atm_vcc *vcc)\n static int clip_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n {\n \tstruct atm_vcc *vcc = ATM_SD(sock);\n+\tstruct sock *sk = sock->sk;\n \tint err = 0;\n \n \tswitch (cmd) {\n@@ -689,14 +693,18 @@ static int clip_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n \t\terr = clip_create(arg);\n \t\tbreak;\n \tcase ATMARPD_CTRL:\n+\t\tlock_sock(sk);\n \t\terr = atm_init_atmarp(vcc);\n \t\tif (!err) {\n \t\t\tsock->state = SS_CONNECTED;\n \t\t\t__module_get(THIS_MODULE);\n \t\t}\n+\t\trelease_sock(sk);\n \t\tbreak;\n \tcase ATMARP_MKIP:\n+\t\tlock_sock(sk);\n \t\terr = clip_mkip(vcc, arg);\n+\t\trelease_sock(sk);\n \t\tbreak;\n \tcase ATMARP_SETENTRY:\n \t\terr = clip_setentry(vcc, (__force __be32)arg);",
    "stats": {
      "insertions": 8,
      "deletions": 0,
      "files": 1
    }
  },
  {
    "sha": "4578a747f3c7950be3feb93c2db32eb597a3e55b",
    "message": "KVM: x86: avoid underflow when scaling TSC frequency\n\nIn function kvm_guest_time_update(), __scale_tsc() is used to calculate\na TSC *frequency* rather than a TSC value.  With low-enough ratios,\na TSC value that is less than 1 would underflow to 0 and to an infinite\nwhile loop in kvm_get_time_scale():\n\n  kvm_guest_time_update(struct kvm_vcpu *v)\n    if (kvm_caps.has_tsc_control)\n      tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,\n                                  v->arch.l1_tsc_scaling_ratio);\n        __scale_tsc(u64 ratio, u64 tsc)\n          ratio=122380531, tsc=2299998, N=48\n          ratio*tsc >> N = 0.999... -> 0\n\nLater in the function:\n\n  Call Trace:\n   <TASK>\n   kvm_get_time_scale arch/x86/kvm/x86.c:2458 [inline]\n   kvm_guest_time_update+0x926/0xb00 arch/x86/kvm/x86.c:3268\n   vcpu_enter_guest.constprop.0+0x1e70/0x3cf0 arch/x86/kvm/x86.c:10678\n   vcpu_run+0x129/0x8d0 arch/x86/kvm/x86.c:11126\n   kvm_arch_vcpu_ioctl_run+0x37a/0x13d0 arch/x86/kvm/x86.c:11352\n   kvm_vcpu_ioctl+0x56b/0xe60 virt/kvm/kvm_main.c:4188\n   vfs_ioctl fs/ioctl.c:51 [inline]\n   __do_sys_ioctl fs/ioctl.c:871 [inline]\n   __se_sys_ioctl+0x12d/0x190 fs/ioctl.c:857\n   do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n   do_syscall_64+0x59/0x110 arch/x86/entry/common.c:81\n   entry_SYSCALL_64_after_hwframe+0x78/0xe2\n\nThis can really happen only when fuzzing, since the TSC frequency\nwould have to be nonsensically low.\n\nFixes: 35181e86df97 (\"KVM: x86: Add a common TSC scaling function\")\nReported-by: Yuntao Liu <liuyuntao12@huawei.com>\nSuggested-by: Sean Christopherson <seanjc@google.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>",
    "author": "Paolo Bonzini",
    "date": "2025-07-09T13:52:50-04:00",
    "files_changed": [
      "arch/x86/kvm/x86.c"
    ],
    "diff": "diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c\nindex b58a74c1722d..de51dbd85a58 100644\n--- a/arch/x86/kvm/x86.c\n+++ b/arch/x86/kvm/x86.c\n@@ -3258,9 +3258,11 @@ int kvm_guest_time_update(struct kvm_vcpu *v)\n \n \t/* With all the info we got, fill in the values */\n \n-\tif (kvm_caps.has_tsc_control)\n+\tif (kvm_caps.has_tsc_control) {\n \t\ttgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,\n \t\t\t\t\t    v->arch.l1_tsc_scaling_ratio);\n+\t\ttgt_tsc_khz = tgt_tsc_khz ? : 1;\n+\t}\n \n \tif (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {\n \t\tkvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,",
    "stats": {
      "insertions": 3,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "8c2e52ebbe885c7eeaabd3b7ddcdc1246fc400d2",
    "message": "eventpoll: don't decrement ep refcount while still holding the ep mutex\n\nJann Horn points out that epoll is decrementing the ep refcount and then\ndoing a\n\n    mutex_unlock(&ep->mtx);\n\nafterwards. That's very wrong, because it can lead to a use-after-free.\n\nThat pattern is actually fine for the very last reference, because the\ncode in question will delay the actual call to \"ep_free(ep)\" until after\nit has unlocked the mutex.\n\nBut it's wrong for the much subtler \"next to last\" case when somebody\n*else* may also be dropping their reference and free the ep while we're\nstill using the mutex.\n\nNote that this is true even if that other user is also using the same ep\nmutex: mutexes, unlike spinlocks, can not be used for object ownership,\neven if they guarantee mutual exclusion.\n\nA mutex \"unlock\" operation is not atomic, and as one user is still\naccessing the mutex as part of unlocking it, another user can come in\nand get the now released mutex and free the data structure while the\nfirst user is still cleaning up.\n\nSee our mutex documentation in Documentation/locking/mutex-design.rst,\nin particular the section [1] about semantics:\n\n\t\"mutex_unlock() may access the mutex structure even after it has\n\t internally released the lock already - so it's not safe for\n\t another context to acquire the mutex and assume that the\n\t mutex_unlock() context is not using the structure anymore\"\n\nSo if we drop our ep ref before the mutex unlock, but we weren't the\nlast one, we may then unlock the mutex, another user comes in, drops\n_their_ reference and releases the 'ep' as it now has no users - all\nwhile the mutex_unlock() is still accessing it.\n\nFix this by simply moving the ep refcount dropping to outside the mutex:\nthe refcount itself is atomic, and doesn't need mutex protection (that's\nthe whole _point_ of refcounts: unlike mutexes, they are inherently\nabout object lifetimes).\n\nReported-by: Jann Horn <jannh@google.com>\nLink: https://docs.kernel.org/locking/mutex-design.html#semantics [1]\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Christian Brauner <brauner@kernel.org>\nCc: Jan Kara <jack@suse.cz>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
    "author": "Linus Torvalds",
    "date": "2025-07-09T10:38:29-07:00",
    "files_changed": [
      "fs/eventpoll.c"
    ],
    "diff": "diff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex a97a771a459c..895256cd2786 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -828,7 +828,7 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n \tkfree_rcu(epi, rcu);\n \n \tpercpu_counter_dec(&ep->user->epoll_watches);\n-\treturn ep_refcount_dec_and_test(ep);\n+\treturn true;\n }\n \n /*\n@@ -836,14 +836,14 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n  */\n static void ep_remove_safe(struct eventpoll *ep, struct epitem *epi)\n {\n-\tWARN_ON_ONCE(__ep_remove(ep, epi, false));\n+\tif (__ep_remove(ep, epi, false))\n+\t\tWARN_ON_ONCE(ep_refcount_dec_and_test(ep));\n }\n \n static void ep_clear_and_put(struct eventpoll *ep)\n {\n \tstruct rb_node *rbp, *next;\n \tstruct epitem *epi;\n-\tbool dispose;\n \n \t/* We need to release all tasks waiting for these file */\n \tif (waitqueue_active(&ep->poll_wait))\n@@ -876,10 +876,8 @@ static void ep_clear_and_put(struct eventpoll *ep)\n \t\tcond_resched();\n \t}\n \n-\tdispose = ep_refcount_dec_and_test(ep);\n \tmutex_unlock(&ep->mtx);\n-\n-\tif (dispose)\n+\tif (ep_refcount_dec_and_test(ep))\n \t\tep_free(ep);\n }\n \n@@ -1100,7 +1098,7 @@ void eventpoll_release_file(struct file *file)\n \t\tdispose = __ep_remove(ep, epi, true);\n \t\tmutex_unlock(&ep->mtx);\n \n-\t\tif (dispose)\n+\t\tif (dispose && ep_refcount_dec_and_test(ep))\n \t\t\tep_free(ep);\n \t\tgoto again;\n \t}",
    "stats": {
      "insertions": 5,
      "deletions": 7,
      "files": 1
    }
  },
  {
    "sha": "bd46cece51a36ef088f22ef0416ac13b0a46d5b0",
    "message": "drm/gem: Fix race in drm_gem_handle_create_tail()\n\nObject creation is a careful dance where we must guarantee that the\nobject is fully constructed before it is visible to other threads, and\nGEM buffer objects are no difference.\n\nFinal publishing happens by calling drm_gem_handle_create(). After\nthat the only allowed thing to do is call drm_gem_object_put() because\na concurrent call to the GEM_CLOSE ioctl with a correctly guessed id\n(which is trivial since we have a linear allocator) can already tear\ndown the object again.\n\nLuckily most drivers get this right, the very few exceptions I've\npinged the relevant maintainers for. Unfortunately we also need\ndrm_gem_handle_create() when creating additional handles for an\nalready existing object (e.g. GETFB ioctl or the various bo import\nioctl), and hence we cannot have a drm_gem_handle_create_and_put() as\nthe only exported function to stop these issues from happening.\n\nNow unfortunately the implementation of drm_gem_handle_create() isn't\nliving up to standards: It does correctly finishe object\ninitialization at the global level, and hence is safe against a\nconcurrent tear down. But it also sets up the file-private aspects of\nthe handle, and that part goes wrong: We fully register the object in\nthe drm_file.object_idr before calling drm_vma_node_allow() or\nobj->funcs->open, which opens up races against concurrent removal of\nthat handle in drm_gem_handle_delete().\n\nFix this with the usual two-stage approach of first reserving the\nhandle id, and then only registering the object after we've completed\nthe file-private setup.\n\nJacek reported this with a testcase of concurrently calling GEM_CLOSE\non a freshly-created object (which also destroys the object), but it\nshould be possible to hit this with just additional handles created\nthrough import or GETFB without completed destroying the underlying\nobject with the concurrent GEM_CLOSE ioctl calls.\n\nNote that the close-side of this race was fixed in f6cd7daecff5 (\"drm:\nRelease driver references to handle before making it available\nagain\"), which means a cool 9 years have passed until someone noticed\nthat we need to make this symmetry or there's still gaps left :-/\nWithout the 2-stage close approach we'd still have a race, therefore\nthat's an integral part of this bugfix.\n\nMore importantly, this means we can have NULL pointers behind\nallocated id in our drm_file.object_idr. We need to check for that\nnow:\n\n- drm_gem_handle_delete() checks for ERR_OR_NULL already\n\n- drm_gem.c:object_lookup() also chekcs for NULL\n\n- drm_gem_release() should never be called if there's another thread\n  still existing that could call into an IOCTL that creates a new\n  handle, so cannot race. For paranoia I added a NULL check to\n  drm_gem_object_release_handle() though.\n\n- most drivers (etnaviv, i915, msm) are find because they use\n  idr_find(), which maps both ENOENT and NULL to NULL.\n\n- drivers using idr_for_each_entry() should also be fine, because\n  idr_get_next does filter out NULL entries and continues the\n  iteration.\n\n- The same holds for drm_show_memory_stats().\n\nv2: Use drm_WARN_ON (Thomas)\n\nReported-by: Jacek Lawrynowicz <jacek.lawrynowicz@linux.intel.com>\nTested-by: Jacek Lawrynowicz <jacek.lawrynowicz@linux.intel.com>\nReviewed-by: Thomas Zimmermann <tzimmermann@suse.de>\nCc: stable@vger.kernel.org\nCc: Jacek Lawrynowicz <jacek.lawrynowicz@linux.intel.com>\nCc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>\nCc: Maxime Ripard <mripard@kernel.org>\nCc: Thomas Zimmermann <tzimmermann@suse.de>\nCc: David Airlie <airlied@gmail.com>\nCc: Simona Vetter <simona@ffwll.ch>\nSigned-off-by: Simona Vetter <simona.vetter@intel.com>\nSigned-off-by: Simona Vetter <simona.vetter@ffwll.ch>\nLink: https://patchwork.freedesktop.org/patch/msgid/20250707151814.603897-1-simona.vetter@ffwll.ch",
    "author": "Simona Vetter",
    "date": "2025-07-09T15:53:34+02:00",
    "files_changed": [
      "drivers/gpu/drm/drm_gem.c",
      "include/drm/drm_file.h"
    ],
    "diff": "diff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c\nindex aad6ac9748cc..ac0524595bd6 100644\n--- a/drivers/gpu/drm/drm_gem.c\n+++ b/drivers/gpu/drm/drm_gem.c\n@@ -325,6 +325,9 @@ drm_gem_object_release_handle(int id, void *ptr, void *data)\n \tstruct drm_file *file_priv = data;\n \tstruct drm_gem_object *obj = ptr;\n \n+\tif (drm_WARN_ON(obj->dev, !data))\n+\t\treturn 0;\n+\n \tif (obj->funcs->close)\n \t\tobj->funcs->close(obj, file_priv);\n \n@@ -445,7 +448,7 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,\n \tidr_preload(GFP_KERNEL);\n \tspin_lock(&file_priv->table_lock);\n \n-\tret = idr_alloc(&file_priv->object_idr, obj, 1, 0, GFP_NOWAIT);\n+\tret = idr_alloc(&file_priv->object_idr, NULL, 1, 0, GFP_NOWAIT);\n \n \tspin_unlock(&file_priv->table_lock);\n \tidr_preload_end();\n@@ -466,6 +469,11 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,\n \t\t\tgoto err_revoke;\n \t}\n \n+\t/* mirrors drm_gem_handle_delete to avoid races */\n+\tspin_lock(&file_priv->table_lock);\n+\tobj = idr_replace(&file_priv->object_idr, obj, handle);\n+\tWARN_ON(obj != NULL);\n+\tspin_unlock(&file_priv->table_lock);\n \t*handlep = handle;\n \treturn 0;\n \ndiff --git a/include/drm/drm_file.h b/include/drm/drm_file.h\nindex 5c3b2aa3e69d..d344d41e6cfe 100644\n--- a/include/drm/drm_file.h\n+++ b/include/drm/drm_file.h\n@@ -300,6 +300,9 @@ struct drm_file {\n \t *\n \t * Mapping of mm object handles to object pointers. Used by the GEM\n \t * subsystem. Protected by @table_lock.\n+\t *\n+\t * Note that allocated entries might be NULL as a transient state when\n+\t * creating or deleting a handle.\n \t */\n \tstruct idr object_idr;\n ",
    "stats": {
      "insertions": 12,
      "deletions": 1,
      "files": 2
    }
  },
  {
    "sha": "72782127388d96e971f0186996a5bd44e64a1665",
    "message": "Merge tag 'modules-6.16-rc6.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/modules/linux\n\nPull modules fixes from Daniel Gomez:\n \"This includes two fixes: one introduced in the current release cycle\n  and another introduced back in v6.4-rc1. Additionally, as Petr and\n  Luis mentioned in previous pull requests, add myself (Daniel Gomez) to\n  the list of modules maintainers.\n\n  The first was reported by Intel's kernel test robot, and it addresses\n  a crash exposed by Sebastian's commit c50d295c37f2 (\"rds: Use\n  nested-BH locking for rds_page_remainder\") by allowing relocations for\n  the per-CPU section even if it lacks the SHF_ALLOC flag.\n\n  Petr and Sebastian went down to the archive history (before Git) and\n  found the commit that broke it at [1] / [2] (\"Don't relocate\n  non-allocated regions in modules.\").\n\n  The second fix, reported and fixed by Petr (with additional cleanup),\n  resolves a memory leak by ensuring proper deallocation if module\n  loading fails.\n\n  We couldn't find a reproducer other than forcing it manually or\n  leveraging eBPF. So, I tested it by enabling error injection in the\n  codetag functions through the error path that produces the leak and\n  made it fail until execmem is unable to allocate more memory\"\n\nLink: https://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux-fullhistory.git/commit/?id=b3b91325f3c7 [1]\nLink: https://git.kernel.org/pub/scm/linux/kernel/git/tglx/history.git/commit/?id=1a6100caae [2]\n\n* tag 'modules-6.16-rc6.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/modules/linux:\n  MAINTAINERS: update Daniel Gomez's role and email address\n  module: Make sure relocations are applied to the per-CPU section\n  module: Avoid unnecessary return value initialization in move_module()\n  module: Fix memory deallocation on error path in move_module()",
    "author": "Linus Torvalds",
    "date": "2025-07-08T13:10:32-07:00",
    "files_changed": [
      "kernel/module/main.c"
    ],
    "diff": "diff --git a/MAINTAINERS b/MAINTAINERS\nindex 2b8d9e608829..240793fbe64b 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16824,8 +16824,8 @@ F:\tinclude/dt-bindings/clock/mobileye,eyeq5-clk.h\n MODULE SUPPORT\n M:\tLuis Chamberlain <mcgrof@kernel.org>\n M:\tPetr Pavlu <petr.pavlu@suse.com>\n+M:\tDaniel Gomez <da.gomez@kernel.org>\n R:\tSami Tolvanen <samitolvanen@google.com>\n-R:\tDaniel Gomez <da.gomez@samsung.com>\n L:\tlinux-modules@vger.kernel.org\n L:\tlinux-kernel@vger.kernel.org\n S:\tMaintained\ndiff --git a/kernel/module/main.c b/kernel/module/main.c\nindex 413ac6ea3702..c2c08007029d 100644\n--- a/kernel/module/main.c\n+++ b/kernel/module/main.c\n@@ -1573,8 +1573,14 @@ static int apply_relocations(struct module *mod, const struct load_info *info)\n \t\tif (infosec >= info->hdr->e_shnum)\n \t\t\tcontinue;\n \n-\t\t/* Don't bother with non-allocated sections */\n-\t\tif (!(info->sechdrs[infosec].sh_flags & SHF_ALLOC))\n+\t\t/*\n+\t\t * Don't bother with non-allocated sections.\n+\t\t * An exception is the percpu section, which has separate allocations\n+\t\t * for individual CPUs. We relocate the percpu section in the initial\n+\t\t * ELF template and subsequently copy it to the per-CPU destinations.\n+\t\t */\n+\t\tif (!(info->sechdrs[infosec].sh_flags & SHF_ALLOC) &&\n+\t\t    (!infosec || infosec != info->index.pcpu))\n \t\t\tcontinue;\n \n \t\tif (info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)\n@@ -2696,9 +2702,8 @@ static int find_module_sections(struct module *mod, struct load_info *info)\n \n static int move_module(struct module *mod, struct load_info *info)\n {\n-\tint i;\n-\tenum mod_mem_type t = 0;\n-\tint ret = -ENOMEM;\n+\tint i, ret;\n+\tenum mod_mem_type t = MOD_MEM_NUM_TYPES;\n \tbool codetag_section_found = false;\n \n \tfor_each_mod_mem_type(type) {\n@@ -2776,7 +2781,7 @@ static int move_module(struct module *mod, struct load_info *info)\n \treturn 0;\n out_err:\n \tmodule_memory_restore_rox(mod);\n-\tfor (t--; t >= 0; t--)\n+\twhile (t--)\n \t\tmodule_memory_free(mod, t);\n \tif (codetag_section_found)\n \t\tcodetag_free_module_sections(mod);",
    "stats": {
      "insertions": 12,
      "deletions": 7,
      "files": 2
    }
  }
]
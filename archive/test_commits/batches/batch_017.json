[
  {
    "sha": "4c06e63b92038fadb566b652ec3ec04e228931e8",
    "message": "Merge tag 'for-6.16-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux\n\nPull btrfs fixes from David Sterba:\n\n - tree-log fixes:\n    - fixes of log tracking of directories and subvolumes\n    - fix iteration and error handling of inode references\n      during log replay\n\n - fix free space tree rebuild (reported by syzbot)\n\n* tag 'for-6.16-rc4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:\n  btrfs: use btrfs_record_snapshot_destroy() during rmdir\n  btrfs: propagate last_unlink_trans earlier when doing a rmdir\n  btrfs: record new subvolume in parent dir earlier to avoid dir logging races\n  btrfs: fix inode lookup error handling during log replay\n  btrfs: fix iteration of extrefs during log replay\n  btrfs: fix missing error handling when searching for inode refs during log replay\n  btrfs: fix failure to rebuild free space tree using multiple transactions",
    "author": "Linus Torvalds",
    "date": "2025-07-03T13:29:56-07:00",
    "files_changed": [
      "fs/btrfs/block-group.h",
      "fs/btrfs/free-space-tree.c",
      "fs/btrfs/inode.c",
      "fs/btrfs/ioctl.c",
      "fs/btrfs/tree-log.c"
    ],
    "diff": "diff --git a/fs/btrfs/block-group.h b/fs/btrfs/block-group.h\nindex 9de356bcb411..aa176cc9a324 100644\n--- a/fs/btrfs/block-group.h\n+++ b/fs/btrfs/block-group.h\n@@ -83,6 +83,8 @@ enum btrfs_block_group_flags {\n \tBLOCK_GROUP_FLAG_ZONED_DATA_RELOC,\n \t/* Does the block group need to be added to the free space tree? */\n \tBLOCK_GROUP_FLAG_NEEDS_FREE_SPACE,\n+\t/* Set after we add a new block group to the free space tree. */\n+\tBLOCK_GROUP_FLAG_FREE_SPACE_ADDED,\n \t/* Indicate that the block group is placed on a sequential zone */\n \tBLOCK_GROUP_FLAG_SEQUENTIAL_ZONE,\n \t/*\ndiff --git a/fs/btrfs/free-space-tree.c b/fs/btrfs/free-space-tree.c\nindex a3e2a2a81461..a83c268f7f87 100644\n--- a/fs/btrfs/free-space-tree.c\n+++ b/fs/btrfs/free-space-tree.c\n@@ -1241,6 +1241,7 @@ static int clear_free_space_tree(struct btrfs_trans_handle *trans,\n {\n \tBTRFS_PATH_AUTO_FREE(path);\n \tstruct btrfs_key key;\n+\tstruct rb_node *node;\n \tint nr;\n \tint ret;\n \n@@ -1269,6 +1270,16 @@ static int clear_free_space_tree(struct btrfs_trans_handle *trans,\n \t\tbtrfs_release_path(path);\n \t}\n \n+\tnode = rb_first_cached(&trans->fs_info->block_group_cache_tree);\n+\twhile (node) {\n+\t\tstruct btrfs_block_group *bg;\n+\n+\t\tbg = rb_entry(node, struct btrfs_block_group, cache_node);\n+\t\tclear_bit(BLOCK_GROUP_FLAG_FREE_SPACE_ADDED, &bg->runtime_flags);\n+\t\tnode = rb_next(node);\n+\t\tcond_resched();\n+\t}\n+\n \treturn 0;\n }\n \n@@ -1358,12 +1369,18 @@ int btrfs_rebuild_free_space_tree(struct btrfs_fs_info *fs_info)\n \n \t\tblock_group = rb_entry(node, struct btrfs_block_group,\n \t\t\t\t       cache_node);\n+\n+\t\tif (test_bit(BLOCK_GROUP_FLAG_FREE_SPACE_ADDED,\n+\t\t\t     &block_group->runtime_flags))\n+\t\t\tgoto next;\n+\n \t\tret = populate_free_space_tree(trans, block_group);\n \t\tif (ret) {\n \t\t\tbtrfs_abort_transaction(trans, ret);\n \t\t\tbtrfs_end_transaction(trans);\n \t\t\treturn ret;\n \t\t}\n+next:\n \t\tif (btrfs_should_end_transaction(trans)) {\n \t\t\tbtrfs_end_transaction(trans);\n \t\t\ttrans = btrfs_start_transaction(free_space_root, 1);\n@@ -1390,6 +1407,29 @@ static int __add_block_group_free_space(struct btrfs_trans_handle *trans,\n \n \tclear_bit(BLOCK_GROUP_FLAG_NEEDS_FREE_SPACE, &block_group->runtime_flags);\n \n+\t/*\n+\t * While rebuilding the free space tree we may allocate new metadata\n+\t * block groups while modifying the free space tree.\n+\t *\n+\t * Because during the rebuild (at btrfs_rebuild_free_space_tree()) we\n+\t * can use multiple transactions, every time btrfs_end_transaction() is\n+\t * called at btrfs_rebuild_free_space_tree() we finish the creation of\n+\t * new block groups by calling btrfs_create_pending_block_groups(), and\n+\t * that in turn calls us, through add_block_group_free_space(), to add\n+\t * a free space info item and a free space extent item for the block\n+\t * group.\n+\t *\n+\t * Then later btrfs_rebuild_free_space_tree() may find such new block\n+\t * groups and processes them with populate_free_space_tree(), which can\n+\t * fail with EEXIST since there are already items for the block group in\n+\t * the free space tree. Notice that we say \"may find\" because a new\n+\t * block group may be added to the block groups rbtree in a node before\n+\t * or after the block group currently being processed by the rebuild\n+\t * process. So signal the rebuild process to skip such new block groups\n+\t * if it finds them.\n+\t */\n+\tset_bit(BLOCK_GROUP_FLAG_FREE_SPACE_ADDED, &block_group->runtime_flags);\n+\n \tret = add_new_free_space_info(trans, block_group, path);\n \tif (ret)\n \t\treturn ret;\ndiff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c\nindex 26d6ed170a19..fc66872b4c74 100644\n--- a/fs/btrfs/inode.c\n+++ b/fs/btrfs/inode.c\n@@ -4710,7 +4710,6 @@ static int btrfs_rmdir(struct inode *dir, struct dentry *dentry)\n \tstruct btrfs_fs_info *fs_info = BTRFS_I(inode)->root->fs_info;\n \tint ret = 0;\n \tstruct btrfs_trans_handle *trans;\n-\tu64 last_unlink_trans;\n \tstruct fscrypt_name fname;\n \n \tif (inode->i_size > BTRFS_EMPTY_DIR_SIZE)\n@@ -4736,6 +4735,23 @@ static int btrfs_rmdir(struct inode *dir, struct dentry *dentry)\n \t\tgoto out_notrans;\n \t}\n \n+\t/*\n+\t * Propagate the last_unlink_trans value of the deleted dir to its\n+\t * parent directory. This is to prevent an unrecoverable log tree in the\n+\t * case we do something like this:\n+\t * 1) create dir foo\n+\t * 2) create snapshot under dir foo\n+\t * 3) delete the snapshot\n+\t * 4) rmdir foo\n+\t * 5) mkdir foo\n+\t * 6) fsync foo or some file inside foo\n+\t *\n+\t * This is because we can't unlink other roots when replaying the dir\n+\t * deletes for directory foo.\n+\t */\n+\tif (BTRFS_I(inode)->last_unlink_trans >= trans->transid)\n+\t\tbtrfs_record_snapshot_destroy(trans, BTRFS_I(dir));\n+\n \tif (unlikely(btrfs_ino(BTRFS_I(inode)) == BTRFS_EMPTY_SUBVOL_DIR_OBJECTID)) {\n \t\tret = btrfs_unlink_subvol(trans, BTRFS_I(dir), dentry);\n \t\tgoto out;\n@@ -4745,27 +4761,11 @@ static int btrfs_rmdir(struct inode *dir, struct dentry *dentry)\n \tif (ret)\n \t\tgoto out;\n \n-\tlast_unlink_trans = BTRFS_I(inode)->last_unlink_trans;\n-\n \t/* now the directory is empty */\n \tret = btrfs_unlink_inode(trans, BTRFS_I(dir), BTRFS_I(d_inode(dentry)),\n \t\t\t\t &fname.disk_name);\n-\tif (!ret) {\n+\tif (!ret)\n \t\tbtrfs_i_size_write(BTRFS_I(inode), 0);\n-\t\t/*\n-\t\t * Propagate the last_unlink_trans value of the deleted dir to\n-\t\t * its parent directory. This is to prevent an unrecoverable\n-\t\t * log tree in the case we do something like this:\n-\t\t * 1) create dir foo\n-\t\t * 2) create snapshot under dir foo\n-\t\t * 3) delete the snapshot\n-\t\t * 4) rmdir foo\n-\t\t * 5) mkdir foo\n-\t\t * 6) fsync foo or some file inside foo\n-\t\t */\n-\t\tif (last_unlink_trans >= trans->transid)\n-\t\t\tBTRFS_I(dir)->last_unlink_trans = last_unlink_trans;\n-\t}\n out:\n \tbtrfs_end_transaction(trans);\n out_notrans:\ndiff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c\nindex 4eda35bdba71..8a60983a697c 100644\n--- a/fs/btrfs/ioctl.c\n+++ b/fs/btrfs/ioctl.c\n@@ -666,14 +666,14 @@ static noinline int create_subvol(struct mnt_idmap *idmap,\n \t\tgoto out;\n \t}\n \n+\tbtrfs_record_new_subvolume(trans, BTRFS_I(dir));\n+\n \tret = btrfs_create_new_inode(trans, &new_inode_args);\n \tif (ret) {\n \t\tbtrfs_abort_transaction(trans, ret);\n \t\tgoto out;\n \t}\n \n-\tbtrfs_record_new_subvolume(trans, BTRFS_I(dir));\n-\n \td_instantiate_new(dentry, new_inode_args.inode);\n \tnew_inode_args.inode = NULL;\n \ndiff --git a/fs/btrfs/tree-log.c b/fs/btrfs/tree-log.c\nindex 858b609e292c..cea8a7e9d6d3 100644\n--- a/fs/btrfs/tree-log.c\n+++ b/fs/btrfs/tree-log.c\n@@ -143,6 +143,9 @@ static struct btrfs_inode *btrfs_iget_logging(u64 objectid, struct btrfs_root *r\n \tunsigned int nofs_flag;\n \tstruct btrfs_inode *inode;\n \n+\t/* Only meant to be called for subvolume roots and not for log roots. */\n+\tASSERT(is_fstree(btrfs_root_id(root)));\n+\n \t/*\n \t * We're holding a transaction handle whether we are logging or\n \t * replaying a log tree, so we must make sure NOFS semantics apply\n@@ -604,21 +607,6 @@ static int read_alloc_one_name(struct extent_buffer *eb, void *start, int len,\n \treturn 0;\n }\n \n-/*\n- * simple helper to read an inode off the disk from a given root\n- * This can only be called for subvolume roots and not for the log\n- */\n-static noinline struct btrfs_inode *read_one_inode(struct btrfs_root *root,\n-\t\t\t\t\t\t   u64 objectid)\n-{\n-\tstruct btrfs_inode *inode;\n-\n-\tinode = btrfs_iget_logging(objectid, root);\n-\tif (IS_ERR(inode))\n-\t\treturn NULL;\n-\treturn inode;\n-}\n-\n /* replays a single extent in 'eb' at 'slot' with 'key' into the\n  * subvolume 'root'.  path is released on entry and should be released\n  * on exit.\n@@ -674,9 +662,9 @@ static noinline int replay_one_extent(struct btrfs_trans_handle *trans,\n \t\treturn -EUCLEAN;\n \t}\n \n-\tinode = read_one_inode(root, key->objectid);\n-\tif (!inode)\n-\t\treturn -EIO;\n+\tinode = btrfs_iget_logging(key->objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n \t/*\n \t * first check to see if we already have this extent in the\n@@ -948,9 +936,10 @@ static noinline int drop_one_dir_item(struct btrfs_trans_handle *trans,\n \n \tbtrfs_release_path(path);\n \n-\tinode = read_one_inode(root, location.objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(location.objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -1073,7 +1062,9 @@ static inline int __add_inode_ref(struct btrfs_trans_handle *trans,\n \tsearch_key.type = BTRFS_INODE_REF_KEY;\n \tsearch_key.offset = parent_objectid;\n \tret = btrfs_search_slot(NULL, root, &search_key, path, 0, 0);\n-\tif (ret == 0) {\n+\tif (ret < 0) {\n+\t\treturn ret;\n+\t} else if (ret == 0) {\n \t\tstruct btrfs_inode_ref *victim_ref;\n \t\tunsigned long ptr;\n \t\tunsigned long ptr_end;\n@@ -1146,13 +1137,13 @@ static inline int __add_inode_ref(struct btrfs_trans_handle *trans,\n \t\t\tstruct fscrypt_str victim_name;\n \n \t\t\textref = (struct btrfs_inode_extref *)(base + cur_offset);\n+\t\t\tvictim_name.len = btrfs_inode_extref_name_len(leaf, extref);\n \n \t\t\tif (btrfs_inode_extref_parent(leaf, extref) != parent_objectid)\n \t\t\t\tgoto next;\n \n \t\t\tret = read_alloc_one_name(leaf, &extref->name,\n-\t\t\t\t btrfs_inode_extref_name_len(leaf, extref),\n-\t\t\t\t &victim_name);\n+\t\t\t\t\t\t  victim_name.len, &victim_name);\n \t\t\tif (ret)\n \t\t\t\treturn ret;\n \n@@ -1167,10 +1158,10 @@ static inline int __add_inode_ref(struct btrfs_trans_handle *trans,\n \t\t\t\tkfree(victim_name.name);\n \t\t\t\treturn ret;\n \t\t\t} else if (!ret) {\n-\t\t\t\tret = -ENOENT;\n-\t\t\t\tvictim_parent = read_one_inode(root,\n-\t\t\t\t\t\tparent_objectid);\n-\t\t\t\tif (victim_parent) {\n+\t\t\t\tvictim_parent = btrfs_iget_logging(parent_objectid, root);\n+\t\t\t\tif (IS_ERR(victim_parent)) {\n+\t\t\t\t\tret = PTR_ERR(victim_parent);\n+\t\t\t\t} else {\n \t\t\t\t\tinc_nlink(&inode->vfs_inode);\n \t\t\t\t\tbtrfs_release_path(path);\n \n@@ -1315,9 +1306,9 @@ static int unlink_old_inode_refs(struct btrfs_trans_handle *trans,\n \t\t\tstruct btrfs_inode *dir;\n \n \t\t\tbtrfs_release_path(path);\n-\t\t\tdir = read_one_inode(root, parent_id);\n-\t\t\tif (!dir) {\n-\t\t\t\tret = -ENOENT;\n+\t\t\tdir = btrfs_iget_logging(parent_id, root);\n+\t\t\tif (IS_ERR(dir)) {\n+\t\t\t\tret = PTR_ERR(dir);\n \t\t\t\tkfree(name.name);\n \t\t\t\tgoto out;\n \t\t\t}\n@@ -1389,15 +1380,17 @@ static noinline int add_inode_ref(struct btrfs_trans_handle *trans,\n \t * copy the back ref in.  The link count fixup code will take\n \t * care of the rest\n \t */\n-\tdir = read_one_inode(root, parent_objectid);\n-\tif (!dir) {\n-\t\tret = -ENOENT;\n+\tdir = btrfs_iget_logging(parent_objectid, root);\n+\tif (IS_ERR(dir)) {\n+\t\tret = PTR_ERR(dir);\n+\t\tdir = NULL;\n \t\tgoto out;\n \t}\n \n-\tinode = read_one_inode(root, inode_objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(inode_objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -1409,11 +1402,13 @@ static noinline int add_inode_ref(struct btrfs_trans_handle *trans,\n \t\t\t * parent object can change from one array\n \t\t\t * item to another.\n \t\t\t */\n-\t\t\tif (!dir)\n-\t\t\t\tdir = read_one_inode(root, parent_objectid);\n \t\t\tif (!dir) {\n-\t\t\t\tret = -ENOENT;\n-\t\t\t\tgoto out;\n+\t\t\t\tdir = btrfs_iget_logging(parent_objectid, root);\n+\t\t\t\tif (IS_ERR(dir)) {\n+\t\t\t\t\tret = PTR_ERR(dir);\n+\t\t\t\t\tdir = NULL;\n+\t\t\t\t\tgoto out;\n+\t\t\t\t}\n \t\t\t}\n \t\t} else {\n \t\t\tret = ref_get_fields(eb, ref_ptr, &name, &ref_index);\n@@ -1682,9 +1677,9 @@ static noinline int fixup_inode_link_counts(struct btrfs_trans_handle *trans,\n \t\t\tbreak;\n \n \t\tbtrfs_release_path(path);\n-\t\tinode = read_one_inode(root, key.offset);\n-\t\tif (!inode) {\n-\t\t\tret = -EIO;\n+\t\tinode = btrfs_iget_logging(key.offset, root);\n+\t\tif (IS_ERR(inode)) {\n+\t\t\tret = PTR_ERR(inode);\n \t\t\tbreak;\n \t\t}\n \n@@ -1720,9 +1715,9 @@ static noinline int link_to_fixup_dir(struct btrfs_trans_handle *trans,\n \tstruct btrfs_inode *inode;\n \tstruct inode *vfs_inode;\n \n-\tinode = read_one_inode(root, objectid);\n-\tif (!inode)\n-\t\treturn -EIO;\n+\tinode = btrfs_iget_logging(objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n \tvfs_inode = &inode->vfs_inode;\n \tkey.objectid = BTRFS_TREE_LOG_FIXUP_OBJECTID;\n@@ -1761,14 +1756,14 @@ static noinline int insert_one_name(struct btrfs_trans_handle *trans,\n \tstruct btrfs_inode *dir;\n \tint ret;\n \n-\tinode = read_one_inode(root, location->objectid);\n-\tif (!inode)\n-\t\treturn -ENOENT;\n+\tinode = btrfs_iget_logging(location->objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n-\tdir = read_one_inode(root, dirid);\n-\tif (!dir) {\n+\tdir = btrfs_iget_logging(dirid, root);\n+\tif (IS_ERR(dir)) {\n \t\tiput(&inode->vfs_inode);\n-\t\treturn -EIO;\n+\t\treturn PTR_ERR(dir);\n \t}\n \n \tret = btrfs_add_link(trans, dir, inode, name, 1, index);\n@@ -1845,9 +1840,9 @@ static noinline int replay_one_name(struct btrfs_trans_handle *trans,\n \tbool update_size = true;\n \tbool name_added = false;\n \n-\tdir = read_one_inode(root, key->objectid);\n-\tif (!dir)\n-\t\treturn -EIO;\n+\tdir = btrfs_iget_logging(key->objectid, root);\n+\tif (IS_ERR(dir))\n+\t\treturn PTR_ERR(dir);\n \n \tret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name);\n \tif (ret)\n@@ -2147,9 +2142,10 @@ static noinline int check_item_in_log(struct btrfs_trans_handle *trans,\n \tbtrfs_dir_item_key_to_cpu(eb, di, &location);\n \tbtrfs_release_path(path);\n \tbtrfs_release_path(log_path);\n-\tinode = read_one_inode(root, location.objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(location.objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -2301,14 +2297,17 @@ static noinline int replay_dir_deletes(struct btrfs_trans_handle *trans,\n \tif (!log_path)\n \t\treturn -ENOMEM;\n \n-\tdir = read_one_inode(root, dirid);\n-\t/* it isn't an error if the inode isn't there, that can happen\n-\t * because we replay the deletes before we copy in the inode item\n-\t * from the log\n+\tdir = btrfs_iget_logging(dirid, root);\n+\t/*\n+\t * It isn't an error if the inode isn't there, that can happen because\n+\t * we replay the deletes before we copy in the inode item from the log.\n \t */\n-\tif (!dir) {\n+\tif (IS_ERR(dir)) {\n \t\tbtrfs_free_path(log_path);\n-\t\treturn 0;\n+\t\tret = PTR_ERR(dir);\n+\t\tif (ret == -ENOENT)\n+\t\t\tret = 0;\n+\t\treturn ret;\n \t}\n \n \trange_start = 0;\n@@ -2467,9 +2466,9 @@ static int replay_one_buffer(struct btrfs_root *log, struct extent_buffer *eb,\n \t\t\t\tstruct btrfs_inode *inode;\n \t\t\t\tu64 from;\n \n-\t\t\t\tinode = read_one_inode(root, key.objectid);\n-\t\t\t\tif (!inode) {\n-\t\t\t\t\tret = -EIO;\n+\t\t\t\tinode = btrfs_iget_logging(key.objectid, root);\n+\t\t\t\tif (IS_ERR(inode)) {\n+\t\t\t\t\tret = PTR_ERR(inode);\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t\tfrom = ALIGN(i_size_read(&inode->vfs_inode),\n@@ -7448,6 +7447,8 @@ void btrfs_record_snapshot_destroy(struct btrfs_trans_handle *trans,\n  * full log sync.\n  * Also we don't need to worry with renames, since btrfs_rename() marks the log\n  * for full commit when renaming a subvolume.\n+ *\n+ * Must be called before creating the subvolume entry in its parent directory.\n  */\n void btrfs_record_new_subvolume(const struct btrfs_trans_handle *trans,\n \t\t\t\tstruct btrfs_inode *dir)",
    "stats": {
      "insertions": 131,
      "deletions": 88,
      "files": 5
    }
  },
  {
    "sha": "025c1970da725b07701464990f747fe1c2bd797f",
    "message": "Merge tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi\n\nPull SCSI fixes from James Bottomley:\n \"Driver fixes plus core sd.c fix are all small and obvious.\n\n  The larger change to hosts.c is less obvious, but required to avoid\n  data corruption caused by bio splitting\"\n\n* tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:\n  scsi: ufs: core: Fix spelling of a sysfs attribute name\n  scsi: core: Enforce unlimited max_segment_size when virt_boundary_mask is set\n  scsi: RDMA/srp: Don't set a max_segment_size when virt_boundary_mask is set\n  scsi: sd: Fix VPD page 0xb7 length check\n  scsi: qla4xxx: Fix missing DMA mapping error in qla4xxx_alloc_pdu()\n  scsi: qla2xxx: Fix DMA mapping test in qla24xx_get_port_database()",
    "author": "Linus Torvalds",
    "date": "2025-07-03T11:52:39-07:00",
    "files_changed": [
      "drivers/infiniband/ulp/srp/ib_srp.c",
      "drivers/scsi/hosts.c",
      "drivers/scsi/qla2xxx/qla_mbx.c",
      "drivers/scsi/qla4xxx/ql4_os.c",
      "drivers/scsi/sd.c",
      "drivers/ufs/core/ufs-sysfs.c"
    ],
    "diff": "diff --git a/Documentation/ABI/testing/sysfs-driver-ufs b/Documentation/ABI/testing/sysfs-driver-ufs\nindex d4140dc6c5ba..615453fcc9ff 100644\n--- a/Documentation/ABI/testing/sysfs-driver-ufs\n+++ b/Documentation/ABI/testing/sysfs-driver-ufs\n@@ -711,7 +711,7 @@ Description:\tThis file shows the thin provisioning type. This is one of\n \n \t\tThe file is read only.\n \n-What:\t\t/sys/class/scsi_device/*/device/unit_descriptor/physical_memory_resourse_count\n+What:\t\t/sys/class/scsi_device/*/device/unit_descriptor/physical_memory_resource_count\n Date:\t\tFebruary 2018\n Contact:\tStanislav Nijnikov <stanislav.nijnikov@wdc.com>\n Description:\tThis file shows the total physical memory resources. This is\ndiff --git a/drivers/infiniband/ulp/srp/ib_srp.c b/drivers/infiniband/ulp/srp/ib_srp.c\nindex 1378651735f6..23ed2fc688f0 100644\n--- a/drivers/infiniband/ulp/srp/ib_srp.c\n+++ b/drivers/infiniband/ulp/srp/ib_srp.c\n@@ -3705,9 +3705,10 @@ static ssize_t add_target_store(struct device *dev,\n \ttarget_host->max_id      = 1;\n \ttarget_host->max_lun     = -1LL;\n \ttarget_host->max_cmd_len = sizeof ((struct srp_cmd *) (void *) 0L)->cdb;\n-\ttarget_host->max_segment_size = ib_dma_max_seg_size(ibdev);\n \n-\tif (!(ibdev->attrs.kernel_cap_flags & IBK_SG_GAPS_REG))\n+\tif (ibdev->attrs.kernel_cap_flags & IBK_SG_GAPS_REG)\n+\t\ttarget_host->max_segment_size = ib_dma_max_seg_size(ibdev);\n+\telse\n \t\ttarget_host->virt_boundary_mask = ~srp_dev->mr_page_mask;\n \n \ttarget = host_to_target(target_host);\ndiff --git a/drivers/scsi/hosts.c b/drivers/scsi/hosts.c\nindex e021f1106bea..cc5d05dc395c 100644\n--- a/drivers/scsi/hosts.c\n+++ b/drivers/scsi/hosts.c\n@@ -473,10 +473,17 @@ struct Scsi_Host *scsi_host_alloc(const struct scsi_host_template *sht, int priv\n \telse\n \t\tshost->max_sectors = SCSI_DEFAULT_MAX_SECTORS;\n \n-\tif (sht->max_segment_size)\n-\t\tshost->max_segment_size = sht->max_segment_size;\n-\telse\n-\t\tshost->max_segment_size = BLK_MAX_SEGMENT_SIZE;\n+\tshost->virt_boundary_mask = sht->virt_boundary_mask;\n+\tif (shost->virt_boundary_mask) {\n+\t\tWARN_ON_ONCE(sht->max_segment_size &&\n+\t\t\t     sht->max_segment_size != UINT_MAX);\n+\t\tshost->max_segment_size = UINT_MAX;\n+\t} else {\n+\t\tif (sht->max_segment_size)\n+\t\t\tshost->max_segment_size = sht->max_segment_size;\n+\t\telse\n+\t\t\tshost->max_segment_size = BLK_MAX_SEGMENT_SIZE;\n+\t}\n \n \t/* 32-byte (dword) is a common minimum for HBAs. */\n \tif (sht->dma_alignment)\n@@ -492,9 +499,6 @@ struct Scsi_Host *scsi_host_alloc(const struct scsi_host_template *sht, int priv\n \telse\n \t\tshost->dma_boundary = 0xffffffff;\n \n-\tif (sht->virt_boundary_mask)\n-\t\tshost->virt_boundary_mask = sht->virt_boundary_mask;\n-\n \tdevice_initialize(&shost->shost_gendev);\n \tdev_set_name(&shost->shost_gendev, \"host%d\", shost->host_no);\n \tshost->shost_gendev.bus = &scsi_bus_type;\ndiff --git a/drivers/scsi/qla2xxx/qla_mbx.c b/drivers/scsi/qla2xxx/qla_mbx.c\nindex 0cd6f3e14882..13b6cb1b93ac 100644\n--- a/drivers/scsi/qla2xxx/qla_mbx.c\n+++ b/drivers/scsi/qla2xxx/qla_mbx.c\n@@ -2147,7 +2147,7 @@ qla24xx_get_port_database(scsi_qla_host_t *vha, u16 nport_handle,\n \n \tpdb_dma = dma_map_single(&vha->hw->pdev->dev, pdb,\n \t    sizeof(*pdb), DMA_FROM_DEVICE);\n-\tif (!pdb_dma) {\n+\tif (dma_mapping_error(&vha->hw->pdev->dev, pdb_dma)) {\n \t\tql_log(ql_log_warn, vha, 0x1116, \"Failed to map dma buffer.\\n\");\n \t\treturn QLA_MEMORY_ALLOC_FAILED;\n \t}\ndiff --git a/drivers/scsi/qla4xxx/ql4_os.c b/drivers/scsi/qla4xxx/ql4_os.c\nindex d4141656b204..a39f1da4ce47 100644\n--- a/drivers/scsi/qla4xxx/ql4_os.c\n+++ b/drivers/scsi/qla4xxx/ql4_os.c\n@@ -3420,6 +3420,8 @@ static int qla4xxx_alloc_pdu(struct iscsi_task *task, uint8_t opcode)\n \t\ttask_data->data_dma = dma_map_single(&ha->pdev->dev, task->data,\n \t\t\t\t\t\t     task->data_count,\n \t\t\t\t\t\t     DMA_TO_DEVICE);\n+\t\tif (dma_mapping_error(&ha->pdev->dev, task_data->data_dma))\n+\t\t\treturn -ENOMEM;\n \t}\n \n \tDEBUG2(ql4_printk(KERN_INFO, ha, \"%s: MaxRecvLen %u, iscsi hrd %d\\n\",\ndiff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c\nindex 3f6e87705b62..eeaa6af294b8 100644\n--- a/drivers/scsi/sd.c\n+++ b/drivers/scsi/sd.c\n@@ -3384,7 +3384,7 @@ static void sd_read_block_limits_ext(struct scsi_disk *sdkp)\n \n \trcu_read_lock();\n \tvpd = rcu_dereference(sdkp->device->vpd_pgb7);\n-\tif (vpd && vpd->len >= 2)\n+\tif (vpd && vpd->len >= 6)\n \t\tsdkp->rscs = vpd->data[5] & 1;\n \trcu_read_unlock();\n }\ndiff --git a/drivers/ufs/core/ufs-sysfs.c b/drivers/ufs/core/ufs-sysfs.c\nindex de8b6acd4058..fcb4b14a710f 100644\n--- a/drivers/ufs/core/ufs-sysfs.c\n+++ b/drivers/ufs/core/ufs-sysfs.c\n@@ -1808,7 +1808,7 @@ UFS_UNIT_DESC_PARAM(logical_block_size, _LOGICAL_BLK_SIZE, 1);\n UFS_UNIT_DESC_PARAM(logical_block_count, _LOGICAL_BLK_COUNT, 8);\n UFS_UNIT_DESC_PARAM(erase_block_size, _ERASE_BLK_SIZE, 4);\n UFS_UNIT_DESC_PARAM(provisioning_type, _PROVISIONING_TYPE, 1);\n-UFS_UNIT_DESC_PARAM(physical_memory_resourse_count, _PHY_MEM_RSRC_CNT, 8);\n+UFS_UNIT_DESC_PARAM(physical_memory_resource_count, _PHY_MEM_RSRC_CNT, 8);\n UFS_UNIT_DESC_PARAM(context_capabilities, _CTX_CAPABILITIES, 2);\n UFS_UNIT_DESC_PARAM(large_unit_granularity, _LARGE_UNIT_SIZE_M1, 1);\n UFS_UNIT_DESC_PARAM(wb_buf_alloc_units, _WB_BUF_ALLOC_UNITS, 4);\n@@ -1825,7 +1825,7 @@ static struct attribute *ufs_sysfs_unit_descriptor[] = {\n \t&dev_attr_logical_block_count.attr,\n \t&dev_attr_erase_block_size.attr,\n \t&dev_attr_provisioning_type.attr,\n-\t&dev_attr_physical_memory_resourse_count.attr,\n+\t&dev_attr_physical_memory_resource_count.attr,\n \t&dev_attr_context_capabilities.attr,\n \t&dev_attr_large_unit_granularity.attr,\n \t&dev_attr_wb_buf_alloc_units.attr,",
    "stats": {
      "insertions": 21,
      "deletions": 14,
      "files": 7
    }
  },
  {
    "sha": "17bbde2e1716e2ee4b997d476b48ae85c5a47671",
    "message": "Merge tag 'net-6.16-rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net\n\nPull networking fixes from Paolo Abeni:\n \"Including fixes from Bluetooth.\n\n  Current release - new code bugs:\n\n    - eth:\n       - txgbe: fix the issue of TX failure\n       - ngbe: specify IRQ vector when the number of VFs is 7\n\n  Previous releases - regressions:\n\n    - sched: always pass notifications when child class becomes empty\n\n    - ipv4: fix stat increase when udp early demux drops the packet\n\n    - bluetooth: prevent unintended pause by checking if advertising is active\n\n    - virtio: fix error reporting in virtqueue_resize\n\n    - eth:\n       - virtio-net:\n          - ensure the received length does not exceed allocated size\n          - fix the xsk frame's length check\n       - lan78xx: fix WARN in __netif_napi_del_locked on disconnect\n\n  Previous releases - always broken:\n\n    - bluetooth: mesh: check instances prior disabling advertising\n\n    - eth:\n       - idpf: convert control queue mutex to a spinlock\n       - dpaa2: fix xdp_rxq_info leak\n       - amd-xgbe: align CL37 AN sequence as per databook\"\n\n* tag 'net-6.16-rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (38 commits)\n  vsock/vmci: Clear the vmci transport packet properly when initializing it\n  dt-bindings: net: sophgo,sg2044-dwmac: Drop status from the example\n  net: ngbe: specify IRQ vector when the number of VFs is 7\n  net: wangxun: revert the adjustment of the IRQ vector sequence\n  net: txgbe: request MISC IRQ in ndo_open\n  virtio_net: Enforce minimum TX ring size for reliability\n  virtio_net: Cleanup '2+MAX_SKB_FRAGS'\n  virtio_ring: Fix error reporting in virtqueue_resize\n  virtio-net: xsk: rx: fix the frame's length check\n  virtio-net: use the check_mergeable_len helper\n  virtio-net: remove redundant truesize check with PAGE_SIZE\n  virtio-net: ensure the received length does not exceed allocated size\n  net: ipv4: fix stat increase when udp early demux drops the packet\n  net: libwx: fix the incorrect display of the queue number\n  amd-xgbe: do not double read link status\n  net/sched: Always pass notifications when child class becomes empty\n  nui: Fix dma_mapping_error() check\n  rose: fix dangling neighbour pointers in rose_rt_device_down()\n  enic: fix incorrect MTU comparison in enic_change_mtu()\n  amd-xgbe: align CL37 AN sequence as per databook\n  ...",
    "author": "Linus Torvalds",
    "date": "2025-07-03T09:18:55-07:00",
    "files_changed": [
      "drivers/net/ethernet/amd/xgbe/xgbe-common.h",
      "drivers/net/ethernet/amd/xgbe/xgbe-mdio.c",
      "drivers/net/ethernet/amd/xgbe/xgbe-phy-v2.c",
      "drivers/net/ethernet/amd/xgbe/xgbe.h",
      "drivers/net/ethernet/atheros/atlx/atl1.c",
      "drivers/net/ethernet/cisco/enic/enic_main.c",
      "drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c",
      "drivers/net/ethernet/intel/idpf/idpf_controlq.c",
      "drivers/net/ethernet/intel/idpf/idpf_controlq_api.h",
      "drivers/net/ethernet/intel/idpf/idpf_ethtool.c",
      "drivers/net/ethernet/intel/idpf/idpf_lib.c",
      "drivers/net/ethernet/intel/igc/igc_main.c",
      "drivers/net/ethernet/sun/niu.c",
      "drivers/net/ethernet/sun/niu.h",
      "drivers/net/ethernet/wangxun/libwx/wx_lib.c",
      "drivers/net/ethernet/wangxun/libwx/wx_sriov.c",
      "drivers/net/ethernet/wangxun/libwx/wx_type.h",
      "drivers/net/ethernet/wangxun/ngbe/ngbe_main.c",
      "drivers/net/ethernet/wangxun/ngbe/ngbe_type.h",
      "drivers/net/ethernet/wangxun/txgbe/txgbe_aml.c",
      "drivers/net/ethernet/wangxun/txgbe/txgbe_irq.c",
      "drivers/net/ethernet/wangxun/txgbe/txgbe_main.c",
      "drivers/net/ethernet/wangxun/txgbe/txgbe_type.h",
      "drivers/net/usb/lan78xx.c",
      "drivers/net/virtio_net.c",
      "drivers/virtio/virtio_ring.c",
      "lib/test_objagg.c",
      "net/bluetooth/hci_event.c",
      "net/bluetooth/hci_sync.c",
      "net/bluetooth/mgmt.c",
      "net/ipv4/ip_input.c",
      "net/rose/rose_route.c",
      "net/sched/sch_api.c",
      "net/vmw_vsock/vmci_transport.c"
    ],
    "diff": "diff --git a/Documentation/devicetree/bindings/net/sophgo,sg2044-dwmac.yaml b/Documentation/devicetree/bindings/net/sophgo,sg2044-dwmac.yaml\nindex 4dd2dc9c678b..8afbd9ebd73f 100644\n--- a/Documentation/devicetree/bindings/net/sophgo,sg2044-dwmac.yaml\n+++ b/Documentation/devicetree/bindings/net/sophgo,sg2044-dwmac.yaml\n@@ -80,6 +80,8 @@ examples:\n       interrupt-parent = <&intc>;\n       interrupts = <296 IRQ_TYPE_LEVEL_HIGH>;\n       interrupt-names = \"macirq\";\n+      phy-handle = <&phy0>;\n+      phy-mode = \"rgmii-id\";\n       resets = <&rst 30>;\n       reset-names = \"stmmaceth\";\n       snps,multicast-filter-bins = <0>;\n@@ -91,7 +93,6 @@ examples:\n       snps,mtl-rx-config = <&gmac0_mtl_rx_setup>;\n       snps,mtl-tx-config = <&gmac0_mtl_tx_setup>;\n       snps,axi-config = <&gmac0_stmmac_axi_setup>;\n-      status = \"disabled\";\n \n       gmac0_mtl_rx_setup: rx-queues-config {\n         snps,rx-queues-to-use = <8>;\ndiff --git a/Documentation/networking/tls.rst b/Documentation/networking/tls.rst\nindex c7904a1bc167..36cc7afc2527 100644\n--- a/Documentation/networking/tls.rst\n+++ b/Documentation/networking/tls.rst\n@@ -16,11 +16,13 @@ User interface\n Creating a TLS connection\n -------------------------\n \n-First create a new TCP socket and set the TLS ULP.\n+First create a new TCP socket and once the connection is established set the\n+TLS ULP.\n \n .. code-block:: c\n \n   sock = socket(AF_INET, SOCK_STREAM, 0);\n+  connect(sock, addr, addrlen);\n   setsockopt(sock, SOL_TCP, TCP_ULP, \"tls\", sizeof(\"tls\"));\n \n Setting the TLS ULP allows us to set/get TLS socket options. Currently\ndiff --git a/Documentation/process/maintainer-netdev.rst b/Documentation/process/maintainer-netdev.rst\nindex 1ac62dc3a66f..e1755610b4bc 100644\n--- a/Documentation/process/maintainer-netdev.rst\n+++ b/Documentation/process/maintainer-netdev.rst\n@@ -312,7 +312,7 @@ Posting as one thread is discouraged because it confuses patchwork\n (as of patchwork 2.2.2).\n \n Co-posting selftests\n---------------------\n+~~~~~~~~~~~~~~~~~~~~\n \n Selftests should be part of the same series as the code changes.\n Specifically for fixes both code change and related test should go into\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 4bac4ea21b64..fad6cb025a19 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -15550,6 +15550,7 @@ F:\tdrivers/net/ethernet/mellanox/mlx4/en_*\n MELLANOX ETHERNET DRIVER (mlx5e)\n M:\tSaeed Mahameed <saeedm@nvidia.com>\n M:\tTariq Toukan <tariqt@nvidia.com>\n+M:\tMark Bloch <mbloch@nvidia.com>\n L:\tnetdev@vger.kernel.org\n S:\tMaintained\n W:\thttps://www.nvidia.com/networking/\n@@ -15619,6 +15620,7 @@ MELLANOX MLX5 core VPI driver\n M:\tSaeed Mahameed <saeedm@nvidia.com>\n M:\tLeon Romanovsky <leonro@nvidia.com>\n M:\tTariq Toukan <tariqt@nvidia.com>\n+M:\tMark Bloch <mbloch@nvidia.com>\n L:\tnetdev@vger.kernel.org\n L:\tlinux-rdma@vger.kernel.org\n S:\tMaintained\n@@ -21198,7 +21200,7 @@ M:\tLad Prabhakar <prabhakar.mahadev-lad.rj@bp.renesas.com>\n L:\tnetdev@vger.kernel.org\n L:\tlinux-renesas-soc@vger.kernel.org\n S:\tMaintained\n-F:\tDocumentation/devicetree/bindings/net/renesas,r9a09g057-gbeth.yaml\n+F:\tDocumentation/devicetree/bindings/net/renesas,rzv2h-gbeth.yaml\n F:\tdrivers/net/ethernet/stmicro/stmmac/dwmac-renesas-gbeth.c\n \n RENESAS RZ/V2H(P) USB2PHY PORT RESET DRIVER\n@@ -22586,9 +22588,11 @@ S:\tMaintained\n F:\tdrivers/misc/sgi-xp/\n \n SHARED MEMORY COMMUNICATIONS (SMC) SOCKETS\n+M:\tD. Wythe <alibuda@linux.alibaba.com>\n+M:\tDust Li <dust.li@linux.alibaba.com>\n+M:\tSidraya Jayagond <sidraya@linux.ibm.com>\n M:\tWenjia Zhang <wenjia@linux.ibm.com>\n-M:\tJan Karcher <jaka@linux.ibm.com>\n-R:\tD. Wythe <alibuda@linux.alibaba.com>\n+R:\tMahanta Jambigi <mjambigi@linux.ibm.com>\n R:\tTony Lu <tonylu@linux.alibaba.com>\n R:\tWen Gu <guwen@linux.alibaba.com>\n L:\tlinux-rdma@vger.kernel.org\ndiff --git a/drivers/net/ethernet/amd/xgbe/xgbe-common.h b/drivers/net/ethernet/amd/xgbe/xgbe-common.h\nindex e1296cbf4ff3..9316de4126cf 100644\n--- a/drivers/net/ethernet/amd/xgbe/xgbe-common.h\n+++ b/drivers/net/ethernet/amd/xgbe/xgbe-common.h\n@@ -1269,6 +1269,8 @@\n #define MDIO_VEND2_CTRL1_SS13\t\tBIT(13)\n #endif\n \n+#define XGBE_VEND2_MAC_AUTO_SW\t\tBIT(9)\n+\n /* MDIO mask values */\n #define XGBE_AN_CL73_INT_CMPLT\t\tBIT(0)\n #define XGBE_AN_CL73_INC_LINK\t\tBIT(1)\ndiff --git a/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c b/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c\nindex 71449edbb76d..1a37ec45e650 100644\n--- a/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c\n+++ b/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c\n@@ -266,6 +266,10 @@ static void xgbe_an37_set(struct xgbe_prv_data *pdata, bool enable,\n \t\treg |= MDIO_VEND2_CTRL1_AN_RESTART;\n \n \tXMDIO_WRITE(pdata, MDIO_MMD_VEND2, MDIO_CTRL1, reg);\n+\n+\treg = XMDIO_READ(pdata, MDIO_MMD_VEND2, MDIO_PCS_DIG_CTRL);\n+\treg |= XGBE_VEND2_MAC_AUTO_SW;\n+\tXMDIO_WRITE(pdata, MDIO_MMD_VEND2, MDIO_PCS_DIG_CTRL, reg);\n }\n \n static void xgbe_an37_restart(struct xgbe_prv_data *pdata)\n@@ -894,6 +898,11 @@ static void xgbe_an37_init(struct xgbe_prv_data *pdata)\n \n \tnetif_dbg(pdata, link, pdata->netdev, \"CL37 AN (%s) initialized\\n\",\n \t\t  (pdata->an_mode == XGBE_AN_MODE_CL37) ? \"BaseX\" : \"SGMII\");\n+\n+\treg = XMDIO_READ(pdata, MDIO_MMD_AN, MDIO_CTRL1);\n+\treg &= ~MDIO_AN_CTRL1_ENABLE;\n+\tXMDIO_WRITE(pdata, MDIO_MMD_AN, MDIO_CTRL1, reg);\n+\n }\n \n static void xgbe_an73_init(struct xgbe_prv_data *pdata)\n@@ -1295,6 +1304,10 @@ static void xgbe_phy_status(struct xgbe_prv_data *pdata)\n \n \tpdata->phy.link = pdata->phy_if.phy_impl.link_status(pdata,\n \t\t\t\t\t\t\t     &an_restart);\n+\t/* bail out if the link status register read fails */\n+\tif (pdata->phy.link < 0)\n+\t\treturn;\n+\n \tif (an_restart) {\n \t\txgbe_phy_config_aneg(pdata);\n \t\tgoto adjust_link;\ndiff --git a/drivers/net/ethernet/amd/xgbe/xgbe-phy-v2.c b/drivers/net/ethernet/amd/xgbe/xgbe-phy-v2.c\nindex 7a4dfa4e19c7..23c39e92e783 100644\n--- a/drivers/net/ethernet/amd/xgbe/xgbe-phy-v2.c\n+++ b/drivers/net/ethernet/amd/xgbe/xgbe-phy-v2.c\n@@ -2746,8 +2746,7 @@ static bool xgbe_phy_valid_speed(struct xgbe_prv_data *pdata, int speed)\n static int xgbe_phy_link_status(struct xgbe_prv_data *pdata, int *an_restart)\n {\n \tstruct xgbe_phy_data *phy_data = pdata->phy_data;\n-\tunsigned int reg;\n-\tint ret;\n+\tint reg, ret;\n \n \t*an_restart = 0;\n \n@@ -2781,11 +2780,20 @@ static int xgbe_phy_link_status(struct xgbe_prv_data *pdata, int *an_restart)\n \t\t\treturn 0;\n \t}\n \n-\t/* Link status is latched low, so read once to clear\n-\t * and then read again to get current state\n-\t */\n-\treg = XMDIO_READ(pdata, MDIO_MMD_PCS, MDIO_STAT1);\n \treg = XMDIO_READ(pdata, MDIO_MMD_PCS, MDIO_STAT1);\n+\tif (reg < 0)\n+\t\treturn reg;\n+\n+\t/* Link status is latched low so that momentary link drops\n+\t * can be detected. If link was already down read again\n+\t * to get the latest state.\n+\t */\n+\n+\tif (!pdata->phy.link && !(reg & MDIO_STAT1_LSTATUS)) {\n+\t\treg = XMDIO_READ(pdata, MDIO_MMD_PCS, MDIO_STAT1);\n+\t\tif (reg < 0)\n+\t\t\treturn reg;\n+\t}\n \n \tif (pdata->en_rx_adap) {\n \t\t/* if the link is available and adaptation is done,\n@@ -2804,9 +2812,7 @@ static int xgbe_phy_link_status(struct xgbe_prv_data *pdata, int *an_restart)\n \t\t\txgbe_phy_set_mode(pdata, phy_data->cur_mode);\n \t\t}\n \n-\t\t/* check again for the link and adaptation status */\n-\t\treg = XMDIO_READ(pdata, MDIO_MMD_PCS, MDIO_STAT1);\n-\t\tif ((reg & MDIO_STAT1_LSTATUS) && pdata->rx_adapt_done)\n+\t\tif (pdata->rx_adapt_done)\n \t\t\treturn 1;\n \t} else if (reg & MDIO_STAT1_LSTATUS)\n \t\treturn 1;\ndiff --git a/drivers/net/ethernet/amd/xgbe/xgbe.h b/drivers/net/ethernet/amd/xgbe/xgbe.h\nindex 6359bb87dc13..057379cd43ba 100644\n--- a/drivers/net/ethernet/amd/xgbe/xgbe.h\n+++ b/drivers/net/ethernet/amd/xgbe/xgbe.h\n@@ -183,12 +183,12 @@\n #define XGBE_LINK_TIMEOUT\t\t5\n #define XGBE_KR_TRAINING_WAIT_ITER\t50\n \n-#define XGBE_SGMII_AN_LINK_STATUS\tBIT(1)\n+#define XGBE_SGMII_AN_LINK_DUPLEX\tBIT(1)\n #define XGBE_SGMII_AN_LINK_SPEED\t(BIT(2) | BIT(3))\n #define XGBE_SGMII_AN_LINK_SPEED_10\t0x00\n #define XGBE_SGMII_AN_LINK_SPEED_100\t0x04\n #define XGBE_SGMII_AN_LINK_SPEED_1000\t0x08\n-#define XGBE_SGMII_AN_LINK_DUPLEX\tBIT(4)\n+#define XGBE_SGMII_AN_LINK_STATUS\tBIT(4)\n \n /* ECC correctable error notification window (seconds) */\n #define XGBE_ECC_LIMIT\t\t\t60\ndiff --git a/drivers/net/ethernet/atheros/atlx/atl1.c b/drivers/net/ethernet/atheros/atlx/atl1.c\nindex cfdb546a09e7..98a4d089270e 100644\n--- a/drivers/net/ethernet/atheros/atlx/atl1.c\n+++ b/drivers/net/ethernet/atheros/atlx/atl1.c\n@@ -1861,14 +1861,21 @@ static u16 atl1_alloc_rx_buffers(struct atl1_adapter *adapter)\n \t\t\tbreak;\n \t\t}\n \n-\t\tbuffer_info->alloced = 1;\n-\t\tbuffer_info->skb = skb;\n-\t\tbuffer_info->length = (u16) adapter->rx_buffer_len;\n \t\tpage = virt_to_page(skb->data);\n \t\toffset = offset_in_page(skb->data);\n \t\tbuffer_info->dma = dma_map_page(&pdev->dev, page, offset,\n \t\t\t\t\t\tadapter->rx_buffer_len,\n \t\t\t\t\t\tDMA_FROM_DEVICE);\n+\t\tif (dma_mapping_error(&pdev->dev, buffer_info->dma)) {\n+\t\t\tkfree_skb(skb);\n+\t\t\tadapter->soft_stats.rx_dropped++;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tbuffer_info->alloced = 1;\n+\t\tbuffer_info->skb = skb;\n+\t\tbuffer_info->length = (u16)adapter->rx_buffer_len;\n+\n \t\trfd_desc->buffer_addr = cpu_to_le64(buffer_info->dma);\n \t\trfd_desc->buf_len = cpu_to_le16(adapter->rx_buffer_len);\n \t\trfd_desc->coalese = 0;\n@@ -2183,8 +2190,8 @@ static int atl1_tx_csum(struct atl1_adapter *adapter, struct sk_buff *skb,\n \treturn 0;\n }\n \n-static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n-\tstruct tx_packet_desc *ptpd)\n+static bool atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n+\t\t\tstruct tx_packet_desc *ptpd)\n {\n \tstruct atl1_tpd_ring *tpd_ring = &adapter->tpd_ring;\n \tstruct atl1_buffer *buffer_info;\n@@ -2194,6 +2201,7 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \tunsigned int nr_frags;\n \tunsigned int f;\n \tint retval;\n+\tu16 first_mapped;\n \tu16 next_to_use;\n \tu16 data_len;\n \tu8 hdr_len;\n@@ -2201,6 +2209,7 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \tbuf_len -= skb->data_len;\n \tnr_frags = skb_shinfo(skb)->nr_frags;\n \tnext_to_use = atomic_read(&tpd_ring->next_to_use);\n+\tfirst_mapped = next_to_use;\n \tbuffer_info = &tpd_ring->buffer_info[next_to_use];\n \tBUG_ON(buffer_info->skb);\n \t/* put skb in last TPD */\n@@ -2216,6 +2225,8 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \t\tbuffer_info->dma = dma_map_page(&adapter->pdev->dev, page,\n \t\t\t\t\t\toffset, hdr_len,\n \t\t\t\t\t\tDMA_TO_DEVICE);\n+\t\tif (dma_mapping_error(&adapter->pdev->dev, buffer_info->dma))\n+\t\t\tgoto dma_err;\n \n \t\tif (++next_to_use == tpd_ring->count)\n \t\t\tnext_to_use = 0;\n@@ -2242,6 +2253,9 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \t\t\t\t\t\t\t\tpage, offset,\n \t\t\t\t\t\t\t\tbuffer_info->length,\n \t\t\t\t\t\t\t\tDMA_TO_DEVICE);\n+\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n+\t\t\t\t\t\t      buffer_info->dma))\n+\t\t\t\t\tgoto dma_err;\n \t\t\t\tif (++next_to_use == tpd_ring->count)\n \t\t\t\t\tnext_to_use = 0;\n \t\t\t}\n@@ -2254,6 +2268,8 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \t\tbuffer_info->dma = dma_map_page(&adapter->pdev->dev, page,\n \t\t\t\t\t\toffset, buf_len,\n \t\t\t\t\t\tDMA_TO_DEVICE);\n+\t\tif (dma_mapping_error(&adapter->pdev->dev, buffer_info->dma))\n+\t\t\tgoto dma_err;\n \t\tif (++next_to_use == tpd_ring->count)\n \t\t\tnext_to_use = 0;\n \t}\n@@ -2277,6 +2293,9 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \t\t\tbuffer_info->dma = skb_frag_dma_map(&adapter->pdev->dev,\n \t\t\t\tfrag, i * ATL1_MAX_TX_BUF_LEN,\n \t\t\t\tbuffer_info->length, DMA_TO_DEVICE);\n+\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n+\t\t\t\t\t      buffer_info->dma))\n+\t\t\t\tgoto dma_err;\n \n \t\t\tif (++next_to_use == tpd_ring->count)\n \t\t\t\tnext_to_use = 0;\n@@ -2285,6 +2304,22 @@ static void atl1_tx_map(struct atl1_adapter *adapter, struct sk_buff *skb,\n \n \t/* last tpd's buffer-info */\n \tbuffer_info->skb = skb;\n+\n+\treturn true;\n+\n+ dma_err:\n+\twhile (first_mapped != next_to_use) {\n+\t\tbuffer_info = &tpd_ring->buffer_info[first_mapped];\n+\t\tdma_unmap_page(&adapter->pdev->dev,\n+\t\t\t       buffer_info->dma,\n+\t\t\t       buffer_info->length,\n+\t\t\t       DMA_TO_DEVICE);\n+\t\tbuffer_info->dma = 0;\n+\n+\t\tif (++first_mapped == tpd_ring->count)\n+\t\t\tfirst_mapped = 0;\n+\t}\n+\treturn false;\n }\n \n static void atl1_tx_queue(struct atl1_adapter *adapter, u16 count,\n@@ -2355,10 +2390,8 @@ static netdev_tx_t atl1_xmit_frame(struct sk_buff *skb,\n \n \tlen = skb_headlen(skb);\n \n-\tif (unlikely(skb->len <= 0)) {\n-\t\tdev_kfree_skb_any(skb);\n-\t\treturn NETDEV_TX_OK;\n-\t}\n+\tif (unlikely(skb->len <= 0))\n+\t\tgoto drop_packet;\n \n \tnr_frags = skb_shinfo(skb)->nr_frags;\n \tfor (f = 0; f < nr_frags; f++) {\n@@ -2371,10 +2404,9 @@ static netdev_tx_t atl1_xmit_frame(struct sk_buff *skb,\n \tif (mss) {\n \t\tif (skb->protocol == htons(ETH_P_IP)) {\n \t\t\tproto_hdr_len = skb_tcp_all_headers(skb);\n-\t\t\tif (unlikely(proto_hdr_len > len)) {\n-\t\t\t\tdev_kfree_skb_any(skb);\n-\t\t\t\treturn NETDEV_TX_OK;\n-\t\t\t}\n+\t\t\tif (unlikely(proto_hdr_len > len))\n+\t\t\t\tgoto drop_packet;\n+\n \t\t\t/* need additional TPD ? */\n \t\t\tif (proto_hdr_len != len)\n \t\t\t\tcount += (len - proto_hdr_len +\n@@ -2406,23 +2438,26 @@ static netdev_tx_t atl1_xmit_frame(struct sk_buff *skb,\n \t}\n \n \ttso = atl1_tso(adapter, skb, ptpd);\n-\tif (tso < 0) {\n-\t\tdev_kfree_skb_any(skb);\n-\t\treturn NETDEV_TX_OK;\n-\t}\n+\tif (tso < 0)\n+\t\tgoto drop_packet;\n \n \tif (!tso) {\n \t\tret_val = atl1_tx_csum(adapter, skb, ptpd);\n-\t\tif (ret_val < 0) {\n-\t\t\tdev_kfree_skb_any(skb);\n-\t\t\treturn NETDEV_TX_OK;\n-\t\t}\n+\t\tif (ret_val < 0)\n+\t\t\tgoto drop_packet;\n \t}\n \n-\tatl1_tx_map(adapter, skb, ptpd);\n+\tif (!atl1_tx_map(adapter, skb, ptpd))\n+\t\tgoto drop_packet;\n+\n \tatl1_tx_queue(adapter, count, ptpd);\n \tatl1_update_mailbox(adapter);\n \treturn NETDEV_TX_OK;\n+\n+drop_packet:\n+\tadapter->soft_stats.tx_errors++;\n+\tdev_kfree_skb_any(skb);\n+\treturn NETDEV_TX_OK;\n }\n \n static int atl1_rings_clean(struct napi_struct *napi, int budget)\ndiff --git a/drivers/net/ethernet/cisco/enic/enic_main.c b/drivers/net/ethernet/cisco/enic/enic_main.c\nindex 773f5ad972a2..6bc8dfdb3d4b 100644\n--- a/drivers/net/ethernet/cisco/enic/enic_main.c\n+++ b/drivers/net/ethernet/cisco/enic/enic_main.c\n@@ -1864,10 +1864,10 @@ static int enic_change_mtu(struct net_device *netdev, int new_mtu)\n \tif (enic_is_dynamic(enic) || enic_is_sriov_vf(enic))\n \t\treturn -EOPNOTSUPP;\n \n-\tif (netdev->mtu > enic->port_mtu)\n+\tif (new_mtu > enic->port_mtu)\n \t\tnetdev_warn(netdev,\n \t\t\t    \"interface MTU (%d) set higher than port MTU (%d)\\n\",\n-\t\t\t    netdev->mtu, enic->port_mtu);\n+\t\t\t    new_mtu, enic->port_mtu);\n \n \treturn _enic_change_mtu(netdev, new_mtu);\n }\ndiff --git a/drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c b/drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c\nindex 2ec2c3dab250..b82f121cadad 100644\n--- a/drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c\n+++ b/drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c\n@@ -3939,6 +3939,7 @@ static int dpaa2_eth_setup_rx_flow(struct dpaa2_eth_priv *priv,\n \t\t\t\t\t MEM_TYPE_PAGE_ORDER0, NULL);\n \tif (err) {\n \t\tdev_err(dev, \"xdp_rxq_info_reg_mem_model failed\\n\");\n+\t\txdp_rxq_info_unreg(&fq->channel->xdp_rxq);\n \t\treturn err;\n \t}\n \n@@ -4432,17 +4433,25 @@ static int dpaa2_eth_bind_dpni(struct dpaa2_eth_priv *priv)\n \t\t\treturn -EINVAL;\n \t\t}\n \t\tif (err)\n-\t\t\treturn err;\n+\t\t\tgoto out;\n \t}\n \n \terr = dpni_get_qdid(priv->mc_io, 0, priv->mc_token,\n \t\t\t    DPNI_QUEUE_TX, &priv->tx_qdid);\n \tif (err) {\n \t\tdev_err(dev, \"dpni_get_qdid() failed\\n\");\n-\t\treturn err;\n+\t\tgoto out;\n \t}\n \n \treturn 0;\n+\n+out:\n+\twhile (i--) {\n+\t\tif (priv->fq[i].type == DPAA2_RX_FQ &&\n+\t\t    xdp_rxq_info_is_reg(&priv->fq[i].channel->xdp_rxq))\n+\t\t\txdp_rxq_info_unreg(&priv->fq[i].channel->xdp_rxq);\n+\t}\n+\treturn err;\n }\n \n /* Allocate rings for storing incoming frame descriptors */\n@@ -4825,6 +4834,17 @@ static void dpaa2_eth_del_ch_napi(struct dpaa2_eth_priv *priv)\n \t}\n }\n \n+static void dpaa2_eth_free_rx_xdp_rxq(struct dpaa2_eth_priv *priv)\n+{\n+\tint i;\n+\n+\tfor (i = 0; i < priv->num_fqs; i++) {\n+\t\tif (priv->fq[i].type == DPAA2_RX_FQ &&\n+\t\t    xdp_rxq_info_is_reg(&priv->fq[i].channel->xdp_rxq))\n+\t\t\txdp_rxq_info_unreg(&priv->fq[i].channel->xdp_rxq);\n+\t}\n+}\n+\n static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)\n {\n \tstruct device *dev;\n@@ -5028,6 +5048,7 @@ static int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)\n \tfree_percpu(priv->percpu_stats);\n err_alloc_percpu_stats:\n \tdpaa2_eth_del_ch_napi(priv);\n+\tdpaa2_eth_free_rx_xdp_rxq(priv);\n err_bind:\n \tdpaa2_eth_free_dpbps(priv);\n err_dpbp_setup:\n@@ -5080,6 +5101,7 @@ static void dpaa2_eth_remove(struct fsl_mc_device *ls_dev)\n \tfree_percpu(priv->percpu_extras);\n \n \tdpaa2_eth_del_ch_napi(priv);\n+\tdpaa2_eth_free_rx_xdp_rxq(priv);\n \tdpaa2_eth_free_dpbps(priv);\n \tdpaa2_eth_free_dpio(priv);\n \tdpaa2_eth_free_dpni(priv);\ndiff --git a/drivers/net/ethernet/intel/idpf/idpf_controlq.c b/drivers/net/ethernet/intel/idpf/idpf_controlq.c\nindex b28991dd1870..48b8e184f3db 100644\n--- a/drivers/net/ethernet/intel/idpf/idpf_controlq.c\n+++ b/drivers/net/ethernet/intel/idpf/idpf_controlq.c\n@@ -96,7 +96,7 @@ static void idpf_ctlq_init_rxq_bufs(struct idpf_ctlq_info *cq)\n  */\n static void idpf_ctlq_shutdown(struct idpf_hw *hw, struct idpf_ctlq_info *cq)\n {\n-\tmutex_lock(&cq->cq_lock);\n+\tspin_lock(&cq->cq_lock);\n \n \t/* free ring buffers and the ring itself */\n \tidpf_ctlq_dealloc_ring_res(hw, cq);\n@@ -104,8 +104,7 @@ static void idpf_ctlq_shutdown(struct idpf_hw *hw, struct idpf_ctlq_info *cq)\n \t/* Set ring_size to 0 to indicate uninitialized queue */\n \tcq->ring_size = 0;\n \n-\tmutex_unlock(&cq->cq_lock);\n-\tmutex_destroy(&cq->cq_lock);\n+\tspin_unlock(&cq->cq_lock);\n }\n \n /**\n@@ -173,7 +172,7 @@ int idpf_ctlq_add(struct idpf_hw *hw,\n \n \tidpf_ctlq_init_regs(hw, cq, is_rxq);\n \n-\tmutex_init(&cq->cq_lock);\n+\tspin_lock_init(&cq->cq_lock);\n \n \tlist_add(&cq->cq_list, &hw->cq_list_head);\n \n@@ -272,7 +271,7 @@ int idpf_ctlq_send(struct idpf_hw *hw, struct idpf_ctlq_info *cq,\n \tint err = 0;\n \tint i;\n \n-\tmutex_lock(&cq->cq_lock);\n+\tspin_lock(&cq->cq_lock);\n \n \t/* Ensure there are enough descriptors to send all messages */\n \tnum_desc_avail = IDPF_CTLQ_DESC_UNUSED(cq);\n@@ -332,7 +331,7 @@ int idpf_ctlq_send(struct idpf_hw *hw, struct idpf_ctlq_info *cq,\n \twr32(hw, cq->reg.tail, cq->next_to_use);\n \n err_unlock:\n-\tmutex_unlock(&cq->cq_lock);\n+\tspin_unlock(&cq->cq_lock);\n \n \treturn err;\n }\n@@ -364,7 +363,7 @@ int idpf_ctlq_clean_sq(struct idpf_ctlq_info *cq, u16 *clean_count,\n \tif (*clean_count > cq->ring_size)\n \t\treturn -EBADR;\n \n-\tmutex_lock(&cq->cq_lock);\n+\tspin_lock(&cq->cq_lock);\n \n \tntc = cq->next_to_clean;\n \n@@ -397,7 +396,7 @@ int idpf_ctlq_clean_sq(struct idpf_ctlq_info *cq, u16 *clean_count,\n \n \tcq->next_to_clean = ntc;\n \n-\tmutex_unlock(&cq->cq_lock);\n+\tspin_unlock(&cq->cq_lock);\n \n \t/* Return number of descriptors actually cleaned */\n \t*clean_count = i;\n@@ -435,7 +434,7 @@ int idpf_ctlq_post_rx_buffs(struct idpf_hw *hw, struct idpf_ctlq_info *cq,\n \tif (*buff_count > 0)\n \t\tbuffs_avail = true;\n \n-\tmutex_lock(&cq->cq_lock);\n+\tspin_lock(&cq->cq_lock);\n \n \tif (tbp >= cq->ring_size)\n \t\ttbp = 0;\n@@ -524,7 +523,7 @@ int idpf_ctlq_post_rx_buffs(struct idpf_hw *hw, struct idpf_ctlq_info *cq,\n \t\twr32(hw, cq->reg.tail, cq->next_to_post);\n \t}\n \n-\tmutex_unlock(&cq->cq_lock);\n+\tspin_unlock(&cq->cq_lock);\n \n \t/* return the number of buffers that were not posted */\n \t*buff_count = *buff_count - i;\n@@ -552,7 +551,7 @@ int idpf_ctlq_recv(struct idpf_ctlq_info *cq, u16 *num_q_msg,\n \tu16 i;\n \n \t/* take the lock before we start messing with the ring */\n-\tmutex_lock(&cq->cq_lock);\n+\tspin_lock(&cq->cq_lock);\n \n \tntc = cq->next_to_clean;\n \n@@ -614,7 +613,7 @@ int idpf_ctlq_recv(struct idpf_ctlq_info *cq, u16 *num_q_msg,\n \n \tcq->next_to_clean = ntc;\n \n-\tmutex_unlock(&cq->cq_lock);\n+\tspin_unlock(&cq->cq_lock);\n \n \t*num_q_msg = i;\n \tif (*num_q_msg == 0)\ndiff --git a/drivers/net/ethernet/intel/idpf/idpf_controlq_api.h b/drivers/net/ethernet/intel/idpf/idpf_controlq_api.h\nindex 9642494a67d8..3414c5f9a831 100644\n--- a/drivers/net/ethernet/intel/idpf/idpf_controlq_api.h\n+++ b/drivers/net/ethernet/intel/idpf/idpf_controlq_api.h\n@@ -99,7 +99,7 @@ struct idpf_ctlq_info {\n \n \tenum idpf_ctlq_type cq_type;\n \tint q_id;\n-\tstruct mutex cq_lock;\t\t/* control queue lock */\n+\tspinlock_t cq_lock;\t\t/* control queue lock */\n \t/* used for interrupt processing */\n \tu16 next_to_use;\n \tu16 next_to_clean;\ndiff --git a/drivers/net/ethernet/intel/idpf/idpf_ethtool.c b/drivers/net/ethernet/intel/idpf/idpf_ethtool.c\nindex 9bdb309b668e..eaf7a2606faa 100644\n--- a/drivers/net/ethernet/intel/idpf/idpf_ethtool.c\n+++ b/drivers/net/ethernet/intel/idpf/idpf_ethtool.c\n@@ -47,7 +47,7 @@ static u32 idpf_get_rxfh_key_size(struct net_device *netdev)\n \tstruct idpf_vport_user_config_data *user_config;\n \n \tif (!idpf_is_cap_ena_all(np->adapter, IDPF_RSS_CAPS, IDPF_CAP_RSS))\n-\t\treturn -EOPNOTSUPP;\n+\t\treturn 0;\n \n \tuser_config = &np->adapter->vport_config[np->vport_idx]->user_config;\n \n@@ -66,7 +66,7 @@ static u32 idpf_get_rxfh_indir_size(struct net_device *netdev)\n \tstruct idpf_vport_user_config_data *user_config;\n \n \tif (!idpf_is_cap_ena_all(np->adapter, IDPF_RSS_CAPS, IDPF_CAP_RSS))\n-\t\treturn -EOPNOTSUPP;\n+\t\treturn 0;\n \n \tuser_config = &np->adapter->vport_config[np->vport_idx]->user_config;\n \ndiff --git a/drivers/net/ethernet/intel/idpf/idpf_lib.c b/drivers/net/ethernet/intel/idpf/idpf_lib.c\nindex 4eb20ec2accb..80382ff4a5fa 100644\n--- a/drivers/net/ethernet/intel/idpf/idpf_lib.c\n+++ b/drivers/net/ethernet/intel/idpf/idpf_lib.c\n@@ -2314,8 +2314,12 @@ void *idpf_alloc_dma_mem(struct idpf_hw *hw, struct idpf_dma_mem *mem, u64 size)\n \tstruct idpf_adapter *adapter = hw->back;\n \tsize_t sz = ALIGN(size, 4096);\n \n-\tmem->va = dma_alloc_coherent(&adapter->pdev->dev, sz,\n-\t\t\t\t     &mem->pa, GFP_KERNEL);\n+\t/* The control queue resources are freed under a spinlock, contiguous\n+\t * pages will avoid IOMMU remapping and the use vmap (and vunmap in\n+\t * dma_free_*() path.\n+\t */\n+\tmem->va = dma_alloc_attrs(&adapter->pdev->dev, sz, &mem->pa,\n+\t\t\t\t  GFP_KERNEL, DMA_ATTR_FORCE_CONTIGUOUS);\n \tmem->size = sz;\n \n \treturn mem->va;\n@@ -2330,8 +2334,8 @@ void idpf_free_dma_mem(struct idpf_hw *hw, struct idpf_dma_mem *mem)\n {\n \tstruct idpf_adapter *adapter = hw->back;\n \n-\tdma_free_coherent(&adapter->pdev->dev, mem->size,\n-\t\t\t  mem->va, mem->pa);\n+\tdma_free_attrs(&adapter->pdev->dev, mem->size,\n+\t\t       mem->va, mem->pa, DMA_ATTR_FORCE_CONTIGUOUS);\n \tmem->size = 0;\n \tmem->va = NULL;\n \tmem->pa = 0;\ndiff --git a/drivers/net/ethernet/intel/igc/igc_main.c b/drivers/net/ethernet/intel/igc/igc_main.c\nindex 686793c539f2..031c332f66c4 100644\n--- a/drivers/net/ethernet/intel/igc/igc_main.c\n+++ b/drivers/net/ethernet/intel/igc/igc_main.c\n@@ -7115,6 +7115,10 @@ static int igc_probe(struct pci_dev *pdev,\n \tadapter->port_num = hw->bus.func;\n \tadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n \n+\t/* Disable ASPM L1.2 on I226 devices to avoid packet loss */\n+\tif (igc_is_device_id_i226(hw))\n+\t\tpci_disable_link_state(pdev, PCIE_LINK_STATE_L1_2);\n+\n \terr = pci_save_state(pdev);\n \tif (err)\n \t\tgoto err_ioremap;\n@@ -7500,6 +7504,9 @@ static int __igc_resume(struct device *dev, bool rpm)\n \tpci_enable_wake(pdev, PCI_D3hot, 0);\n \tpci_enable_wake(pdev, PCI_D3cold, 0);\n \n+\tif (igc_is_device_id_i226(hw))\n+\t\tpci_disable_link_state(pdev, PCIE_LINK_STATE_L1_2);\n+\n \tif (igc_init_interrupt_scheme(adapter, true)) {\n \t\tnetdev_err(netdev, \"Unable to allocate memory for queues\\n\");\n \t\treturn -ENOMEM;\n@@ -7625,6 +7632,9 @@ static pci_ers_result_t igc_io_slot_reset(struct pci_dev *pdev)\n \t\tpci_enable_wake(pdev, PCI_D3hot, 0);\n \t\tpci_enable_wake(pdev, PCI_D3cold, 0);\n \n+\t\tif (igc_is_device_id_i226(hw))\n+\t\t\tpci_disable_link_state_locked(pdev, PCIE_LINK_STATE_L1_2);\n+\n \t\t/* In case of PCI error, adapter loses its HW address\n \t\t * so we should re-assign it here.\n \t\t */\ndiff --git a/drivers/net/ethernet/sun/niu.c b/drivers/net/ethernet/sun/niu.c\nindex ddca8fc7883e..26119d02a94d 100644\n--- a/drivers/net/ethernet/sun/niu.c\n+++ b/drivers/net/ethernet/sun/niu.c\n@@ -3336,7 +3336,7 @@ static int niu_rbr_add_page(struct niu *np, struct rx_ring_info *rp,\n \n \taddr = np->ops->map_page(np->device, page, 0,\n \t\t\t\t PAGE_SIZE, DMA_FROM_DEVICE);\n-\tif (!addr) {\n+\tif (np->ops->mapping_error(np->device, addr)) {\n \t\t__free_page(page);\n \t\treturn -ENOMEM;\n \t}\n@@ -6676,6 +6676,8 @@ static netdev_tx_t niu_start_xmit(struct sk_buff *skb,\n \tlen = skb_headlen(skb);\n \tmapping = np->ops->map_single(np->device, skb->data,\n \t\t\t\t      len, DMA_TO_DEVICE);\n+\tif (np->ops->mapping_error(np->device, mapping))\n+\t\tgoto out_drop;\n \n \tprod = rp->prod;\n \n@@ -6717,6 +6719,8 @@ static netdev_tx_t niu_start_xmit(struct sk_buff *skb,\n \t\tmapping = np->ops->map_page(np->device, skb_frag_page(frag),\n \t\t\t\t\t    skb_frag_off(frag), len,\n \t\t\t\t\t    DMA_TO_DEVICE);\n+\t\tif (np->ops->mapping_error(np->device, mapping))\n+\t\t\tgoto out_unmap;\n \n \t\trp->tx_buffs[prod].skb = NULL;\n \t\trp->tx_buffs[prod].mapping = mapping;\n@@ -6741,6 +6745,19 @@ static netdev_tx_t niu_start_xmit(struct sk_buff *skb,\n out:\n \treturn NETDEV_TX_OK;\n \n+out_unmap:\n+\twhile (i--) {\n+\t\tconst skb_frag_t *frag;\n+\n+\t\tprod = PREVIOUS_TX(rp, prod);\n+\t\tfrag = &skb_shinfo(skb)->frags[i];\n+\t\tnp->ops->unmap_page(np->device, rp->tx_buffs[prod].mapping,\n+\t\t\t\t    skb_frag_size(frag), DMA_TO_DEVICE);\n+\t}\n+\n+\tnp->ops->unmap_single(np->device, rp->tx_buffs[rp->prod].mapping,\n+\t\t\t      skb_headlen(skb), DMA_TO_DEVICE);\n+\n out_drop:\n \trp->tx_errors++;\n \tkfree_skb(skb);\n@@ -9644,6 +9661,11 @@ static void niu_pci_unmap_single(struct device *dev, u64 dma_address,\n \tdma_unmap_single(dev, dma_address, size, direction);\n }\n \n+static int niu_pci_mapping_error(struct device *dev, u64 addr)\n+{\n+\treturn dma_mapping_error(dev, addr);\n+}\n+\n static const struct niu_ops niu_pci_ops = {\n \t.alloc_coherent\t= niu_pci_alloc_coherent,\n \t.free_coherent\t= niu_pci_free_coherent,\n@@ -9651,6 +9673,7 @@ static const struct niu_ops niu_pci_ops = {\n \t.unmap_page\t= niu_pci_unmap_page,\n \t.map_single\t= niu_pci_map_single,\n \t.unmap_single\t= niu_pci_unmap_single,\n+\t.mapping_error\t= niu_pci_mapping_error,\n };\n \n static void niu_driver_version(void)\n@@ -10019,6 +10042,11 @@ static void niu_phys_unmap_single(struct device *dev, u64 dma_address,\n \t/* Nothing to do.  */\n }\n \n+static int niu_phys_mapping_error(struct device *dev, u64 dma_address)\n+{\n+\treturn false;\n+}\n+\n static const struct niu_ops niu_phys_ops = {\n \t.alloc_coherent\t= niu_phys_alloc_coherent,\n \t.free_coherent\t= niu_phys_free_coherent,\n@@ -10026,6 +10054,7 @@ static const struct niu_ops niu_phys_ops = {\n \t.unmap_page\t= niu_phys_unmap_page,\n \t.map_single\t= niu_phys_map_single,\n \t.unmap_single\t= niu_phys_unmap_single,\n+\t.mapping_error\t= niu_phys_mapping_error,\n };\n \n static int niu_of_probe(struct platform_device *op)\ndiff --git a/drivers/net/ethernet/sun/niu.h b/drivers/net/ethernet/sun/niu.h\nindex 04c215f91fc0..0b169c08b0f2 100644\n--- a/drivers/net/ethernet/sun/niu.h\n+++ b/drivers/net/ethernet/sun/niu.h\n@@ -2879,6 +2879,9 @@ struct tx_ring_info {\n #define NEXT_TX(tp, index) \\\n \t(((index) + 1) < (tp)->pending ? ((index) + 1) : 0)\n \n+#define PREVIOUS_TX(tp, index) \\\n+\t(((index) - 1) >= 0 ? ((index) - 1) : (((tp)->pending) - 1))\n+\n static inline u32 niu_tx_avail(struct tx_ring_info *tp)\n {\n \treturn (tp->pending -\n@@ -3140,6 +3143,7 @@ struct niu_ops {\n \t\t\t  enum dma_data_direction direction);\n \tvoid (*unmap_single)(struct device *dev, u64 dma_address,\n \t\t\t     size_t size, enum dma_data_direction direction);\n+\tint (*mapping_error)(struct device *dev, u64 dma_address);\n };\n \n struct niu_link_config {\ndiff --git a/drivers/net/ethernet/wangxun/libwx/wx_lib.c b/drivers/net/ethernet/wangxun/libwx/wx_lib.c\nindex c57cc4f27249..55e252789db3 100644\n--- a/drivers/net/ethernet/wangxun/libwx/wx_lib.c\n+++ b/drivers/net/ethernet/wangxun/libwx/wx_lib.c\n@@ -1705,6 +1705,7 @@ static void wx_set_rss_queues(struct wx *wx)\n \n \tclear_bit(WX_FLAG_FDIR_HASH, wx->flags);\n \n+\twx->ring_feature[RING_F_FDIR].indices = 1;\n \t/* Use Flow Director in addition to RSS to ensure the best\n \t * distribution of flows across cores, even when an FDIR flow\n \t * isn't matched.\n@@ -1746,7 +1747,7 @@ static void wx_set_num_queues(struct wx *wx)\n  */\n static int wx_acquire_msix_vectors(struct wx *wx)\n {\n-\tstruct irq_affinity affd = { .pre_vectors = 1 };\n+\tstruct irq_affinity affd = { .post_vectors = 1 };\n \tint nvecs, i;\n \n \t/* We start by asking for one vector per queue pair */\n@@ -1783,16 +1784,24 @@ static int wx_acquire_msix_vectors(struct wx *wx)\n \t\treturn nvecs;\n \t}\n \n-\twx->msix_entry->entry = 0;\n-\twx->msix_entry->vector = pci_irq_vector(wx->pdev, 0);\n \tnvecs -= 1;\n \tfor (i = 0; i < nvecs; i++) {\n \t\twx->msix_q_entries[i].entry = i;\n-\t\twx->msix_q_entries[i].vector = pci_irq_vector(wx->pdev, i + 1);\n+\t\twx->msix_q_entries[i].vector = pci_irq_vector(wx->pdev, i);\n \t}\n \n \twx->num_q_vectors = nvecs;\n \n+\twx->msix_entry->entry = nvecs;\n+\twx->msix_entry->vector = pci_irq_vector(wx->pdev, nvecs);\n+\n+\tif (test_bit(WX_FLAG_IRQ_VECTOR_SHARED, wx->flags)) {\n+\t\twx->msix_entry->entry = 0;\n+\t\twx->msix_entry->vector = pci_irq_vector(wx->pdev, 0);\n+\t\twx->msix_q_entries[0].entry = 0;\n+\t\twx->msix_q_entries[0].vector = pci_irq_vector(wx->pdev, 1);\n+\t}\n+\n \treturn 0;\n }\n \n@@ -2291,6 +2300,8 @@ static void wx_set_ivar(struct wx *wx, s8 direction,\n \n \tif (direction == -1) {\n \t\t/* other causes */\n+\t\tif (test_bit(WX_FLAG_IRQ_VECTOR_SHARED, wx->flags))\n+\t\t\tmsix_vector = 0;\n \t\tmsix_vector |= WX_PX_IVAR_ALLOC_VAL;\n \t\tindex = 0;\n \t\tivar = rd32(wx, WX_PX_MISC_IVAR);\n@@ -2299,8 +2310,6 @@ static void wx_set_ivar(struct wx *wx, s8 direction,\n \t\twr32(wx, WX_PX_MISC_IVAR, ivar);\n \t} else {\n \t\t/* tx or rx causes */\n-\t\tif (!(wx->mac.type == wx_mac_em && wx->num_vfs == 7))\n-\t\t\tmsix_vector += 1; /* offset for queue vectors */\n \t\tmsix_vector |= WX_PX_IVAR_ALLOC_VAL;\n \t\tindex = ((16 * (queue & 1)) + (8 * direction));\n \t\tivar = rd32(wx, WX_PX_IVAR(queue >> 1));\n@@ -2339,7 +2348,7 @@ void wx_write_eitr(struct wx_q_vector *q_vector)\n \n \titr_reg |= WX_PX_ITR_CNT_WDIS;\n \n-\twr32(wx, WX_PX_ITR(v_idx + 1), itr_reg);\n+\twr32(wx, WX_PX_ITR(v_idx), itr_reg);\n }\n \n /**\n@@ -2392,9 +2401,9 @@ void wx_configure_vectors(struct wx *wx)\n \t\twx_write_eitr(q_vector);\n \t}\n \n-\twx_set_ivar(wx, -1, 0, 0);\n+\twx_set_ivar(wx, -1, 0, v_idx);\n \tif (pdev->msix_enabled)\n-\t\twr32(wx, WX_PX_ITR(0), 1950);\n+\t\twr32(wx, WX_PX_ITR(v_idx), 1950);\n }\n EXPORT_SYMBOL(wx_configure_vectors);\n \ndiff --git a/drivers/net/ethernet/wangxun/libwx/wx_sriov.c b/drivers/net/ethernet/wangxun/libwx/wx_sriov.c\nindex e8656d9d733b..c82ae137756c 100644\n--- a/drivers/net/ethernet/wangxun/libwx/wx_sriov.c\n+++ b/drivers/net/ethernet/wangxun/libwx/wx_sriov.c\n@@ -64,6 +64,7 @@ static void wx_sriov_clear_data(struct wx *wx)\n \twr32m(wx, WX_PSR_VM_CTL, WX_PSR_VM_CTL_POOL_MASK, 0);\n \twx->ring_feature[RING_F_VMDQ].offset = 0;\n \n+\tclear_bit(WX_FLAG_IRQ_VECTOR_SHARED, wx->flags);\n \tclear_bit(WX_FLAG_SRIOV_ENABLED, wx->flags);\n \t/* Disable VMDq flag so device will be set in NM mode */\n \tif (wx->ring_feature[RING_F_VMDQ].limit == 1)\n@@ -78,6 +79,9 @@ static int __wx_enable_sriov(struct wx *wx, u8 num_vfs)\n \tset_bit(WX_FLAG_SRIOV_ENABLED, wx->flags);\n \tdev_info(&wx->pdev->dev, \"SR-IOV enabled with %d VFs\\n\", num_vfs);\n \n+\tif (num_vfs == 7 && wx->mac.type == wx_mac_em)\n+\t\tset_bit(WX_FLAG_IRQ_VECTOR_SHARED, wx->flags);\n+\n \t/* Enable VMDq flag so device will be set in VM mode */\n \tset_bit(WX_FLAG_VMDQ_ENABLED, wx->flags);\n \tif (!wx->ring_feature[RING_F_VMDQ].limit)\ndiff --git a/drivers/net/ethernet/wangxun/libwx/wx_type.h b/drivers/net/ethernet/wangxun/libwx/wx_type.h\nindex 7730c9fc3e02..c363379126c0 100644\n--- a/drivers/net/ethernet/wangxun/libwx/wx_type.h\n+++ b/drivers/net/ethernet/wangxun/libwx/wx_type.h\n@@ -1191,6 +1191,7 @@ enum wx_pf_flags {\n \tWX_FLAG_VMDQ_ENABLED,\n \tWX_FLAG_VLAN_PROMISC,\n \tWX_FLAG_SRIOV_ENABLED,\n+\tWX_FLAG_IRQ_VECTOR_SHARED,\n \tWX_FLAG_FDIR_CAPABLE,\n \tWX_FLAG_FDIR_HASH,\n \tWX_FLAG_FDIR_PERFECT,\n@@ -1343,7 +1344,7 @@ struct wx {\n };\n \n #define WX_INTR_ALL (~0ULL)\n-#define WX_INTR_Q(i) BIT((i) + 1)\n+#define WX_INTR_Q(i) BIT((i))\n \n /* register operations */\n #define wr32(a, reg, value)\twritel((value), ((a)->hw_addr + (reg)))\ndiff --git a/drivers/net/ethernet/wangxun/ngbe/ngbe_main.c b/drivers/net/ethernet/wangxun/ngbe/ngbe_main.c\nindex b5022c49dc5e..e0fc897b0a58 100644\n--- a/drivers/net/ethernet/wangxun/ngbe/ngbe_main.c\n+++ b/drivers/net/ethernet/wangxun/ngbe/ngbe_main.c\n@@ -161,7 +161,7 @@ static void ngbe_irq_enable(struct wx *wx, bool queues)\n \tif (queues)\n \t\twx_intr_enable(wx, NGBE_INTR_ALL);\n \telse\n-\t\twx_intr_enable(wx, NGBE_INTR_MISC);\n+\t\twx_intr_enable(wx, NGBE_INTR_MISC(wx));\n }\n \n /**\n@@ -286,7 +286,7 @@ static int ngbe_request_msix_irqs(struct wx *wx)\n \t * for queue. But when num_vfs == 7, vector[1] is assigned to vf6.\n \t * Misc and queue should reuse interrupt vector[0].\n \t */\n-\tif (wx->num_vfs == 7)\n+\tif (test_bit(WX_FLAG_IRQ_VECTOR_SHARED, wx->flags))\n \t\terr = request_irq(wx->msix_entry->vector,\n \t\t\t\t  ngbe_misc_and_queue, 0, netdev->name, wx);\n \telse\ndiff --git a/drivers/net/ethernet/wangxun/ngbe/ngbe_type.h b/drivers/net/ethernet/wangxun/ngbe/ngbe_type.h\nindex bb74263f0498..3b2ca7f47e33 100644\n--- a/drivers/net/ethernet/wangxun/ngbe/ngbe_type.h\n+++ b/drivers/net/ethernet/wangxun/ngbe/ngbe_type.h\n@@ -87,7 +87,7 @@\n #define NGBE_PX_MISC_IC_TIMESYNC\t\tBIT(11) /* time sync */\n \n #define NGBE_INTR_ALL\t\t\t\t0x1FF\n-#define NGBE_INTR_MISC\t\t\t\tBIT(0)\n+#define NGBE_INTR_MISC(A)\t\t\tBIT((A)->msix_entry->entry)\n \n #define NGBE_PHY_CONFIG(reg_offset)\t\t(0x14000 + ((reg_offset) * 4))\n #define NGBE_CFG_LAN_SPEED\t\t\t0x14440\ndiff --git a/drivers/net/ethernet/wangxun/txgbe/txgbe_aml.c b/drivers/net/ethernet/wangxun/txgbe/txgbe_aml.c\nindex 7dbcf41750c1..dc87ccad9652 100644\n--- a/drivers/net/ethernet/wangxun/txgbe/txgbe_aml.c\n+++ b/drivers/net/ethernet/wangxun/txgbe/txgbe_aml.c\n@@ -294,6 +294,7 @@ static void txgbe_mac_link_up_aml(struct phylink_config *config,\n \twx_fc_enable(wx, tx_pause, rx_pause);\n \n \ttxgbe_reconfig_mac(wx);\n+\ttxgbe_enable_sec_tx_path(wx);\n \n \ttxcfg = rd32(wx, TXGBE_AML_MAC_TX_CFG);\n \ttxcfg &= ~TXGBE_AML_MAC_TX_CFG_SPEED_MASK;\ndiff --git a/drivers/net/ethernet/wangxun/txgbe/txgbe_irq.c b/drivers/net/ethernet/wangxun/txgbe/txgbe_irq.c\nindex 20b9a28bcb55..3885283681ec 100644\n--- a/drivers/net/ethernet/wangxun/txgbe/txgbe_irq.c\n+++ b/drivers/net/ethernet/wangxun/txgbe/txgbe_irq.c\n@@ -31,7 +31,7 @@ void txgbe_irq_enable(struct wx *wx, bool queues)\n \twr32(wx, WX_PX_MISC_IEN, misc_ien);\n \n \t/* unmask interrupt */\n-\twx_intr_enable(wx, TXGBE_INTR_MISC);\n+\twx_intr_enable(wx, TXGBE_INTR_MISC(wx));\n \tif (queues)\n \t\twx_intr_enable(wx, TXGBE_INTR_QALL(wx));\n }\n@@ -78,7 +78,6 @@ int txgbe_request_queue_irqs(struct wx *wx)\n \t\tfree_irq(wx->msix_q_entries[vector].vector,\n \t\t\t wx->q_vector[vector]);\n \t}\n-\twx_reset_interrupt_capability(wx);\n \treturn err;\n }\n \n@@ -132,7 +131,7 @@ static irqreturn_t txgbe_misc_irq_handle(int irq, void *data)\n \t\ttxgbe->eicr = eicr;\n \t\tif (eicr & TXGBE_PX_MISC_IC_VF_MBOX) {\n \t\t\twx_msg_task(txgbe->wx);\n-\t\t\twx_intr_enable(wx, TXGBE_INTR_MISC);\n+\t\t\twx_intr_enable(wx, TXGBE_INTR_MISC(wx));\n \t\t}\n \t\treturn IRQ_WAKE_THREAD;\n \t}\n@@ -184,7 +183,7 @@ static irqreturn_t txgbe_misc_irq_thread_fn(int irq, void *data)\n \t\tnhandled++;\n \t}\n \n-\twx_intr_enable(wx, TXGBE_INTR_MISC);\n+\twx_intr_enable(wx, TXGBE_INTR_MISC(wx));\n \treturn (nhandled > 0 ? IRQ_HANDLED : IRQ_NONE);\n }\n \n@@ -211,6 +210,7 @@ void txgbe_free_misc_irq(struct txgbe *txgbe)\n \tfree_irq(txgbe->link_irq, txgbe);\n \tfree_irq(txgbe->misc.irq, txgbe);\n \ttxgbe_del_irq_domain(txgbe);\n+\ttxgbe->wx->misc_irq_domain = false;\n }\n \n int txgbe_setup_misc_irq(struct txgbe *txgbe)\ndiff --git a/drivers/net/ethernet/wangxun/txgbe/txgbe_main.c b/drivers/net/ethernet/wangxun/txgbe/txgbe_main.c\nindex f3d2778b8e35..a5867f3c93fc 100644\n--- a/drivers/net/ethernet/wangxun/txgbe/txgbe_main.c\n+++ b/drivers/net/ethernet/wangxun/txgbe/txgbe_main.c\n@@ -458,10 +458,14 @@ static int txgbe_open(struct net_device *netdev)\n \n \twx_configure(wx);\n \n-\terr = txgbe_request_queue_irqs(wx);\n+\terr = txgbe_setup_misc_irq(wx->priv);\n \tif (err)\n \t\tgoto err_free_resources;\n \n+\terr = txgbe_request_queue_irqs(wx);\n+\tif (err)\n+\t\tgoto err_free_misc_irq;\n+\n \t/* Notify the stack of the actual queue counts. */\n \terr = netif_set_real_num_tx_queues(netdev, wx->num_tx_queues);\n \tif (err)\n@@ -479,6 +483,9 @@ static int txgbe_open(struct net_device *netdev)\n \n err_free_irq:\n \twx_free_irq(wx);\n+err_free_misc_irq:\n+\ttxgbe_free_misc_irq(wx->priv);\n+\twx_reset_interrupt_capability(wx);\n err_free_resources:\n \twx_free_resources(wx);\n err_reset:\n@@ -519,6 +526,7 @@ static int txgbe_close(struct net_device *netdev)\n \twx_ptp_stop(wx);\n \ttxgbe_down(wx);\n \twx_free_irq(wx);\n+\ttxgbe_free_misc_irq(wx->priv);\n \twx_free_resources(wx);\n \ttxgbe_fdir_filter_exit(wx);\n \twx_control_hw(wx, false);\n@@ -564,7 +572,6 @@ static void txgbe_shutdown(struct pci_dev *pdev)\n int txgbe_setup_tc(struct net_device *dev, u8 tc)\n {\n \tstruct wx *wx = netdev_priv(dev);\n-\tstruct txgbe *txgbe = wx->priv;\n \n \t/* Hardware has to reinitialize queues and interrupts to\n \t * match packet buffer alignment. Unfortunately, the\n@@ -575,7 +582,6 @@ int txgbe_setup_tc(struct net_device *dev, u8 tc)\n \telse\n \t\ttxgbe_reset(wx);\n \n-\ttxgbe_free_misc_irq(txgbe);\n \twx_clear_interrupt_scheme(wx);\n \n \tif (tc)\n@@ -584,7 +590,6 @@ int txgbe_setup_tc(struct net_device *dev, u8 tc)\n \t\tnetdev_reset_tc(dev);\n \n \twx_init_interrupt_scheme(wx);\n-\ttxgbe_setup_misc_irq(txgbe);\n \n \tif (netif_running(dev))\n \t\ttxgbe_open(dev);\n@@ -882,13 +887,9 @@ static int txgbe_probe(struct pci_dev *pdev,\n \n \ttxgbe_init_fdir(txgbe);\n \n-\terr = txgbe_setup_misc_irq(txgbe);\n-\tif (err)\n-\t\tgoto err_release_hw;\n-\n \terr = txgbe_init_phy(txgbe);\n \tif (err)\n-\t\tgoto err_free_misc_irq;\n+\t\tgoto err_release_hw;\n \n \terr = register_netdev(netdev);\n \tif (err)\n@@ -916,8 +917,6 @@ static int txgbe_probe(struct pci_dev *pdev,\n \n err_remove_phy:\n \ttxgbe_remove_phy(txgbe);\n-err_free_misc_irq:\n-\ttxgbe_free_misc_irq(txgbe);\n err_release_hw:\n \twx_clear_interrupt_scheme(wx);\n \twx_control_hw(wx, false);\n@@ -957,7 +956,6 @@ static void txgbe_remove(struct pci_dev *pdev)\n \tunregister_netdev(netdev);\n \n \ttxgbe_remove_phy(txgbe);\n-\ttxgbe_free_misc_irq(txgbe);\n \twx_free_isb_resources(wx);\n \n \tpci_release_selected_regions(pdev,\ndiff --git a/drivers/net/ethernet/wangxun/txgbe/txgbe_type.h b/drivers/net/ethernet/wangxun/txgbe/txgbe_type.h\nindex 42ec815159e8..41915d7dd372 100644\n--- a/drivers/net/ethernet/wangxun/txgbe/txgbe_type.h\n+++ b/drivers/net/ethernet/wangxun/txgbe/txgbe_type.h\n@@ -302,8 +302,8 @@ struct txgbe_fdir_filter {\n #define TXGBE_DEFAULT_RX_WORK           128\n #endif\n \n-#define TXGBE_INTR_MISC       BIT(0)\n-#define TXGBE_INTR_QALL(A)    GENMASK((A)->num_q_vectors, 1)\n+#define TXGBE_INTR_MISC(A)    BIT((A)->num_q_vectors)\n+#define TXGBE_INTR_QALL(A)    (TXGBE_INTR_MISC(A) - 1)\n \n #define TXGBE_MAX_EITR        GENMASK(11, 3)\n \ndiff --git a/drivers/net/usb/lan78xx.c b/drivers/net/usb/lan78xx.c\nindex f53e255116ea..e3ca6e91efe1 100644\n--- a/drivers/net/usb/lan78xx.c\n+++ b/drivers/net/usb/lan78xx.c\n@@ -4567,8 +4567,6 @@ static void lan78xx_disconnect(struct usb_interface *intf)\n \tif (!dev)\n \t\treturn;\n \n-\tnetif_napi_del(&dev->napi);\n-\n \tudev = interface_to_usbdev(intf);\n \tnet = dev->net;\n \ndiff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c\nindex e53ba600605a..5d674eb9a0f2 100644\n--- a/drivers/net/virtio_net.c\n+++ b/drivers/net/virtio_net.c\n@@ -778,6 +778,26 @@ static unsigned int mergeable_ctx_to_truesize(void *mrg_ctx)\n \treturn (unsigned long)mrg_ctx & ((1 << MRG_CTX_HEADER_SHIFT) - 1);\n }\n \n+static int check_mergeable_len(struct net_device *dev, void *mrg_ctx,\n+\t\t\t       unsigned int len)\n+{\n+\tunsigned int headroom, tailroom, room, truesize;\n+\n+\ttruesize = mergeable_ctx_to_truesize(mrg_ctx);\n+\theadroom = mergeable_ctx_to_headroom(mrg_ctx);\n+\ttailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n+\troom = SKB_DATA_ALIGN(headroom + tailroom);\n+\n+\tif (len > truesize - room) {\n+\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n+\t\t\t dev->name, len, (unsigned long)(truesize - room));\n+\t\tDEV_STATS_INC(dev, rx_length_errors);\n+\t\treturn -1;\n+\t}\n+\n+\treturn 0;\n+}\n+\n static struct sk_buff *virtnet_build_skb(void *buf, unsigned int buflen,\n \t\t\t\t\t unsigned int headroom,\n \t\t\t\t\t unsigned int len)\n@@ -1084,7 +1104,7 @@ static bool tx_may_stop(struct virtnet_info *vi,\n \t * Since most packets only take 1 or 2 ring slots, stopping the queue\n \t * early means 16 slots are typically wasted.\n \t */\n-\tif (sq->vq->num_free < 2+MAX_SKB_FRAGS) {\n+\tif (sq->vq->num_free < MAX_SKB_FRAGS + 2) {\n \t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, qnum);\n \n \t\tnetif_tx_stop_queue(txq);\n@@ -1116,7 +1136,7 @@ static void check_sq_full_and_disable(struct virtnet_info *vi,\n \t\t} else if (unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {\n \t\t\t/* More just got used, free them then recheck. */\n \t\t\tfree_old_xmit(sq, txq, false);\n-\t\t\tif (sq->vq->num_free >= 2+MAX_SKB_FRAGS) {\n+\t\t\tif (sq->vq->num_free >= MAX_SKB_FRAGS + 2) {\n \t\t\t\tnetif_start_subqueue(dev, qnum);\n \t\t\t\tu64_stats_update_begin(&sq->stats.syncp);\n \t\t\t\tu64_stats_inc(&sq->stats.wake);\n@@ -1127,15 +1147,29 @@ static void check_sq_full_and_disable(struct virtnet_info *vi,\n \t}\n }\n \n+/* Note that @len is the length of received data without virtio header */\n static struct xdp_buff *buf_to_xdp(struct virtnet_info *vi,\n-\t\t\t\t   struct receive_queue *rq, void *buf, u32 len)\n+\t\t\t\t   struct receive_queue *rq, void *buf,\n+\t\t\t\t   u32 len, bool first_buf)\n {\n \tstruct xdp_buff *xdp;\n \tu32 bufsize;\n \n \txdp = (struct xdp_buff *)buf;\n \n-\tbufsize = xsk_pool_get_rx_frame_size(rq->xsk_pool) + vi->hdr_len;\n+\t/* In virtnet_add_recvbuf_xsk, we use part of XDP_PACKET_HEADROOM for\n+\t * virtio header and ask the vhost to fill data from\n+\t *         hard_start + XDP_PACKET_HEADROOM - vi->hdr_len\n+\t * The first buffer has virtio header so the remaining region for frame\n+\t * data is\n+\t *         xsk_pool_get_rx_frame_size()\n+\t * While other buffers than the first one do not have virtio header, so\n+\t * the maximum frame data's length can be\n+\t *         xsk_pool_get_rx_frame_size() + vi->hdr_len\n+\t */\n+\tbufsize = xsk_pool_get_rx_frame_size(rq->xsk_pool);\n+\tif (!first_buf)\n+\t\tbufsize += vi->hdr_len;\n \n \tif (unlikely(len > bufsize)) {\n \t\tpr_debug(\"%s: rx error: len %u exceeds truesize %u\\n\",\n@@ -1260,7 +1294,7 @@ static int xsk_append_merge_buffer(struct virtnet_info *vi,\n \n \t\tu64_stats_add(&stats->bytes, len);\n \n-\t\txdp = buf_to_xdp(vi, rq, buf, len);\n+\t\txdp = buf_to_xdp(vi, rq, buf, len, false);\n \t\tif (!xdp)\n \t\t\tgoto err;\n \n@@ -1358,7 +1392,7 @@ static void virtnet_receive_xsk_buf(struct virtnet_info *vi, struct receive_queu\n \n \tu64_stats_add(&stats->bytes, len);\n \n-\txdp = buf_to_xdp(vi, rq, buf, len);\n+\txdp = buf_to_xdp(vi, rq, buf, len, true);\n \tif (!xdp)\n \t\treturn;\n \n@@ -1797,7 +1831,8 @@ static unsigned int virtnet_get_headroom(struct virtnet_info *vi)\n  * across multiple buffers (num_buf > 1), and we make sure buffers\n  * have enough headroom.\n  */\n-static struct page *xdp_linearize_page(struct receive_queue *rq,\n+static struct page *xdp_linearize_page(struct net_device *dev,\n+\t\t\t\t       struct receive_queue *rq,\n \t\t\t\t       int *num_buf,\n \t\t\t\t       struct page *p,\n \t\t\t\t       int offset,\n@@ -1817,18 +1852,27 @@ static struct page *xdp_linearize_page(struct receive_queue *rq,\n \tmemcpy(page_address(page) + page_off, page_address(p) + offset, *len);\n \tpage_off += *len;\n \n+\t/* Only mergeable mode can go inside this while loop. In small mode,\n+\t * *num_buf == 1, so it cannot go inside.\n+\t */\n \twhile (--*num_buf) {\n \t\tunsigned int buflen;\n \t\tvoid *buf;\n+\t\tvoid *ctx;\n \t\tint off;\n \n-\t\tbuf = virtnet_rq_get_buf(rq, &buflen, NULL);\n+\t\tbuf = virtnet_rq_get_buf(rq, &buflen, &ctx);\n \t\tif (unlikely(!buf))\n \t\t\tgoto err_buf;\n \n \t\tp = virt_to_head_page(buf);\n \t\toff = buf - page_address(p);\n \n+\t\tif (check_mergeable_len(dev, ctx, buflen)) {\n+\t\t\tput_page(p);\n+\t\t\tgoto err_buf;\n+\t\t}\n+\n \t\t/* guard against a misconfigured or uncooperative backend that\n \t\t * is sending packet larger than the MTU.\n \t\t */\n@@ -1917,7 +1961,7 @@ static struct sk_buff *receive_small_xdp(struct net_device *dev,\n \t\theadroom = vi->hdr_len + header_offset;\n \t\tbuflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +\n \t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n-\t\txdp_page = xdp_linearize_page(rq, &num_buf, page,\n+\t\txdp_page = xdp_linearize_page(dev, rq, &num_buf, page,\n \t\t\t\t\t      offset, header_offset,\n \t\t\t\t\t      &tlen);\n \t\tif (!xdp_page)\n@@ -2126,10 +2170,9 @@ static int virtnet_build_xdp_buff_mrg(struct net_device *dev,\n \t\t\t\t      struct virtnet_rq_stats *stats)\n {\n \tstruct virtio_net_hdr_mrg_rxbuf *hdr = buf;\n-\tunsigned int headroom, tailroom, room;\n-\tunsigned int truesize, cur_frag_size;\n \tstruct skb_shared_info *shinfo;\n \tunsigned int xdp_frags_truesz = 0;\n+\tunsigned int truesize;\n \tstruct page *page;\n \tskb_frag_t *frag;\n \tint offset;\n@@ -2172,21 +2215,14 @@ static int virtnet_build_xdp_buff_mrg(struct net_device *dev,\n \t\tpage = virt_to_head_page(buf);\n \t\toffset = buf - page_address(page);\n \n-\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n-\t\theadroom = mergeable_ctx_to_headroom(ctx);\n-\t\ttailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n-\t\troom = SKB_DATA_ALIGN(headroom + tailroom);\n-\n-\t\tcur_frag_size = truesize;\n-\t\txdp_frags_truesz += cur_frag_size;\n-\t\tif (unlikely(len > truesize - room || cur_frag_size > PAGE_SIZE)) {\n+\t\tif (check_mergeable_len(dev, ctx, len)) {\n \t\t\tput_page(page);\n-\t\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n-\t\t\t\t dev->name, len, (unsigned long)(truesize - room));\n-\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n \t\t\tgoto err;\n \t\t}\n \n+\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n+\t\txdp_frags_truesz += truesize;\n+\n \t\tfrag = &shinfo->frags[shinfo->nr_frags++];\n \t\tskb_frag_fill_page_desc(frag, page, offset, len);\n \t\tif (page_is_pfmemalloc(page))\n@@ -2252,7 +2288,7 @@ static void *mergeable_xdp_get_buf(struct virtnet_info *vi,\n \t */\n \tif (!xdp_prog->aux->xdp_has_frags) {\n \t\t/* linearize data for XDP */\n-\t\txdp_page = xdp_linearize_page(rq, num_buf,\n+\t\txdp_page = xdp_linearize_page(vi->dev, rq, num_buf,\n \t\t\t\t\t      *page, offset,\n \t\t\t\t\t      XDP_PACKET_HEADROOM,\n \t\t\t\t\t      len);\n@@ -2400,18 +2436,12 @@ static struct sk_buff *receive_mergeable(struct net_device *dev,\n \tstruct sk_buff *head_skb, *curr_skb;\n \tunsigned int truesize = mergeable_ctx_to_truesize(ctx);\n \tunsigned int headroom = mergeable_ctx_to_headroom(ctx);\n-\tunsigned int tailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n-\tunsigned int room = SKB_DATA_ALIGN(headroom + tailroom);\n \n \thead_skb = NULL;\n \tu64_stats_add(&stats->bytes, len - vi->hdr_len);\n \n-\tif (unlikely(len > truesize - room)) {\n-\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n-\t\t\t dev->name, len, (unsigned long)(truesize - room));\n-\t\tDEV_STATS_INC(dev, rx_length_errors);\n+\tif (check_mergeable_len(dev, ctx, len))\n \t\tgoto err_skb;\n-\t}\n \n \tif (unlikely(vi->xdp_enabled)) {\n \t\tstruct bpf_prog *xdp_prog;\n@@ -2446,17 +2476,10 @@ static struct sk_buff *receive_mergeable(struct net_device *dev,\n \t\tu64_stats_add(&stats->bytes, len);\n \t\tpage = virt_to_head_page(buf);\n \n-\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n-\t\theadroom = mergeable_ctx_to_headroom(ctx);\n-\t\ttailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n-\t\troom = SKB_DATA_ALIGN(headroom + tailroom);\n-\t\tif (unlikely(len > truesize - room)) {\n-\t\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n-\t\t\t\t dev->name, len, (unsigned long)(truesize - room));\n-\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n+\t\tif (check_mergeable_len(dev, ctx, len))\n \t\t\tgoto err_skb;\n-\t\t}\n \n+\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n \t\tcurr_skb  = virtnet_skb_append_frag(head_skb, curr_skb, page,\n \t\t\t\t\t\t    buf, len, truesize);\n \t\tif (!curr_skb)\n@@ -2998,7 +3021,7 @@ static void virtnet_poll_cleantx(struct receive_queue *rq, int budget)\n \t\t\tfree_old_xmit(sq, txq, !!budget);\n \t\t} while (unlikely(!virtqueue_enable_cb_delayed(sq->vq)));\n \n-\t\tif (sq->vq->num_free >= 2 + MAX_SKB_FRAGS) {\n+\t\tif (sq->vq->num_free >= MAX_SKB_FRAGS + 2) {\n \t\t\tif (netif_tx_queue_stopped(txq)) {\n \t\t\t\tu64_stats_update_begin(&sq->stats.syncp);\n \t\t\t\tu64_stats_inc(&sq->stats.wake);\n@@ -3195,7 +3218,7 @@ static int virtnet_poll_tx(struct napi_struct *napi, int budget)\n \telse\n \t\tfree_old_xmit(sq, txq, !!budget);\n \n-\tif (sq->vq->num_free >= 2 + MAX_SKB_FRAGS) {\n+\tif (sq->vq->num_free >= MAX_SKB_FRAGS + 2) {\n \t\tif (netif_tx_queue_stopped(txq)) {\n \t\t\tu64_stats_update_begin(&sq->stats.syncp);\n \t\t\tu64_stats_inc(&sq->stats.wake);\n@@ -3481,6 +3504,12 @@ static int virtnet_tx_resize(struct virtnet_info *vi, struct send_queue *sq,\n {\n \tint qindex, err;\n \n+\tif (ring_num <= MAX_SKB_FRAGS + 2) {\n+\t\tnetdev_err(vi->dev, \"tx size (%d) cannot be smaller than %d\\n\",\n+\t\t\t   ring_num, MAX_SKB_FRAGS + 2);\n+\t\treturn -EINVAL;\n+\t}\n+\n \tqindex = sq - vi->sq;\n \n \tvirtnet_tx_pause(vi, sq);\ndiff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c\nindex b784aab66867..4397392bfef0 100644\n--- a/drivers/virtio/virtio_ring.c\n+++ b/drivers/virtio/virtio_ring.c\n@@ -2797,7 +2797,7 @@ int virtqueue_resize(struct virtqueue *_vq, u32 num,\n \t\t     void (*recycle_done)(struct virtqueue *vq))\n {\n \tstruct vring_virtqueue *vq = to_vvq(_vq);\n-\tint err;\n+\tint err, err_reset;\n \n \tif (num > vq->vq.num_max)\n \t\treturn -E2BIG;\n@@ -2819,7 +2819,11 @@ int virtqueue_resize(struct virtqueue *_vq, u32 num,\n \telse\n \t\terr = virtqueue_resize_split(_vq, num);\n \n-\treturn virtqueue_enable_after_reset(_vq);\n+\terr_reset = virtqueue_enable_after_reset(_vq);\n+\tif (err_reset)\n+\t\treturn err_reset;\n+\n+\treturn err;\n }\n EXPORT_SYMBOL_GPL(virtqueue_resize);\n \ndiff --git a/lib/test_objagg.c b/lib/test_objagg.c\nindex d34df4306b87..222b39fc2629 100644\n--- a/lib/test_objagg.c\n+++ b/lib/test_objagg.c\n@@ -899,8 +899,10 @@ static int check_expect_hints_stats(struct objagg_hints *objagg_hints,\n \tint err;\n \n \tstats = objagg_hints_stats_get(objagg_hints);\n-\tif (IS_ERR(stats))\n+\tif (IS_ERR(stats)) {\n+\t\t*errmsg = \"objagg_hints_stats_get() failed.\";\n \t\treturn PTR_ERR(stats);\n+\t}\n \terr = __check_expect_stats(stats, expect_stats, errmsg);\n \tobjagg_stats_put(stats);\n \treturn err;\ndiff --git a/net/bluetooth/hci_event.c b/net/bluetooth/hci_event.c\nindex 66052d6aaa1d..4d5ace9d245d 100644\n--- a/net/bluetooth/hci_event.c\n+++ b/net/bluetooth/hci_event.c\n@@ -2150,40 +2150,6 @@ static u8 hci_cc_set_adv_param(struct hci_dev *hdev, void *data,\n \treturn rp->status;\n }\n \n-static u8 hci_cc_set_ext_adv_param(struct hci_dev *hdev, void *data,\n-\t\t\t\t   struct sk_buff *skb)\n-{\n-\tstruct hci_rp_le_set_ext_adv_params *rp = data;\n-\tstruct hci_cp_le_set_ext_adv_params *cp;\n-\tstruct adv_info *adv_instance;\n-\n-\tbt_dev_dbg(hdev, \"status 0x%2.2x\", rp->status);\n-\n-\tif (rp->status)\n-\t\treturn rp->status;\n-\n-\tcp = hci_sent_cmd_data(hdev, HCI_OP_LE_SET_EXT_ADV_PARAMS);\n-\tif (!cp)\n-\t\treturn rp->status;\n-\n-\thci_dev_lock(hdev);\n-\thdev->adv_addr_type = cp->own_addr_type;\n-\tif (!cp->handle) {\n-\t\t/* Store in hdev for instance 0 */\n-\t\thdev->adv_tx_power = rp->tx_power;\n-\t} else {\n-\t\tadv_instance = hci_find_adv_instance(hdev, cp->handle);\n-\t\tif (adv_instance)\n-\t\t\tadv_instance->tx_power = rp->tx_power;\n-\t}\n-\t/* Update adv data as tx power is known now */\n-\thci_update_adv_data(hdev, cp->handle);\n-\n-\thci_dev_unlock(hdev);\n-\n-\treturn rp->status;\n-}\n-\n static u8 hci_cc_read_rssi(struct hci_dev *hdev, void *data,\n \t\t\t   struct sk_buff *skb)\n {\n@@ -4164,8 +4130,6 @@ static const struct hci_cc {\n \tHCI_CC(HCI_OP_LE_READ_NUM_SUPPORTED_ADV_SETS,\n \t       hci_cc_le_read_num_adv_sets,\n \t       sizeof(struct hci_rp_le_read_num_supported_adv_sets)),\n-\tHCI_CC(HCI_OP_LE_SET_EXT_ADV_PARAMS, hci_cc_set_ext_adv_param,\n-\t       sizeof(struct hci_rp_le_set_ext_adv_params)),\n \tHCI_CC_STATUS(HCI_OP_LE_SET_EXT_ADV_ENABLE,\n \t\t      hci_cc_le_set_ext_adv_enable),\n \tHCI_CC_STATUS(HCI_OP_LE_SET_ADV_SET_RAND_ADDR,\ndiff --git a/net/bluetooth/hci_sync.c b/net/bluetooth/hci_sync.c\nindex 6687f2a4d1eb..77b3691f3423 100644\n--- a/net/bluetooth/hci_sync.c\n+++ b/net/bluetooth/hci_sync.c\n@@ -1205,9 +1205,126 @@ static int hci_set_adv_set_random_addr_sync(struct hci_dev *hdev, u8 instance,\n \t\t\t\t     sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n }\n \n+static int\n+hci_set_ext_adv_params_sync(struct hci_dev *hdev, struct adv_info *adv,\n+\t\t\t    const struct hci_cp_le_set_ext_adv_params *cp,\n+\t\t\t    struct hci_rp_le_set_ext_adv_params *rp)\n+{\n+\tstruct sk_buff *skb;\n+\n+\tskb = __hci_cmd_sync(hdev, HCI_OP_LE_SET_EXT_ADV_PARAMS, sizeof(*cp),\n+\t\t\t     cp, HCI_CMD_TIMEOUT);\n+\n+\t/* If command return a status event, skb will be set to -ENODATA */\n+\tif (skb == ERR_PTR(-ENODATA))\n+\t\treturn 0;\n+\n+\tif (IS_ERR(skb)) {\n+\t\tbt_dev_err(hdev, \"Opcode 0x%4.4x failed: %ld\",\n+\t\t\t   HCI_OP_LE_SET_EXT_ADV_PARAMS, PTR_ERR(skb));\n+\t\treturn PTR_ERR(skb);\n+\t}\n+\n+\tif (skb->len != sizeof(*rp)) {\n+\t\tbt_dev_err(hdev, \"Invalid response length for 0x%4.4x: %u\",\n+\t\t\t   HCI_OP_LE_SET_EXT_ADV_PARAMS, skb->len);\n+\t\tkfree_skb(skb);\n+\t\treturn -EIO;\n+\t}\n+\n+\tmemcpy(rp, skb->data, sizeof(*rp));\n+\tkfree_skb(skb);\n+\n+\tif (!rp->status) {\n+\t\thdev->adv_addr_type = cp->own_addr_type;\n+\t\tif (!cp->handle) {\n+\t\t\t/* Store in hdev for instance 0 */\n+\t\t\thdev->adv_tx_power = rp->tx_power;\n+\t\t} else if (adv) {\n+\t\t\tadv->tx_power = rp->tx_power;\n+\t\t}\n+\t}\n+\n+\treturn rp->status;\n+}\n+\n+static int hci_set_ext_adv_data_sync(struct hci_dev *hdev, u8 instance)\n+{\n+\tDEFINE_FLEX(struct hci_cp_le_set_ext_adv_data, pdu, data, length,\n+\t\t    HCI_MAX_EXT_AD_LENGTH);\n+\tu8 len;\n+\tstruct adv_info *adv = NULL;\n+\tint err;\n+\n+\tif (instance) {\n+\t\tadv = hci_find_adv_instance(hdev, instance);\n+\t\tif (!adv || !adv->adv_data_changed)\n+\t\t\treturn 0;\n+\t}\n+\n+\tlen = eir_create_adv_data(hdev, instance, pdu->data,\n+\t\t\t\t  HCI_MAX_EXT_AD_LENGTH);\n+\n+\tpdu->length = len;\n+\tpdu->handle = adv ? adv->handle : instance;\n+\tpdu->operation = LE_SET_ADV_DATA_OP_COMPLETE;\n+\tpdu->frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n+\n+\terr = __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_EXT_ADV_DATA,\n+\t\t\t\t    struct_size(pdu, data, len), pdu,\n+\t\t\t\t    HCI_CMD_TIMEOUT);\n+\tif (err)\n+\t\treturn err;\n+\n+\t/* Update data if the command succeed */\n+\tif (adv) {\n+\t\tadv->adv_data_changed = false;\n+\t} else {\n+\t\tmemcpy(hdev->adv_data, pdu->data, len);\n+\t\thdev->adv_data_len = len;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int hci_set_adv_data_sync(struct hci_dev *hdev, u8 instance)\n+{\n+\tstruct hci_cp_le_set_adv_data cp;\n+\tu8 len;\n+\n+\tmemset(&cp, 0, sizeof(cp));\n+\n+\tlen = eir_create_adv_data(hdev, instance, cp.data, sizeof(cp.data));\n+\n+\t/* There's nothing to do if the data hasn't changed */\n+\tif (hdev->adv_data_len == len &&\n+\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n+\t\treturn 0;\n+\n+\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n+\thdev->adv_data_len = len;\n+\n+\tcp.length = len;\n+\n+\treturn __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_ADV_DATA,\n+\t\t\t\t     sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n+}\n+\n+int hci_update_adv_data_sync(struct hci_dev *hdev, u8 instance)\n+{\n+\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n+\t\treturn 0;\n+\n+\tif (ext_adv_capable(hdev))\n+\t\treturn hci_set_ext_adv_data_sync(hdev, instance);\n+\n+\treturn hci_set_adv_data_sync(hdev, instance);\n+}\n+\n int hci_setup_ext_adv_instance_sync(struct hci_dev *hdev, u8 instance)\n {\n \tstruct hci_cp_le_set_ext_adv_params cp;\n+\tstruct hci_rp_le_set_ext_adv_params rp;\n \tbool connectable;\n \tu32 flags;\n \tbdaddr_t random_addr;\n@@ -1316,8 +1433,12 @@ int hci_setup_ext_adv_instance_sync(struct hci_dev *hdev, u8 instance)\n \t\tcp.secondary_phy = HCI_ADV_PHY_1M;\n \t}\n \n-\terr = __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_EXT_ADV_PARAMS,\n-\t\t\t\t    sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n+\terr = hci_set_ext_adv_params_sync(hdev, adv, &cp, &rp);\n+\tif (err)\n+\t\treturn err;\n+\n+\t/* Update adv data as tx power is known now */\n+\terr = hci_set_ext_adv_data_sync(hdev, cp.handle);\n \tif (err)\n \t\treturn err;\n \n@@ -1822,79 +1943,6 @@ int hci_le_terminate_big_sync(struct hci_dev *hdev, u8 handle, u8 reason)\n \t\t\t\t     sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n }\n \n-static int hci_set_ext_adv_data_sync(struct hci_dev *hdev, u8 instance)\n-{\n-\tDEFINE_FLEX(struct hci_cp_le_set_ext_adv_data, pdu, data, length,\n-\t\t    HCI_MAX_EXT_AD_LENGTH);\n-\tu8 len;\n-\tstruct adv_info *adv = NULL;\n-\tint err;\n-\n-\tif (instance) {\n-\t\tadv = hci_find_adv_instance(hdev, instance);\n-\t\tif (!adv || !adv->adv_data_changed)\n-\t\t\treturn 0;\n-\t}\n-\n-\tlen = eir_create_adv_data(hdev, instance, pdu->data,\n-\t\t\t\t  HCI_MAX_EXT_AD_LENGTH);\n-\n-\tpdu->length = len;\n-\tpdu->handle = adv ? adv->handle : instance;\n-\tpdu->operation = LE_SET_ADV_DATA_OP_COMPLETE;\n-\tpdu->frag_pref = LE_SET_ADV_DATA_NO_FRAG;\n-\n-\terr = __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_EXT_ADV_DATA,\n-\t\t\t\t    struct_size(pdu, data, len), pdu,\n-\t\t\t\t    HCI_CMD_TIMEOUT);\n-\tif (err)\n-\t\treturn err;\n-\n-\t/* Update data if the command succeed */\n-\tif (adv) {\n-\t\tadv->adv_data_changed = false;\n-\t} else {\n-\t\tmemcpy(hdev->adv_data, pdu->data, len);\n-\t\thdev->adv_data_len = len;\n-\t}\n-\n-\treturn 0;\n-}\n-\n-static int hci_set_adv_data_sync(struct hci_dev *hdev, u8 instance)\n-{\n-\tstruct hci_cp_le_set_adv_data cp;\n-\tu8 len;\n-\n-\tmemset(&cp, 0, sizeof(cp));\n-\n-\tlen = eir_create_adv_data(hdev, instance, cp.data, sizeof(cp.data));\n-\n-\t/* There's nothing to do if the data hasn't changed */\n-\tif (hdev->adv_data_len == len &&\n-\t    memcmp(cp.data, hdev->adv_data, len) == 0)\n-\t\treturn 0;\n-\n-\tmemcpy(hdev->adv_data, cp.data, sizeof(cp.data));\n-\thdev->adv_data_len = len;\n-\n-\tcp.length = len;\n-\n-\treturn __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_ADV_DATA,\n-\t\t\t\t     sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n-}\n-\n-int hci_update_adv_data_sync(struct hci_dev *hdev, u8 instance)\n-{\n-\tif (!hci_dev_test_flag(hdev, HCI_LE_ENABLED))\n-\t\treturn 0;\n-\n-\tif (ext_adv_capable(hdev))\n-\t\treturn hci_set_ext_adv_data_sync(hdev, instance);\n-\n-\treturn hci_set_adv_data_sync(hdev, instance);\n-}\n-\n int hci_schedule_adv_instance_sync(struct hci_dev *hdev, u8 instance,\n \t\t\t\t   bool force)\n {\n@@ -1970,13 +2018,10 @@ static int hci_clear_adv_sets_sync(struct hci_dev *hdev, struct sock *sk)\n static int hci_clear_adv_sync(struct hci_dev *hdev, struct sock *sk, bool force)\n {\n \tstruct adv_info *adv, *n;\n-\tint err = 0;\n \n \tif (ext_adv_capable(hdev))\n \t\t/* Remove all existing sets */\n-\t\terr = hci_clear_adv_sets_sync(hdev, sk);\n-\tif (ext_adv_capable(hdev))\n-\t\treturn err;\n+\t\treturn hci_clear_adv_sets_sync(hdev, sk);\n \n \t/* This is safe as long as there is no command send while the lock is\n \t * held.\n@@ -2004,13 +2049,11 @@ static int hci_clear_adv_sync(struct hci_dev *hdev, struct sock *sk, bool force)\n static int hci_remove_adv_sync(struct hci_dev *hdev, u8 instance,\n \t\t\t       struct sock *sk)\n {\n-\tint err = 0;\n+\tint err;\n \n \t/* If we use extended advertising, instance has to be removed first. */\n \tif (ext_adv_capable(hdev))\n-\t\terr = hci_remove_ext_adv_instance_sync(hdev, instance, sk);\n-\tif (ext_adv_capable(hdev))\n-\t\treturn err;\n+\t\treturn hci_remove_ext_adv_instance_sync(hdev, instance, sk);\n \n \t/* This is safe as long as there is no command send while the lock is\n \t * held.\n@@ -2109,16 +2152,13 @@ int hci_read_tx_power_sync(struct hci_dev *hdev, __le16 handle, u8 type)\n int hci_disable_advertising_sync(struct hci_dev *hdev)\n {\n \tu8 enable = 0x00;\n-\tint err = 0;\n \n \t/* If controller is not advertising we are done. */\n \tif (!hci_dev_test_flag(hdev, HCI_LE_ADV))\n \t\treturn 0;\n \n \tif (ext_adv_capable(hdev))\n-\t\terr = hci_disable_ext_adv_instance_sync(hdev, 0x00);\n-\tif (ext_adv_capable(hdev))\n-\t\treturn err;\n+\t\treturn hci_disable_ext_adv_instance_sync(hdev, 0x00);\n \n \treturn __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_ADV_ENABLE,\n \t\t\t\t     sizeof(enable), &enable, HCI_CMD_TIMEOUT);\n@@ -2481,6 +2521,10 @@ static int hci_pause_advertising_sync(struct hci_dev *hdev)\n \tint err;\n \tint old_state;\n \n+\t/* If controller is not advertising we are done. */\n+\tif (!hci_dev_test_flag(hdev, HCI_LE_ADV))\n+\t\treturn 0;\n+\n \t/* If already been paused there is nothing to do. */\n \tif (hdev->advertising_paused)\n \t\treturn 0;\n@@ -6277,6 +6321,7 @@ static int hci_le_ext_directed_advertising_sync(struct hci_dev *hdev,\n \t\t\t\t\t\tstruct hci_conn *conn)\n {\n \tstruct hci_cp_le_set_ext_adv_params cp;\n+\tstruct hci_rp_le_set_ext_adv_params rp;\n \tint err;\n \tbdaddr_t random_addr;\n \tu8 own_addr_type;\n@@ -6318,8 +6363,12 @@ static int hci_le_ext_directed_advertising_sync(struct hci_dev *hdev,\n \tif (err)\n \t\treturn err;\n \n-\terr = __hci_cmd_sync_status(hdev, HCI_OP_LE_SET_EXT_ADV_PARAMS,\n-\t\t\t\t    sizeof(cp), &cp, HCI_CMD_TIMEOUT);\n+\terr = hci_set_ext_adv_params_sync(hdev, NULL, &cp, &rp);\n+\tif (err)\n+\t\treturn err;\n+\n+\t/* Update adv data as tx power is known now */\n+\terr = hci_set_ext_adv_data_sync(hdev, cp.handle);\n \tif (err)\n \t\treturn err;\n \ndiff --git a/net/bluetooth/mgmt.c b/net/bluetooth/mgmt.c\nindex d540f7b4f75f..1485b455ade4 100644\n--- a/net/bluetooth/mgmt.c\n+++ b/net/bluetooth/mgmt.c\n@@ -1080,7 +1080,8 @@ static int mesh_send_done_sync(struct hci_dev *hdev, void *data)\n \tstruct mgmt_mesh_tx *mesh_tx;\n \n \thci_dev_clear_flag(hdev, HCI_MESH_SENDING);\n-\thci_disable_advertising_sync(hdev);\n+\tif (list_empty(&hdev->adv_instances))\n+\t\thci_disable_advertising_sync(hdev);\n \tmesh_tx = mgmt_mesh_next(hdev, NULL);\n \n \tif (mesh_tx)\n@@ -2153,6 +2154,9 @@ static int set_mesh_sync(struct hci_dev *hdev, void *data)\n \telse\n \t\thci_dev_clear_flag(hdev, HCI_MESH);\n \n+\thdev->le_scan_interval = __le16_to_cpu(cp->period);\n+\thdev->le_scan_window = __le16_to_cpu(cp->window);\n+\n \tlen -= sizeof(*cp);\n \n \t/* If filters don't fit, forward all adv pkts */\n@@ -2167,6 +2171,7 @@ static int set_mesh(struct sock *sk, struct hci_dev *hdev, void *data, u16 len)\n {\n \tstruct mgmt_cp_set_mesh *cp = data;\n \tstruct mgmt_pending_cmd *cmd;\n+\t__u16 period, window;\n \tint err = 0;\n \n \tbt_dev_dbg(hdev, \"sock %p\", sk);\n@@ -2180,6 +2185,23 @@ static int set_mesh(struct sock *sk, struct hci_dev *hdev, void *data, u16 len)\n \t\treturn mgmt_cmd_status(sk, hdev->id, MGMT_OP_SET_MESH_RECEIVER,\n \t\t\t\t       MGMT_STATUS_INVALID_PARAMS);\n \n+\t/* Keep allowed ranges in sync with set_scan_params() */\n+\tperiod = __le16_to_cpu(cp->period);\n+\n+\tif (period < 0x0004 || period > 0x4000)\n+\t\treturn mgmt_cmd_status(sk, hdev->id, MGMT_OP_SET_MESH_RECEIVER,\n+\t\t\t\t       MGMT_STATUS_INVALID_PARAMS);\n+\n+\twindow = __le16_to_cpu(cp->window);\n+\n+\tif (window < 0x0004 || window > 0x4000)\n+\t\treturn mgmt_cmd_status(sk, hdev->id, MGMT_OP_SET_MESH_RECEIVER,\n+\t\t\t\t       MGMT_STATUS_INVALID_PARAMS);\n+\n+\tif (window > period)\n+\t\treturn mgmt_cmd_status(sk, hdev->id, MGMT_OP_SET_MESH_RECEIVER,\n+\t\t\t\t       MGMT_STATUS_INVALID_PARAMS);\n+\n \thci_dev_lock(hdev);\n \n \tcmd = mgmt_pending_add(sk, MGMT_OP_SET_MESH_RECEIVER, hdev, data, len);\n@@ -6432,6 +6454,7 @@ static int set_scan_params(struct sock *sk, struct hci_dev *hdev,\n \t\treturn mgmt_cmd_status(sk, hdev->id, MGMT_OP_SET_SCAN_PARAMS,\n \t\t\t\t       MGMT_STATUS_NOT_SUPPORTED);\n \n+\t/* Keep allowed ranges in sync with set_mesh() */\n \tinterval = __le16_to_cpu(cp->interval);\n \n \tif (interval < 0x0004 || interval > 0x4000)\ndiff --git a/net/ipv4/ip_input.c b/net/ipv4/ip_input.c\nindex 30a5e9460d00..5a49eb99e5c4 100644\n--- a/net/ipv4/ip_input.c\n+++ b/net/ipv4/ip_input.c\n@@ -319,8 +319,8 @@ static int ip_rcv_finish_core(struct net *net,\n \t\t\t      const struct sk_buff *hint)\n {\n \tconst struct iphdr *iph = ip_hdr(skb);\n-\tint err, drop_reason;\n \tstruct rtable *rt;\n+\tint drop_reason;\n \n \tif (ip_can_use_hint(skb, iph, hint)) {\n \t\tdrop_reason = ip_route_use_hint(skb, iph->daddr, iph->saddr,\n@@ -345,9 +345,10 @@ static int ip_rcv_finish_core(struct net *net,\n \t\t\tbreak;\n \t\tcase IPPROTO_UDP:\n \t\t\tif (READ_ONCE(net->ipv4.sysctl_udp_early_demux)) {\n-\t\t\t\terr = udp_v4_early_demux(skb);\n-\t\t\t\tif (unlikely(err))\n+\t\t\t\tdrop_reason = udp_v4_early_demux(skb);\n+\t\t\t\tif (unlikely(drop_reason))\n \t\t\t\t\tgoto drop_error;\n+\t\t\t\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n \n \t\t\t\t/* must reload iph, skb->head might have changed */\n \t\t\t\tiph = ip_hdr(skb);\ndiff --git a/net/rose/rose_route.c b/net/rose/rose_route.c\nindex 2dd6bd3a3011..b72bf8a08d48 100644\n--- a/net/rose/rose_route.c\n+++ b/net/rose/rose_route.c\n@@ -497,22 +497,15 @@ void rose_rt_device_down(struct net_device *dev)\n \t\t\tt         = rose_node;\n \t\t\trose_node = rose_node->next;\n \n-\t\t\tfor (i = 0; i < t->count; i++) {\n+\t\t\tfor (i = t->count - 1; i >= 0; i--) {\n \t\t\t\tif (t->neighbour[i] != s)\n \t\t\t\t\tcontinue;\n \n \t\t\t\tt->count--;\n \n-\t\t\t\tswitch (i) {\n-\t\t\t\tcase 0:\n-\t\t\t\t\tt->neighbour[0] = t->neighbour[1];\n-\t\t\t\t\tfallthrough;\n-\t\t\t\tcase 1:\n-\t\t\t\t\tt->neighbour[1] = t->neighbour[2];\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase 2:\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\t\tmemmove(&t->neighbour[i], &t->neighbour[i + 1],\n+\t\t\t\t\tsizeof(t->neighbour[0]) *\n+\t\t\t\t\t\t(t->count - i));\n \t\t\t}\n \n \t\t\tif (t->count <= 0)\ndiff --git a/net/sched/sch_api.c b/net/sched/sch_api.c\nindex c5e3673aadbe..d8a33486c511 100644\n--- a/net/sched/sch_api.c\n+++ b/net/sched/sch_api.c\n@@ -780,15 +780,12 @@ static u32 qdisc_alloc_handle(struct net_device *dev)\n \n void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)\n {\n-\tbool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED;\n \tconst struct Qdisc_class_ops *cops;\n \tunsigned long cl;\n \tu32 parentid;\n \tbool notify;\n \tint drops;\n \n-\tif (n == 0 && len == 0)\n-\t\treturn;\n \tdrops = max_t(int, n, 0);\n \trcu_read_lock();\n \twhile ((parentid = sch->parent)) {\n@@ -797,17 +794,8 @@ void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)\n \n \t\tif (sch->flags & TCQ_F_NOPARENT)\n \t\t\tbreak;\n-\t\t/* Notify parent qdisc only if child qdisc becomes empty.\n-\t\t *\n-\t\t * If child was empty even before update then backlog\n-\t\t * counter is screwed and we skip notification because\n-\t\t * parent class is already passive.\n-\t\t *\n-\t\t * If the original child was offloaded then it is allowed\n-\t\t * to be seem as empty, so the parent is notified anyway.\n-\t\t */\n-\t\tnotify = !sch->q.qlen && !WARN_ON_ONCE(!n &&\n-\t\t\t\t\t\t       !qdisc_is_offloaded);\n+\t\t/* Notify parent qdisc only if child qdisc becomes empty. */\n+\t\tnotify = !sch->q.qlen;\n \t\t/* TODO: perform the search on a per txq basis */\n \t\tsch = qdisc_lookup_rcu(qdisc_dev(sch), TC_H_MAJ(parentid));\n \t\tif (sch == NULL) {\n@@ -816,6 +804,9 @@ void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)\n \t\t}\n \t\tcops = sch->ops->cl_ops;\n \t\tif (notify && cops->qlen_notify) {\n+\t\t\t/* Note that qlen_notify must be idempotent as it may get called\n+\t\t\t * multiple times.\n+\t\t\t */\n \t\t\tcl = cops->find(sch, parentid);\n \t\t\tcops->qlen_notify(sch, cl);\n \t\t}\ndiff --git a/net/vmw_vsock/vmci_transport.c b/net/vmw_vsock/vmci_transport.c\nindex b370070194fa..7eccd6708d66 100644\n--- a/net/vmw_vsock/vmci_transport.c\n+++ b/net/vmw_vsock/vmci_transport.c\n@@ -119,6 +119,8 @@ vmci_transport_packet_init(struct vmci_transport_packet *pkt,\n \t\t\t   u16 proto,\n \t\t\t   struct vmci_handle handle)\n {\n+\tmemset(pkt, 0, sizeof(*pkt));\n+\n \t/* We register the stream control handler as an any cid handle so we\n \t * must always send from a source address of VMADDR_CID_ANY\n \t */\n@@ -131,8 +133,6 @@ vmci_transport_packet_init(struct vmci_transport_packet *pkt,\n \tpkt->type = type;\n \tpkt->src_port = src->svm_port;\n \tpkt->dst_port = dst->svm_port;\n-\tmemset(&pkt->proto, 0, sizeof(pkt->proto));\n-\tmemset(&pkt->_reserved2, 0, sizeof(pkt->_reserved2));\n \n \tswitch (pkt->type) {\n \tcase VMCI_TRANSPORT_PACKET_TYPE_INVALID:",
    "stats": {
      "insertions": 494,
      "deletions": 296,
      "files": 38
    }
  },
  {
    "sha": "d32e907d15f7257f69d38b4c829f87a79ecf8b7f",
    "message": "Merge tag 'xfs-fixes-6.16-rc5' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux\n\nPull xfs fixes from Carlos Maiolino:\n\n - Fix umount hang with unflushable inodes (and add new tracepoint used\n   for debugging this)\n\n - Fix ABBA deadlock in xfs_reclaim_inode() vs xfs_ifree_cluster()\n\n - Fix dquot buffer pin deadlock\n\n* tag 'xfs-fixes-6.16-rc5' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux:\n  xfs: add FALLOC_FL_ALLOCATE_RANGE to supported flags mask\n  xfs: fix unmount hang with unflushable inodes stuck in the AIL\n  xfs: factor out stale buffer item completion\n  xfs: rearrange code in xfs_buf_item.c\n  xfs: add tracepoints for stale pinned inode state debug\n  xfs: avoid dquot buffer pin deadlock\n  xfs: catch stale AGF/AGF metadata\n  xfs: xfs_ifree_cluster vs xfs_iflush_shutdown_abort deadlock\n  xfs: actually use the xfs_growfs_check_rtgeom tracepoint\n  xfs: Improve error handling in xfs_mru_cache_create()\n  xfs: move xfs_submit_zoned_bio a bit\n  xfs: use xfs_readonly_buftarg in xfs_remount_rw\n  xfs: remove NULL pointer checks in xfs_mru_cache_insert\n  xfs: check for shutdown before going to sleep in xfs_select_zone",
    "author": "Linus Torvalds",
    "date": "2025-07-03T09:00:04-07:00",
    "files_changed": [
      "fs/xfs/libxfs/xfs_alloc.c",
      "fs/xfs/libxfs/xfs_ialloc.c",
      "fs/xfs/xfs_buf.c",
      "fs/xfs/xfs_buf.h",
      "fs/xfs/xfs_buf_item.c",
      "fs/xfs/xfs_buf_item.h",
      "fs/xfs/xfs_dquot.c",
      "fs/xfs/xfs_file.c",
      "fs/xfs/xfs_icache.c",
      "fs/xfs/xfs_inode.c",
      "fs/xfs/xfs_inode_item.c",
      "fs/xfs/xfs_log_cil.c",
      "fs/xfs/xfs_mru_cache.c",
      "fs/xfs/xfs_qm.c",
      "fs/xfs/xfs_rtalloc.c",
      "fs/xfs/xfs_super.c",
      "fs/xfs/xfs_trace.h",
      "fs/xfs/xfs_trans.c",
      "fs/xfs/xfs_zone_alloc.c"
    ],
    "diff": "diff --git a/fs/xfs/libxfs/xfs_alloc.c b/fs/xfs/libxfs/xfs_alloc.c\nindex 7839efe050bf..000cc7f4a3ce 100644\n--- a/fs/xfs/libxfs/xfs_alloc.c\n+++ b/fs/xfs/libxfs/xfs_alloc.c\n@@ -3444,16 +3444,41 @@ xfs_alloc_read_agf(\n \n \t\tset_bit(XFS_AGSTATE_AGF_INIT, &pag->pag_opstate);\n \t}\n+\n #ifdef DEBUG\n-\telse if (!xfs_is_shutdown(mp)) {\n-\t\tASSERT(pag->pagf_freeblks == be32_to_cpu(agf->agf_freeblks));\n-\t\tASSERT(pag->pagf_btreeblks == be32_to_cpu(agf->agf_btreeblks));\n-\t\tASSERT(pag->pagf_flcount == be32_to_cpu(agf->agf_flcount));\n-\t\tASSERT(pag->pagf_longest == be32_to_cpu(agf->agf_longest));\n-\t\tASSERT(pag->pagf_bno_level == be32_to_cpu(agf->agf_bno_level));\n-\t\tASSERT(pag->pagf_cnt_level == be32_to_cpu(agf->agf_cnt_level));\n+\t/*\n+\t * It's possible for the AGF to be out of sync if the block device is\n+\t * silently dropping writes. This can happen in fstests with dmflakey\n+\t * enabled, which allows the buffer to be cleaned and reclaimed by\n+\t * memory pressure and then re-read from disk here. We will get a\n+\t * stale version of the AGF from disk, and nothing good can happen from\n+\t * here. Hence if we detect this situation, immediately shut down the\n+\t * filesystem.\n+\t *\n+\t * This can also happen if we are already in the middle of a forced\n+\t * shutdown, so don't bother checking if we are already shut down.\n+\t */\n+\tif (!xfs_is_shutdown(pag_mount(pag))) {\n+\t\tbool\tok = true;\n+\n+\t\tok &= pag->pagf_freeblks == be32_to_cpu(agf->agf_freeblks);\n+\t\tok &= pag->pagf_freeblks == be32_to_cpu(agf->agf_freeblks);\n+\t\tok &= pag->pagf_btreeblks == be32_to_cpu(agf->agf_btreeblks);\n+\t\tok &= pag->pagf_flcount == be32_to_cpu(agf->agf_flcount);\n+\t\tok &= pag->pagf_longest == be32_to_cpu(agf->agf_longest);\n+\t\tok &= pag->pagf_bno_level == be32_to_cpu(agf->agf_bno_level);\n+\t\tok &= pag->pagf_cnt_level == be32_to_cpu(agf->agf_cnt_level);\n+\n+\t\tif (XFS_IS_CORRUPT(pag_mount(pag), !ok)) {\n+\t\t\txfs_ag_mark_sick(pag, XFS_SICK_AG_AGF);\n+\t\t\txfs_trans_brelse(tp, agfbp);\n+\t\t\txfs_force_shutdown(pag_mount(pag),\n+\t\t\t\t\tSHUTDOWN_CORRUPT_ONDISK);\n+\t\t\treturn -EFSCORRUPTED;\n+\t\t}\n \t}\n-#endif\n+#endif /* DEBUG */\n+\n \tif (agfbpp)\n \t\t*agfbpp = agfbp;\n \telse\ndiff --git a/fs/xfs/libxfs/xfs_ialloc.c b/fs/xfs/libxfs/xfs_ialloc.c\nindex 0c47b5c6ca7d..750111634d9f 100644\n--- a/fs/xfs/libxfs/xfs_ialloc.c\n+++ b/fs/xfs/libxfs/xfs_ialloc.c\n@@ -2801,12 +2801,35 @@ xfs_ialloc_read_agi(\n \t\tset_bit(XFS_AGSTATE_AGI_INIT, &pag->pag_opstate);\n \t}\n \n+#ifdef DEBUG\n \t/*\n-\t * It's possible for these to be out of sync if\n-\t * we are in the middle of a forced shutdown.\n+\t * It's possible for the AGF to be out of sync if the block device is\n+\t * silently dropping writes. This can happen in fstests with dmflakey\n+\t * enabled, which allows the buffer to be cleaned and reclaimed by\n+\t * memory pressure and then re-read from disk here. We will get a\n+\t * stale version of the AGF from disk, and nothing good can happen from\n+\t * here. Hence if we detect this situation, immediately shut down the\n+\t * filesystem.\n+\t *\n+\t * This can also happen if we are already in the middle of a forced\n+\t * shutdown, so don't bother checking if we are already shut down.\n \t */\n-\tASSERT(pag->pagi_freecount == be32_to_cpu(agi->agi_freecount) ||\n-\t\txfs_is_shutdown(pag_mount(pag)));\n+\tif (!xfs_is_shutdown(pag_mount(pag))) {\n+\t\tbool\tok = true;\n+\n+\t\tok &= pag->pagi_freecount == be32_to_cpu(agi->agi_freecount);\n+\t\tok &= pag->pagi_count == be32_to_cpu(agi->agi_count);\n+\n+\t\tif (XFS_IS_CORRUPT(pag_mount(pag), !ok)) {\n+\t\t\txfs_ag_mark_sick(pag, XFS_SICK_AG_AGI);\n+\t\t\txfs_trans_brelse(tp, agibp);\n+\t\t\txfs_force_shutdown(pag_mount(pag),\n+\t\t\t\t\tSHUTDOWN_CORRUPT_ONDISK);\n+\t\t\treturn -EFSCORRUPTED;\n+\t\t}\n+\t}\n+#endif /* DEBUG */\n+\n \tif (agibpp)\n \t\t*agibpp = agibp;\n \telse\ndiff --git a/fs/xfs/xfs_buf.c b/fs/xfs/xfs_buf.c\nindex 8af83bd161f9..ba5bd6031ece 100644\n--- a/fs/xfs/xfs_buf.c\n+++ b/fs/xfs/xfs_buf.c\n@@ -2082,44 +2082,6 @@ xfs_buf_delwri_submit(\n \treturn error;\n }\n \n-/*\n- * Push a single buffer on a delwri queue.\n- *\n- * The purpose of this function is to submit a single buffer of a delwri queue\n- * and return with the buffer still on the original queue.\n- *\n- * The buffer locking and queue management logic between _delwri_pushbuf() and\n- * _delwri_queue() guarantee that the buffer cannot be queued to another list\n- * before returning.\n- */\n-int\n-xfs_buf_delwri_pushbuf(\n-\tstruct xfs_buf\t\t*bp,\n-\tstruct list_head\t*buffer_list)\n-{\n-\tint\t\t\terror;\n-\n-\tASSERT(bp->b_flags & _XBF_DELWRI_Q);\n-\n-\ttrace_xfs_buf_delwri_pushbuf(bp, _RET_IP_);\n-\n-\txfs_buf_lock(bp);\n-\tbp->b_flags &= ~(_XBF_DELWRI_Q | XBF_ASYNC);\n-\tbp->b_flags |= XBF_WRITE;\n-\txfs_buf_submit(bp);\n-\n-\t/*\n-\t * The buffer is now locked, under I/O but still on the original delwri\n-\t * queue. Wait for I/O completion, restore the DELWRI_Q flag and\n-\t * return with the buffer unlocked and still on the original queue.\n-\t */\n-\terror = xfs_buf_iowait(bp);\n-\tbp->b_flags |= _XBF_DELWRI_Q;\n-\txfs_buf_unlock(bp);\n-\n-\treturn error;\n-}\n-\n void xfs_buf_set_ref(struct xfs_buf *bp, int lru_ref)\n {\n \t/*\ndiff --git a/fs/xfs/xfs_buf.h b/fs/xfs/xfs_buf.h\nindex 9d2ab567cf81..15fc56948346 100644\n--- a/fs/xfs/xfs_buf.h\n+++ b/fs/xfs/xfs_buf.h\n@@ -326,7 +326,6 @@ extern bool xfs_buf_delwri_queue(struct xfs_buf *, struct list_head *);\n void xfs_buf_delwri_queue_here(struct xfs_buf *bp, struct list_head *bl);\n extern int xfs_buf_delwri_submit(struct list_head *);\n extern int xfs_buf_delwri_submit_nowait(struct list_head *);\n-extern int xfs_buf_delwri_pushbuf(struct xfs_buf *, struct list_head *);\n \n static inline xfs_daddr_t xfs_buf_daddr(struct xfs_buf *bp)\n {\ndiff --git a/fs/xfs/xfs_buf_item.c b/fs/xfs/xfs_buf_item.c\nindex 90139e0f3271..7fc54725c5f6 100644\n--- a/fs/xfs/xfs_buf_item.c\n+++ b/fs/xfs/xfs_buf_item.c\n@@ -32,6 +32,61 @@ static inline struct xfs_buf_log_item *BUF_ITEM(struct xfs_log_item *lip)\n \treturn container_of(lip, struct xfs_buf_log_item, bli_item);\n }\n \n+static void\n+xfs_buf_item_get_format(\n+\tstruct xfs_buf_log_item\t*bip,\n+\tint\t\t\tcount)\n+{\n+\tASSERT(bip->bli_formats == NULL);\n+\tbip->bli_format_count = count;\n+\n+\tif (count == 1) {\n+\t\tbip->bli_formats = &bip->__bli_format;\n+\t\treturn;\n+\t}\n+\n+\tbip->bli_formats = kzalloc(count * sizeof(struct xfs_buf_log_format),\n+\t\t\t\tGFP_KERNEL | __GFP_NOFAIL);\n+}\n+\n+static void\n+xfs_buf_item_free_format(\n+\tstruct xfs_buf_log_item\t*bip)\n+{\n+\tif (bip->bli_formats != &bip->__bli_format) {\n+\t\tkfree(bip->bli_formats);\n+\t\tbip->bli_formats = NULL;\n+\t}\n+}\n+\n+static void\n+xfs_buf_item_free(\n+\tstruct xfs_buf_log_item\t*bip)\n+{\n+\txfs_buf_item_free_format(bip);\n+\tkvfree(bip->bli_item.li_lv_shadow);\n+\tkmem_cache_free(xfs_buf_item_cache, bip);\n+}\n+\n+/*\n+ * xfs_buf_item_relse() is called when the buf log item is no longer needed.\n+ */\n+static void\n+xfs_buf_item_relse(\n+\tstruct xfs_buf_log_item\t*bip)\n+{\n+\tstruct xfs_buf\t\t*bp = bip->bli_buf;\n+\n+\ttrace_xfs_buf_item_relse(bp, _RET_IP_);\n+\n+\tASSERT(!test_bit(XFS_LI_IN_AIL, &bip->bli_item.li_flags));\n+\tASSERT(atomic_read(&bip->bli_refcount) == 0);\n+\n+\tbp->b_log_item = NULL;\n+\txfs_buf_rele(bp);\n+\txfs_buf_item_free(bip);\n+}\n+\n /* Is this log iovec plausibly large enough to contain the buffer log format? */\n bool\n xfs_buf_log_check_iovec(\n@@ -389,6 +444,42 @@ xfs_buf_item_pin(\n \tatomic_inc(&bip->bli_buf->b_pin_count);\n }\n \n+/*\n+ * For a stale BLI, process all the necessary completions that must be\n+ * performed when the final BLI reference goes away. The buffer will be\n+ * referenced and locked here - we return to the caller with the buffer still\n+ * referenced and locked for them to finalise processing of the buffer.\n+ */\n+static void\n+xfs_buf_item_finish_stale(\n+\tstruct xfs_buf_log_item\t*bip)\n+{\n+\tstruct xfs_buf\t\t*bp = bip->bli_buf;\n+\tstruct xfs_log_item\t*lip = &bip->bli_item;\n+\n+\tASSERT(bip->bli_flags & XFS_BLI_STALE);\n+\tASSERT(xfs_buf_islocked(bp));\n+\tASSERT(bp->b_flags & XBF_STALE);\n+\tASSERT(bip->__bli_format.blf_flags & XFS_BLF_CANCEL);\n+\tASSERT(list_empty(&lip->li_trans));\n+\tASSERT(!bp->b_transp);\n+\n+\tif (bip->bli_flags & XFS_BLI_STALE_INODE) {\n+\t\txfs_buf_item_done(bp);\n+\t\txfs_buf_inode_iodone(bp);\n+\t\tASSERT(list_empty(&bp->b_li_list));\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * We may or may not be on the AIL here, xfs_trans_ail_delete() will do\n+\t * the right thing regardless of the situation in which we are called.\n+\t */\n+\txfs_trans_ail_delete(lip, SHUTDOWN_LOG_IO_ERROR);\n+\txfs_buf_item_relse(bip);\n+\tASSERT(bp->b_log_item == NULL);\n+}\n+\n /*\n  * This is called to unpin the buffer associated with the buf log item which was\n  * previously pinned with a call to xfs_buf_item_pin().  We enter this function\n@@ -438,13 +529,6 @@ xfs_buf_item_unpin(\n \t}\n \n \tif (stale) {\n-\t\tASSERT(bip->bli_flags & XFS_BLI_STALE);\n-\t\tASSERT(xfs_buf_islocked(bp));\n-\t\tASSERT(bp->b_flags & XBF_STALE);\n-\t\tASSERT(bip->__bli_format.blf_flags & XFS_BLF_CANCEL);\n-\t\tASSERT(list_empty(&lip->li_trans));\n-\t\tASSERT(!bp->b_transp);\n-\n \t\ttrace_xfs_buf_item_unpin_stale(bip);\n \n \t\t/*\n@@ -455,22 +539,7 @@ xfs_buf_item_unpin(\n \t\t * processing is complete.\n \t\t */\n \t\txfs_buf_rele(bp);\n-\n-\t\t/*\n-\t\t * If we get called here because of an IO error, we may or may\n-\t\t * not have the item on the AIL. xfs_trans_ail_delete() will\n-\t\t * take care of that situation. xfs_trans_ail_delete() drops\n-\t\t * the AIL lock.\n-\t\t */\n-\t\tif (bip->bli_flags & XFS_BLI_STALE_INODE) {\n-\t\t\txfs_buf_item_done(bp);\n-\t\t\txfs_buf_inode_iodone(bp);\n-\t\t\tASSERT(list_empty(&bp->b_li_list));\n-\t\t} else {\n-\t\t\txfs_trans_ail_delete(lip, SHUTDOWN_LOG_IO_ERROR);\n-\t\t\txfs_buf_item_relse(bp);\n-\t\t\tASSERT(bp->b_log_item == NULL);\n-\t\t}\n+\t\txfs_buf_item_finish_stale(bip);\n \t\txfs_buf_relse(bp);\n \t\treturn;\n \t}\n@@ -543,43 +612,42 @@ xfs_buf_item_push(\n  * Drop the buffer log item refcount and take appropriate action. This helper\n  * determines whether the bli must be freed or not, since a decrement to zero\n  * does not necessarily mean the bli is unused.\n- *\n- * Return true if the bli is freed, false otherwise.\n  */\n-bool\n+void\n xfs_buf_item_put(\n \tstruct xfs_buf_log_item\t*bip)\n {\n-\tstruct xfs_log_item\t*lip = &bip->bli_item;\n-\tbool\t\t\taborted;\n-\tbool\t\t\tdirty;\n+\n+\tASSERT(xfs_buf_islocked(bip->bli_buf));\n \n \t/* drop the bli ref and return if it wasn't the last one */\n \tif (!atomic_dec_and_test(&bip->bli_refcount))\n-\t\treturn false;\n+\t\treturn;\n \n-\t/*\n-\t * We dropped the last ref and must free the item if clean or aborted.\n-\t * If the bli is dirty and non-aborted, the buffer was clean in the\n-\t * transaction but still awaiting writeback from previous changes. In\n-\t * that case, the bli is freed on buffer writeback completion.\n-\t */\n-\taborted = test_bit(XFS_LI_ABORTED, &lip->li_flags) ||\n-\t\t\txlog_is_shutdown(lip->li_log);\n-\tdirty = bip->bli_flags & XFS_BLI_DIRTY;\n-\tif (dirty && !aborted)\n-\t\treturn false;\n+\t/* If the BLI is in the AIL, then it is still dirty and in use */\n+\tif (test_bit(XFS_LI_IN_AIL, &bip->bli_item.li_flags)) {\n+\t\tASSERT(bip->bli_flags & XFS_BLI_DIRTY);\n+\t\treturn;\n+\t}\n \n \t/*\n-\t * The bli is aborted or clean. An aborted item may be in the AIL\n-\t * regardless of dirty state.  For example, consider an aborted\n-\t * transaction that invalidated a dirty bli and cleared the dirty\n-\t * state.\n+\t * In shutdown conditions, we can be asked to free a dirty BLI that\n+\t * isn't in the AIL. This can occur due to a checkpoint aborting a BLI\n+\t * instead of inserting it into the AIL at checkpoint IO completion. If\n+\t * there's another bli reference (e.g. a btree cursor holds a clean\n+\t * reference) and it is released via xfs_trans_brelse(), we can get here\n+\t * with that aborted, dirty BLI. In this case, it is safe to free the\n+\t * dirty BLI immediately, as it is not in the AIL and there are no\n+\t * other references to it.\n+\t *\n+\t * We should never get here with a stale BLI via that path as\n+\t * xfs_trans_brelse() specifically holds onto stale buffers rather than\n+\t * releasing them.\n \t */\n-\tif (aborted)\n-\t\txfs_trans_ail_delete(lip, 0);\n-\txfs_buf_item_relse(bip->bli_buf);\n-\treturn true;\n+\tASSERT(!(bip->bli_flags & XFS_BLI_DIRTY) ||\n+\t\t\ttest_bit(XFS_LI_ABORTED, &bip->bli_item.li_flags));\n+\tASSERT(!(bip->bli_flags & XFS_BLI_STALE));\n+\txfs_buf_item_relse(bip);\n }\n \n /*\n@@ -600,6 +668,15 @@ xfs_buf_item_put(\n  * if necessary but do not unlock the buffer.  This is for support of\n  * xfs_trans_bhold(). Make sure the XFS_BLI_HOLD field is cleared if we don't\n  * free the item.\n+ *\n+ * If the XFS_BLI_STALE flag is set, the last reference to the BLI *must*\n+ * perform a completion abort of any objects attached to the buffer for IO\n+ * tracking purposes. This generally only happens in shutdown situations,\n+ * normally xfs_buf_item_unpin() will drop the last BLI reference and perform\n+ * completion processing. However, because transaction completion can race with\n+ * checkpoint completion during a shutdown, this release context may end up\n+ * being the last active reference to the BLI and so needs to perform this\n+ * cleanup.\n  */\n STATIC void\n xfs_buf_item_release(\n@@ -607,18 +684,19 @@ xfs_buf_item_release(\n {\n \tstruct xfs_buf_log_item\t*bip = BUF_ITEM(lip);\n \tstruct xfs_buf\t\t*bp = bip->bli_buf;\n-\tbool\t\t\treleased;\n \tbool\t\t\thold = bip->bli_flags & XFS_BLI_HOLD;\n \tbool\t\t\tstale = bip->bli_flags & XFS_BLI_STALE;\n-#if defined(DEBUG) || defined(XFS_WARN)\n-\tbool\t\t\tordered = bip->bli_flags & XFS_BLI_ORDERED;\n-\tbool\t\t\tdirty = bip->bli_flags & XFS_BLI_DIRTY;\n \tbool\t\t\taborted = test_bit(XFS_LI_ABORTED,\n \t\t\t\t\t\t   &lip->li_flags);\n+\tbool\t\t\tdirty = bip->bli_flags & XFS_BLI_DIRTY;\n+#if defined(DEBUG) || defined(XFS_WARN)\n+\tbool\t\t\tordered = bip->bli_flags & XFS_BLI_ORDERED;\n #endif\n \n \ttrace_xfs_buf_item_release(bip);\n \n+\tASSERT(xfs_buf_islocked(bp));\n+\n \t/*\n \t * The bli dirty state should match whether the blf has logged segments\n \t * except for ordered buffers, where only the bli should be dirty.\n@@ -634,16 +712,56 @@ xfs_buf_item_release(\n \tbp->b_transp = NULL;\n \tbip->bli_flags &= ~(XFS_BLI_LOGGED | XFS_BLI_HOLD | XFS_BLI_ORDERED);\n \n+\t/* If there are other references, then we have nothing to do. */\n+\tif (!atomic_dec_and_test(&bip->bli_refcount))\n+\t\tgoto out_release;\n+\n+\t/*\n+\t * Stale buffer completion frees the BLI, unlocks and releases the\n+\t * buffer. Neither the BLI or buffer are safe to reference after this\n+\t * call, so there's nothing more we need to do here.\n+\t *\n+\t * If we get here with a stale buffer and references to the BLI remain,\n+\t * we must not unlock the buffer as the last BLI reference owns lock\n+\t * context, not us.\n+\t */\n+\tif (stale) {\n+\t\txfs_buf_item_finish_stale(bip);\n+\t\txfs_buf_relse(bp);\n+\t\tASSERT(!hold);\n+\t\treturn;\n+\t}\n+\n \t/*\n-\t * Unref the item and unlock the buffer unless held or stale. Stale\n-\t * buffers remain locked until final unpin unless the bli is freed by\n-\t * the unref call. The latter implies shutdown because buffer\n-\t * invalidation dirties the bli and transaction.\n+\t * Dirty or clean, aborted items are done and need to be removed from\n+\t * the AIL and released. This frees the BLI, but leaves the buffer\n+\t * locked and referenced.\n \t */\n-\treleased = xfs_buf_item_put(bip);\n-\tif (hold || (stale && !released))\n+\tif (aborted || xlog_is_shutdown(lip->li_log)) {\n+\t\tASSERT(list_empty(&bip->bli_buf->b_li_list));\n+\t\txfs_buf_item_done(bp);\n+\t\tgoto out_release;\n+\t}\n+\n+\t/*\n+\t * Clean, unreferenced BLIs can be immediately freed, leaving the buffer\n+\t * locked and referenced.\n+\t *\n+\t * Dirty, unreferenced BLIs *must* be in the AIL awaiting writeback.\n+\t */\n+\tif (!dirty)\n+\t\txfs_buf_item_relse(bip);\n+\telse\n+\t\tASSERT(test_bit(XFS_LI_IN_AIL, &lip->li_flags));\n+\n+\t/* Not safe to reference the BLI from here */\n+out_release:\n+\t/*\n+\t * If we get here with a stale buffer, we must not unlock the\n+\t * buffer as the last BLI reference owns lock context, not us.\n+\t */\n+\tif (stale || hold)\n \t\treturn;\n-\tASSERT(!stale || aborted);\n \txfs_buf_relse(bp);\n }\n \n@@ -729,33 +847,6 @@ static const struct xfs_item_ops xfs_buf_item_ops = {\n \t.iop_push\t= xfs_buf_item_push,\n };\n \n-STATIC void\n-xfs_buf_item_get_format(\n-\tstruct xfs_buf_log_item\t*bip,\n-\tint\t\t\tcount)\n-{\n-\tASSERT(bip->bli_formats == NULL);\n-\tbip->bli_format_count = count;\n-\n-\tif (count == 1) {\n-\t\tbip->bli_formats = &bip->__bli_format;\n-\t\treturn;\n-\t}\n-\n-\tbip->bli_formats = kzalloc(count * sizeof(struct xfs_buf_log_format),\n-\t\t\t\tGFP_KERNEL | __GFP_NOFAIL);\n-}\n-\n-STATIC void\n-xfs_buf_item_free_format(\n-\tstruct xfs_buf_log_item\t*bip)\n-{\n-\tif (bip->bli_formats != &bip->__bli_format) {\n-\t\tkfree(bip->bli_formats);\n-\t\tbip->bli_formats = NULL;\n-\t}\n-}\n-\n /*\n  * Allocate a new buf log item to go with the given buffer.\n  * Set the buffer's b_log_item field to point to the new\n@@ -976,34 +1067,6 @@ xfs_buf_item_dirty_format(\n \treturn false;\n }\n \n-STATIC void\n-xfs_buf_item_free(\n-\tstruct xfs_buf_log_item\t*bip)\n-{\n-\txfs_buf_item_free_format(bip);\n-\tkvfree(bip->bli_item.li_lv_shadow);\n-\tkmem_cache_free(xfs_buf_item_cache, bip);\n-}\n-\n-/*\n- * xfs_buf_item_relse() is called when the buf log item is no longer needed.\n- */\n-void\n-xfs_buf_item_relse(\n-\tstruct xfs_buf\t*bp)\n-{\n-\tstruct xfs_buf_log_item\t*bip = bp->b_log_item;\n-\n-\ttrace_xfs_buf_item_relse(bp, _RET_IP_);\n-\tASSERT(!test_bit(XFS_LI_IN_AIL, &bip->bli_item.li_flags));\n-\n-\tif (atomic_read(&bip->bli_refcount))\n-\t\treturn;\n-\tbp->b_log_item = NULL;\n-\txfs_buf_rele(bp);\n-\txfs_buf_item_free(bip);\n-}\n-\n void\n xfs_buf_item_done(\n \tstruct xfs_buf\t\t*bp)\n@@ -1023,5 +1086,5 @@ xfs_buf_item_done(\n \txfs_trans_ail_delete(&bp->b_log_item->bli_item,\n \t\t\t     (bp->b_flags & _XBF_LOGRECOVERY) ? 0 :\n \t\t\t     SHUTDOWN_CORRUPT_INCORE);\n-\txfs_buf_item_relse(bp);\n+\txfs_buf_item_relse(bp->b_log_item);\n }\ndiff --git a/fs/xfs/xfs_buf_item.h b/fs/xfs/xfs_buf_item.h\nindex e10e324cd245..416890b84f8c 100644\n--- a/fs/xfs/xfs_buf_item.h\n+++ b/fs/xfs/xfs_buf_item.h\n@@ -49,8 +49,7 @@ struct xfs_buf_log_item {\n \n int\txfs_buf_item_init(struct xfs_buf *, struct xfs_mount *);\n void\txfs_buf_item_done(struct xfs_buf *bp);\n-void\txfs_buf_item_relse(struct xfs_buf *);\n-bool\txfs_buf_item_put(struct xfs_buf_log_item *);\n+void\txfs_buf_item_put(struct xfs_buf_log_item *bip);\n void\txfs_buf_item_log(struct xfs_buf_log_item *, uint, uint);\n bool\txfs_buf_item_dirty_format(struct xfs_buf_log_item *);\n void\txfs_buf_inode_iodone(struct xfs_buf *);\ndiff --git a/fs/xfs/xfs_dquot.c b/fs/xfs/xfs_dquot.c\nindex b4e32f0860b7..0bd8022e47b4 100644\n--- a/fs/xfs/xfs_dquot.c\n+++ b/fs/xfs/xfs_dquot.c\n@@ -1398,11 +1398,9 @@ xfs_qm_dqflush(\n \n \tASSERT(XFS_DQ_IS_LOCKED(dqp));\n \tASSERT(!completion_done(&dqp->q_flush));\n+\tASSERT(atomic_read(&dqp->q_pincount) == 0);\n \n \ttrace_xfs_dqflush(dqp);\n-\n-\txfs_qm_dqunpin_wait(dqp);\n-\n \tfa = xfs_qm_dqflush_check(dqp);\n \tif (fa) {\n \t\txfs_alert(mp, \"corrupt dquot ID 0x%x in memory at %pS\",\ndiff --git a/fs/xfs/xfs_file.c b/fs/xfs/xfs_file.c\nindex 48254a72071b..0b41b18debf3 100644\n--- a/fs/xfs/xfs_file.c\n+++ b/fs/xfs/xfs_file.c\n@@ -1335,9 +1335,10 @@ xfs_falloc_allocate_range(\n }\n \n #define\tXFS_FALLOC_FL_SUPPORTED\t\t\t\t\t\t\\\n-\t\t(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |\t\t\\\n-\t\t FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |\t\\\n-\t\t FALLOC_FL_INSERT_RANGE | FALLOC_FL_UNSHARE_RANGE)\n+\t\t(FALLOC_FL_ALLOCATE_RANGE | FALLOC_FL_KEEP_SIZE |\t\\\n+\t\t FALLOC_FL_PUNCH_HOLE |\tFALLOC_FL_COLLAPSE_RANGE |\t\\\n+\t\t FALLOC_FL_ZERO_RANGE |\tFALLOC_FL_INSERT_RANGE |\t\\\n+\t\t FALLOC_FL_UNSHARE_RANGE)\n \n STATIC long\n __xfs_file_fallocate(\ndiff --git a/fs/xfs/xfs_icache.c b/fs/xfs/xfs_icache.c\nindex 726e29b837e6..bbc2f2973dcc 100644\n--- a/fs/xfs/xfs_icache.c\n+++ b/fs/xfs/xfs_icache.c\n@@ -979,7 +979,15 @@ xfs_reclaim_inode(\n \t */\n \tif (xlog_is_shutdown(ip->i_mount->m_log)) {\n \t\txfs_iunpin_wait(ip);\n+\t\t/*\n+\t\t * Avoid a ABBA deadlock on the inode cluster buffer vs\n+\t\t * concurrent xfs_ifree_cluster() trying to mark the inode\n+\t\t * stale. We don't need the inode locked to run the flush abort\n+\t\t * code, but the flush abort needs to lock the cluster buffer.\n+\t\t */\n+\t\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n \t\txfs_iflush_shutdown_abort(ip);\n+\t\txfs_ilock(ip, XFS_ILOCK_EXCL);\n \t\tgoto reclaim;\n \t}\n \tif (xfs_ipincount(ip))\ndiff --git a/fs/xfs/xfs_inode.c b/fs/xfs/xfs_inode.c\nindex ee3e0f284287..761a996a857c 100644\n--- a/fs/xfs/xfs_inode.c\n+++ b/fs/xfs/xfs_inode.c\n@@ -1635,7 +1635,7 @@ xfs_ifree_mark_inode_stale(\n \tiip = ip->i_itemp;\n \tif (__xfs_iflags_test(ip, XFS_IFLUSHING)) {\n \t\tASSERT(!list_empty(&iip->ili_item.li_bio_list));\n-\t\tASSERT(iip->ili_last_fields);\n+\t\tASSERT(iip->ili_last_fields || xlog_is_shutdown(mp->m_log));\n \t\tgoto out_iunlock;\n \t}\n \ndiff --git a/fs/xfs/xfs_inode_item.c b/fs/xfs/xfs_inode_item.c\nindex c6cb0b6b9e46..285e27ff89e2 100644\n--- a/fs/xfs/xfs_inode_item.c\n+++ b/fs/xfs/xfs_inode_item.c\n@@ -758,11 +758,14 @@ xfs_inode_item_push(\n \t\t * completed and items removed from the AIL before the next push\n \t\t * attempt.\n \t\t */\n+\t\ttrace_xfs_inode_push_stale(ip, _RET_IP_);\n \t\treturn XFS_ITEM_PINNED;\n \t}\n \n-\tif (xfs_ipincount(ip) > 0 || xfs_buf_ispinned(bp))\n+\tif (xfs_ipincount(ip) > 0 || xfs_buf_ispinned(bp)) {\n+\t\ttrace_xfs_inode_push_pinned(ip, _RET_IP_);\n \t\treturn XFS_ITEM_PINNED;\n+\t}\n \n \tif (xfs_iflags_test(ip, XFS_IFLUSHING))\n \t\treturn XFS_ITEM_FLUSHING;\ndiff --git a/fs/xfs/xfs_log_cil.c b/fs/xfs/xfs_log_cil.c\nindex f66d2d430e4f..a80cb6b9969a 100644\n--- a/fs/xfs/xfs_log_cil.c\n+++ b/fs/xfs/xfs_log_cil.c\n@@ -793,8 +793,10 @@ xlog_cil_ail_insert(\n \t\tstruct xfs_log_item\t*lip = lv->lv_item;\n \t\txfs_lsn_t\t\titem_lsn;\n \n-\t\tif (aborted)\n+\t\tif (aborted) {\n+\t\t\ttrace_xlog_ail_insert_abort(lip);\n \t\t\tset_bit(XFS_LI_ABORTED, &lip->li_flags);\n+\t\t}\n \n \t\tif (lip->li_ops->flags & XFS_ITEM_RELEASE_WHEN_COMMITTED) {\n \t\t\tlip->li_ops->iop_release(lip);\ndiff --git a/fs/xfs/xfs_mru_cache.c b/fs/xfs/xfs_mru_cache.c\nindex 08443ceec329..866c71d9fbae 100644\n--- a/fs/xfs/xfs_mru_cache.c\n+++ b/fs/xfs/xfs_mru_cache.c\n@@ -320,7 +320,7 @@ xfs_mru_cache_create(\n \txfs_mru_cache_free_func_t free_func)\n {\n \tstruct xfs_mru_cache\t*mru = NULL;\n-\tint\t\t\terr = 0, grp;\n+\tint\t\t\tgrp;\n \tunsigned int\t\tgrp_time;\n \n \tif (mrup)\n@@ -341,8 +341,8 @@ xfs_mru_cache_create(\n \tmru->lists = kzalloc(mru->grp_count * sizeof(*mru->lists),\n \t\t\t\tGFP_KERNEL | __GFP_NOFAIL);\n \tif (!mru->lists) {\n-\t\terr = -ENOMEM;\n-\t\tgoto exit;\n+\t\tkfree(mru);\n+\t\treturn -ENOMEM;\n \t}\n \n \tfor (grp = 0; grp < mru->grp_count; grp++)\n@@ -361,14 +361,7 @@ xfs_mru_cache_create(\n \tmru->free_func = free_func;\n \tmru->data = data;\n \t*mrup = mru;\n-\n-exit:\n-\tif (err && mru && mru->lists)\n-\t\tkfree(mru->lists);\n-\tif (err && mru)\n-\t\tkfree(mru);\n-\n-\treturn err;\n+\treturn 0;\n }\n \n /*\n@@ -425,10 +418,6 @@ xfs_mru_cache_insert(\n {\n \tint\t\t\terror = -EINVAL;\n \n-\tASSERT(mru && mru->lists);\n-\tif (!mru || !mru->lists)\n-\t\tgoto out_free;\n-\n \terror = -ENOMEM;\n \tif (radix_tree_preload(GFP_KERNEL))\n \t\tgoto out_free;\ndiff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c\nindex 417439b58785..fa135ac26471 100644\n--- a/fs/xfs/xfs_qm.c\n+++ b/fs/xfs/xfs_qm.c\n@@ -134,6 +134,7 @@ xfs_qm_dqpurge(\n \n \tdqp->q_flags |= XFS_DQFLAG_FREEING;\n \n+\txfs_qm_dqunpin_wait(dqp);\n \txfs_dqflock(dqp);\n \n \t/*\n@@ -465,6 +466,7 @@ xfs_qm_dquot_isolate(\n \tstruct xfs_dquot\t*dqp = container_of(item,\n \t\t\t\t\t\tstruct xfs_dquot, q_lru);\n \tstruct xfs_qm_isolate\t*isol = arg;\n+\tenum lru_status\t\tret = LRU_SKIP;\n \n \tif (!xfs_dqlock_nowait(dqp))\n \t\tgoto out_miss_busy;\n@@ -477,6 +479,16 @@ xfs_qm_dquot_isolate(\n \tif (dqp->q_flags & XFS_DQFLAG_FREEING)\n \t\tgoto out_miss_unlock;\n \n+\t/*\n+\t * If the dquot is pinned or dirty, rotate it to the end of the LRU to\n+\t * give some time for it to be cleaned before we try to isolate it\n+\t * again.\n+\t */\n+\tret = LRU_ROTATE;\n+\tif (XFS_DQ_IS_DIRTY(dqp) || atomic_read(&dqp->q_pincount) > 0) {\n+\t\tgoto out_miss_unlock;\n+\t}\n+\n \t/*\n \t * This dquot has acquired a reference in the meantime remove it from\n \t * the freelist and try again.\n@@ -492,41 +504,14 @@ xfs_qm_dquot_isolate(\n \t}\n \n \t/*\n-\t * If the dquot is dirty, flush it. If it's already being flushed, just\n-\t * skip it so there is time for the IO to complete before we try to\n-\t * reclaim it again on the next LRU pass.\n+\t * The dquot may still be under IO, in which case the flush lock will be\n+\t * held. If we can't get the flush lock now, just skip over the dquot as\n+\t * if it was dirty.\n \t */\n \tif (!xfs_dqflock_nowait(dqp))\n \t\tgoto out_miss_unlock;\n \n-\tif (XFS_DQ_IS_DIRTY(dqp)) {\n-\t\tstruct xfs_buf\t*bp = NULL;\n-\t\tint\t\terror;\n-\n-\t\ttrace_xfs_dqreclaim_dirty(dqp);\n-\n-\t\t/* we have to drop the LRU lock to flush the dquot */\n-\t\tspin_unlock(&lru->lock);\n-\n-\t\terror = xfs_dquot_use_attached_buf(dqp, &bp);\n-\t\tif (!bp || error == -EAGAIN) {\n-\t\t\txfs_dqfunlock(dqp);\n-\t\t\tgoto out_unlock_dirty;\n-\t\t}\n-\n-\t\t/*\n-\t\t * dqflush completes dqflock on error, and the delwri ioend\n-\t\t * does it on success.\n-\t\t */\n-\t\terror = xfs_qm_dqflush(dqp, bp);\n-\t\tif (error)\n-\t\t\tgoto out_unlock_dirty;\n-\n-\t\txfs_buf_delwri_queue(bp, &isol->buffers);\n-\t\txfs_buf_relse(bp);\n-\t\tgoto out_unlock_dirty;\n-\t}\n-\n+\tASSERT(!XFS_DQ_IS_DIRTY(dqp));\n \txfs_dquot_detach_buf(dqp);\n \txfs_dqfunlock(dqp);\n \n@@ -548,13 +533,7 @@ xfs_qm_dquot_isolate(\n out_miss_busy:\n \ttrace_xfs_dqreclaim_busy(dqp);\n \tXFS_STATS_INC(dqp->q_mount, xs_qm_dqreclaim_misses);\n-\treturn LRU_SKIP;\n-\n-out_unlock_dirty:\n-\ttrace_xfs_dqreclaim_busy(dqp);\n-\tXFS_STATS_INC(dqp->q_mount, xs_qm_dqreclaim_misses);\n-\txfs_dqunlock(dqp);\n-\treturn LRU_RETRY;\n+\treturn ret;\n }\n \n static unsigned long\n@@ -1486,7 +1465,6 @@ xfs_qm_flush_one(\n \tstruct xfs_dquot\t*dqp,\n \tvoid\t\t\t*data)\n {\n-\tstruct xfs_mount\t*mp = dqp->q_mount;\n \tstruct list_head\t*buffer_list = data;\n \tstruct xfs_buf\t\t*bp = NULL;\n \tint\t\t\terror = 0;\n@@ -1497,34 +1475,8 @@ xfs_qm_flush_one(\n \tif (!XFS_DQ_IS_DIRTY(dqp))\n \t\tgoto out_unlock;\n \n-\t/*\n-\t * The only way the dquot is already flush locked by the time quotacheck\n-\t * gets here is if reclaim flushed it before the dqadjust walk dirtied\n-\t * it for the final time. Quotacheck collects all dquot bufs in the\n-\t * local delwri queue before dquots are dirtied, so reclaim can't have\n-\t * possibly queued it for I/O. The only way out is to push the buffer to\n-\t * cycle the flush lock.\n-\t */\n-\tif (!xfs_dqflock_nowait(dqp)) {\n-\t\t/* buf is pinned in-core by delwri list */\n-\t\terror = xfs_buf_incore(mp->m_ddev_targp, dqp->q_blkno,\n-\t\t\t\tmp->m_quotainfo->qi_dqchunklen, 0, &bp);\n-\t\tif (error)\n-\t\t\tgoto out_unlock;\n-\n-\t\tif (!(bp->b_flags & _XBF_DELWRI_Q)) {\n-\t\t\terror = -EAGAIN;\n-\t\t\txfs_buf_relse(bp);\n-\t\t\tgoto out_unlock;\n-\t\t}\n-\t\txfs_buf_unlock(bp);\n-\n-\t\txfs_buf_delwri_pushbuf(bp, buffer_list);\n-\t\txfs_buf_rele(bp);\n-\n-\t\terror = -EAGAIN;\n-\t\tgoto out_unlock;\n-\t}\n+\txfs_qm_dqunpin_wait(dqp);\n+\txfs_dqflock(dqp);\n \n \terror = xfs_dquot_use_attached_buf(dqp, &bp);\n \tif (error)\ndiff --git a/fs/xfs/xfs_rtalloc.c b/fs/xfs/xfs_rtalloc.c\nindex 6484c596ecea..736eb0924573 100644\n--- a/fs/xfs/xfs_rtalloc.c\n+++ b/fs/xfs/xfs_rtalloc.c\n@@ -1259,6 +1259,8 @@ xfs_growfs_check_rtgeom(\n \n \tkfree(nmp);\n \n+\ttrace_xfs_growfs_check_rtgeom(mp, min_logfsbs);\n+\n \tif (min_logfsbs > mp->m_sb.sb_logblocks)\n \t\treturn -EINVAL;\n \ndiff --git a/fs/xfs/xfs_super.c b/fs/xfs/xfs_super.c\nindex 0bc4b5489078..bb0a82635a77 100644\n--- a/fs/xfs/xfs_super.c\n+++ b/fs/xfs/xfs_super.c\n@@ -2020,14 +2020,13 @@ xfs_remount_rw(\n \tint error;\n \n \tif (mp->m_logdev_targp && mp->m_logdev_targp != mp->m_ddev_targp &&\n-\t    bdev_read_only(mp->m_logdev_targp->bt_bdev)) {\n+\t    xfs_readonly_buftarg(mp->m_logdev_targp)) {\n \t\txfs_warn(mp,\n \t\t\t\"ro->rw transition prohibited by read-only logdev\");\n \t\treturn -EACCES;\n \t}\n \n-\tif (mp->m_rtdev_targp &&\n-\t    bdev_read_only(mp->m_rtdev_targp->bt_bdev)) {\n+\tif (mp->m_rtdev_targp && xfs_readonly_buftarg(mp->m_rtdev_targp)) {\n \t\txfs_warn(mp,\n \t\t\t\"ro->rw transition prohibited by read-only rtdev\");\n \t\treturn -EACCES;\ndiff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h\nindex 01d284a1c759..ba45d801df1c 100644\n--- a/fs/xfs/xfs_trace.h\n+++ b/fs/xfs/xfs_trace.h\n@@ -778,7 +778,6 @@ DEFINE_BUF_EVENT(xfs_buf_iowait_done);\n DEFINE_BUF_EVENT(xfs_buf_delwri_queue);\n DEFINE_BUF_EVENT(xfs_buf_delwri_queued);\n DEFINE_BUF_EVENT(xfs_buf_delwri_split);\n-DEFINE_BUF_EVENT(xfs_buf_delwri_pushbuf);\n DEFINE_BUF_EVENT(xfs_buf_get_uncached);\n DEFINE_BUF_EVENT(xfs_buf_item_relse);\n DEFINE_BUF_EVENT(xfs_buf_iodone_async);\n@@ -1147,6 +1146,7 @@ DECLARE_EVENT_CLASS(xfs_iref_class,\n \t\t__field(xfs_ino_t, ino)\n \t\t__field(int, count)\n \t\t__field(int, pincount)\n+\t\t__field(unsigned long, iflags)\n \t\t__field(unsigned long, caller_ip)\n \t),\n \tTP_fast_assign(\n@@ -1154,13 +1154,15 @@ DECLARE_EVENT_CLASS(xfs_iref_class,\n \t\t__entry->ino = ip->i_ino;\n \t\t__entry->count = atomic_read(&VFS_I(ip)->i_count);\n \t\t__entry->pincount = atomic_read(&ip->i_pincount);\n+\t\t__entry->iflags = ip->i_flags;\n \t\t__entry->caller_ip = caller_ip;\n \t),\n-\tTP_printk(\"dev %d:%d ino 0x%llx count %d pincount %d caller %pS\",\n+\tTP_printk(\"dev %d:%d ino 0x%llx count %d pincount %d iflags 0x%lx caller %pS\",\n \t\t  MAJOR(__entry->dev), MINOR(__entry->dev),\n \t\t  __entry->ino,\n \t\t  __entry->count,\n \t\t  __entry->pincount,\n+\t\t  __entry->iflags,\n \t\t  (char *)__entry->caller_ip)\n )\n \n@@ -1250,6 +1252,8 @@ DEFINE_IREF_EVENT(xfs_irele);\n DEFINE_IREF_EVENT(xfs_inode_pin);\n DEFINE_IREF_EVENT(xfs_inode_unpin);\n DEFINE_IREF_EVENT(xfs_inode_unpin_nowait);\n+DEFINE_IREF_EVENT(xfs_inode_push_pinned);\n+DEFINE_IREF_EVENT(xfs_inode_push_stale);\n \n DECLARE_EVENT_CLASS(xfs_namespace_class,\n \tTP_PROTO(struct xfs_inode *dp, const struct xfs_name *name),\n@@ -1654,6 +1658,8 @@ DEFINE_LOG_ITEM_EVENT(xfs_ail_flushing);\n DEFINE_LOG_ITEM_EVENT(xfs_cil_whiteout_mark);\n DEFINE_LOG_ITEM_EVENT(xfs_cil_whiteout_skip);\n DEFINE_LOG_ITEM_EVENT(xfs_cil_whiteout_unpin);\n+DEFINE_LOG_ITEM_EVENT(xlog_ail_insert_abort);\n+DEFINE_LOG_ITEM_EVENT(xfs_trans_free_abort);\n \n DECLARE_EVENT_CLASS(xfs_ail_class,\n \tTP_PROTO(struct xfs_log_item *lip, xfs_lsn_t old_lsn, xfs_lsn_t new_lsn),\ndiff --git a/fs/xfs/xfs_trans.c b/fs/xfs/xfs_trans.c\nindex c6657072361a..b4a07af513ba 100644\n--- a/fs/xfs/xfs_trans.c\n+++ b/fs/xfs/xfs_trans.c\n@@ -742,8 +742,10 @@ xfs_trans_free_items(\n \n \tlist_for_each_entry_safe(lip, next, &tp->t_items, li_trans) {\n \t\txfs_trans_del_item(lip);\n-\t\tif (abort)\n+\t\tif (abort) {\n+\t\t\ttrace_xfs_trans_free_abort(lip);\n \t\t\tset_bit(XFS_LI_ABORTED, &lip->li_flags);\n+\t\t}\n \t\tif (lip->li_ops->iop_release)\n \t\t\tlip->li_ops->iop_release(lip);\n \t}\ndiff --git a/fs/xfs/xfs_zone_alloc.c b/fs/xfs/xfs_zone_alloc.c\nindex 80add26c0111..01315ed75502 100644\n--- a/fs/xfs/xfs_zone_alloc.c\n+++ b/fs/xfs/xfs_zone_alloc.c\n@@ -727,7 +727,7 @@ xfs_select_zone(\n \tfor (;;) {\n \t\tprepare_to_wait(&zi->zi_zone_wait, &wait, TASK_UNINTERRUPTIBLE);\n \t\toz = xfs_select_zone_nowait(mp, write_hint, pack_tight);\n-\t\tif (oz)\n+\t\tif (oz || xfs_is_shutdown(mp))\n \t\t\tbreak;\n \t\tschedule();\n \t}\n@@ -777,26 +777,6 @@ xfs_mark_rtg_boundary(\n \t\tioend->io_flags |= IOMAP_IOEND_BOUNDARY;\n }\n \n-static void\n-xfs_submit_zoned_bio(\n-\tstruct iomap_ioend\t*ioend,\n-\tstruct xfs_open_zone\t*oz,\n-\tbool\t\t\tis_seq)\n-{\n-\tioend->io_bio.bi_iter.bi_sector = ioend->io_sector;\n-\tioend->io_private = oz;\n-\tatomic_inc(&oz->oz_ref); /* for xfs_zoned_end_io */\n-\n-\tif (is_seq) {\n-\t\tioend->io_bio.bi_opf &= ~REQ_OP_WRITE;\n-\t\tioend->io_bio.bi_opf |= REQ_OP_ZONE_APPEND;\n-\t} else {\n-\t\txfs_mark_rtg_boundary(ioend);\n-\t}\n-\n-\tsubmit_bio(&ioend->io_bio);\n-}\n-\n /*\n  * Cache the last zone written to for an inode so that it is considered first\n  * for subsequent writes.\n@@ -891,6 +871,26 @@ xfs_zone_cache_create_association(\n \txfs_mru_cache_insert(mp->m_zone_cache, ip->i_ino, &item->mru);\n }\n \n+static void\n+xfs_submit_zoned_bio(\n+\tstruct iomap_ioend\t*ioend,\n+\tstruct xfs_open_zone\t*oz,\n+\tbool\t\t\tis_seq)\n+{\n+\tioend->io_bio.bi_iter.bi_sector = ioend->io_sector;\n+\tioend->io_private = oz;\n+\tatomic_inc(&oz->oz_ref); /* for xfs_zoned_end_io */\n+\n+\tif (is_seq) {\n+\t\tioend->io_bio.bi_opf &= ~REQ_OP_WRITE;\n+\t\tioend->io_bio.bi_opf |= REQ_OP_ZONE_APPEND;\n+\t} else {\n+\t\txfs_mark_rtg_boundary(ioend);\n+\t}\n+\n+\tsubmit_bio(&ioend->io_bio);\n+}\n+\n void\n xfs_zone_alloc_and_submit(\n \tstruct iomap_ioend\t*ioend,",
    "stats": {
      "insertions": 320,
      "deletions": 287,
      "files": 19
    }
  },
  {
    "sha": "75ef7b8d44c30a76cfbe42dde9413d43055a00a7",
    "message": "Merge tag 'nvme-6.16-2025-07-03' of git://git.infradead.org/nvme into block-6.16\n\nPull NVMe fixes from Christoph:\n\n\"- fix incorrect cdw15 value in passthru error logging (Alok Tiwari)\n - fix memory leak of bio integrity in nvmet (Dmitry Bogdanov)\n - refresh visible attrs after being checked (Eugen Hristev)\n - fix suspicious RCU usage warning in the multipath code (Geliang Tang)\n - correctly account for namespace head reference counter (Nilay Shroff)\"\n\n* tag 'nvme-6.16-2025-07-03' of git://git.infradead.org/nvme:\n  nvme-multipath: fix suspicious RCU usage warning\n  nvme-pci: refresh visible attrs after being checked\n  nvmet: fix memory leak of bio integrity\n  nvme: correctly account for namespace head reference counter\n  nvme: Fix incorrect cdw15 value in passthru error logging",
    "author": "Jens Axboe",
    "date": "2025-07-03T09:42:07-06:00",
    "files_changed": [
      "drivers/nvme/host/core.c",
      "drivers/nvme/host/multipath.c",
      "drivers/nvme/host/pci.c",
      "drivers/nvme/target/nvmet.h"
    ],
    "diff": "diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c\nindex e533d791955d..7493e5aa984c 100644\n--- a/drivers/nvme/host/core.c\n+++ b/drivers/nvme/host/core.c\n@@ -386,7 +386,7 @@ static void nvme_log_err_passthru(struct request *req)\n \t\tnr->cmd->common.cdw12,\n \t\tnr->cmd->common.cdw13,\n \t\tnr->cmd->common.cdw14,\n-\t\tnr->cmd->common.cdw14);\n+\t\tnr->cmd->common.cdw15);\n }\n \n enum nvme_disposition {\n@@ -4086,6 +4086,7 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, struct nvme_ns_info *info)\n \tstruct nvme_ns *ns;\n \tstruct gendisk *disk;\n \tint node = ctrl->numa_node;\n+\tbool last_path = false;\n \n \tns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);\n \tif (!ns)\n@@ -4178,9 +4179,22 @@ static void nvme_alloc_ns(struct nvme_ctrl *ctrl, struct nvme_ns_info *info)\n  out_unlink_ns:\n \tmutex_lock(&ctrl->subsys->lock);\n \tlist_del_rcu(&ns->siblings);\n-\tif (list_empty(&ns->head->list))\n+\tif (list_empty(&ns->head->list)) {\n \t\tlist_del_init(&ns->head->entry);\n+\t\t/*\n+\t\t * If multipath is not configured, we still create a namespace\n+\t\t * head (nshead), but head->disk is not initialized in that\n+\t\t * case.  As a result, only a single reference to nshead is held\n+\t\t * (via kref_init()) when it is created. Therefore, ensure that\n+\t\t * we do not release the reference to nshead twice if head->disk\n+\t\t * is not present.\n+\t\t */\n+\t\tif (ns->head->disk)\n+\t\t\tlast_path = true;\n+\t}\n \tmutex_unlock(&ctrl->subsys->lock);\n+\tif (last_path)\n+\t\tnvme_put_ns_head(ns->head);\n \tnvme_put_ns_head(ns->head);\n  out_cleanup_disk:\n \tput_disk(disk);\ndiff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c\nindex 1062467595f3..5d1ad28986e9 100644\n--- a/drivers/nvme/host/multipath.c\n+++ b/drivers/nvme/host/multipath.c\n@@ -690,8 +690,8 @@ static void nvme_remove_head(struct nvme_ns_head *head)\n \t\tnvme_cdev_del(&head->cdev, &head->cdev_device);\n \t\tsynchronize_srcu(&head->srcu);\n \t\tdel_gendisk(head->disk);\n-\t\tnvme_put_ns_head(head);\n \t}\n+\tnvme_put_ns_head(head);\n }\n \n static void nvme_remove_head_work(struct work_struct *work)\n@@ -1200,7 +1200,8 @@ void nvme_mpath_add_sysfs_link(struct nvme_ns_head *head)\n \t */\n \tsrcu_idx = srcu_read_lock(&head->srcu);\n \n-\tlist_for_each_entry_rcu(ns, &head->list, siblings) {\n+\tlist_for_each_entry_srcu(ns, &head->list, siblings,\n+\t\t\t\t srcu_read_lock_held(&head->srcu)) {\n \t\t/*\n \t\t * Ensure that ns path disk node is already added otherwise we\n \t\t * may get invalid kobj name for target\n@@ -1291,6 +1292,9 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)\n {\n \tbool remove = false;\n \n+\tif (!head->disk)\n+\t\treturn;\n+\n \tmutex_lock(&head->subsys->lock);\n \t/*\n \t * We are called when all paths have been removed, and at that point\ndiff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c\nindex 8ff12e415cb5..320aaa41ec39 100644\n--- a/drivers/nvme/host/pci.c\n+++ b/drivers/nvme/host/pci.c\n@@ -2101,8 +2101,6 @@ static void nvme_map_cmb(struct nvme_dev *dev)\n \tif ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==\n \t\t\t(NVME_CMBSZ_WDS | NVME_CMBSZ_RDS))\n \t\tpci_p2pmem_publish(pdev, true);\n-\n-\tnvme_update_attrs(dev);\n }\n \n static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)\n@@ -3010,6 +3008,8 @@ static void nvme_reset_work(struct work_struct *work)\n \tif (result < 0)\n \t\tgoto out;\n \n+\tnvme_update_attrs(dev);\n+\n \tresult = nvme_setup_io_queues(dev);\n \tif (result)\n \t\tgoto out;\n@@ -3343,6 +3343,8 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n \tif (result < 0)\n \t\tgoto out_disable;\n \n+\tnvme_update_attrs(dev);\n+\n \tresult = nvme_setup_io_queues(dev);\n \tif (result)\n \t\tgoto out_disable;\ndiff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h\nindex df69a9dee71c..51df72f5e89b 100644\n--- a/drivers/nvme/target/nvmet.h\n+++ b/drivers/nvme/target/nvmet.h\n@@ -867,6 +867,8 @@ static inline void nvmet_req_bio_put(struct nvmet_req *req, struct bio *bio)\n {\n \tif (bio != &req->b.inline_bio)\n \t\tbio_put(bio);\n+\telse\n+\t\tbio_uninit(bio);\n }\n \n #ifdef CONFIG_NVME_TARGET_TCP_TLS",
    "stats": {
      "insertions": 28,
      "deletions": 6,
      "files": 4
    }
  }
]
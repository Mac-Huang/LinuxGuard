[
  {
    "sha": "080e8d2ecdfde588897aa8a87a8884061f4dbbbb",
    "message": "LoongArch: KVM: Avoid overflow with array index\n\nThe variable index is modified and reused as array index when modify\nregister EIOINTC_ENABLE. There will be array index overflow problem.\n\nCc: stable@vger.kernel.org\nFixes: 3956a52bc05b (\"LoongArch: KVM: Add EIOINTC read and write functions\")\nSigned-off-by: Bibo Mao <maobibo@loongson.cn>\nSigned-off-by: Huacai Chen <chenhuacai@loongson.cn>",
    "author": "Bibo Mao",
    "date": "2025-06-26T20:07:27+08:00",
    "files_changed": [
      "arch/loongarch/kvm/intc/eiointc.c"
    ],
    "diff": "diff --git a/arch/loongarch/kvm/intc/eiointc.c b/arch/loongarch/kvm/intc/eiointc.c\nindex f39929d7bf8a..9c47456b805c 100644\n--- a/arch/loongarch/kvm/intc/eiointc.c\n+++ b/arch/loongarch/kvm/intc/eiointc.c\n@@ -436,17 +436,16 @@ static int loongarch_eiointc_writew(struct kvm_vcpu *vcpu,\n \t\tbreak;\n \tcase EIOINTC_ENABLE_START ... EIOINTC_ENABLE_END:\n \t\tindex = (offset - EIOINTC_ENABLE_START) >> 1;\n-\t\told_data = s->enable.reg_u32[index];\n+\t\told_data = s->enable.reg_u16[index];\n \t\ts->enable.reg_u16[index] = data;\n \t\t/*\n \t\t * 1: enable irq.\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u16[index] & ~old_data & s->isr.reg_u16[index];\n-\t\tindex = index << 1;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 2 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -455,7 +454,7 @@ static int loongarch_eiointc_writew(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u16[index] & old_data & s->isr.reg_u16[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 2 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:\n@@ -529,10 +528,9 @@ static int loongarch_eiointc_writel(struct kvm_vcpu *vcpu,\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u32[index] & ~old_data & s->isr.reg_u32[index];\n-\t\tindex = index << 2;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 4 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -541,7 +539,7 @@ static int loongarch_eiointc_writel(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u32[index] & old_data & s->isr.reg_u32[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 4 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:\n@@ -615,10 +613,9 @@ static int loongarch_eiointc_writeq(struct kvm_vcpu *vcpu,\n \t\t * update irq when isr is set.\n \t\t */\n \t\tdata = s->enable.reg_u64[index] & ~old_data & s->isr.reg_u64[index];\n-\t\tindex = index << 3;\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index + i, mask, 1);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 8 + i, mask, 1);\n \t\t}\n \t\t/*\n \t\t * 0: disable irq.\n@@ -627,7 +624,7 @@ static int loongarch_eiointc_writeq(struct kvm_vcpu *vcpu,\n \t\tdata = ~s->enable.reg_u64[index] & old_data & s->isr.reg_u64[index];\n \t\tfor (i = 0; i < sizeof(data); i++) {\n \t\t\tu8 mask = (data >> (i * 8)) & 0xff;\n-\t\t\teiointc_enable_irq(vcpu, s, index, mask, 0);\n+\t\t\teiointc_enable_irq(vcpu, s, index * 8 + i, mask, 0);\n \t\t}\n \t\tbreak;\n \tcase EIOINTC_BOUNCE_START ... EIOINTC_BOUNCE_END:",
    "stats": {
      "insertions": 7,
      "deletions": 10,
      "files": 1
    }
  },
  {
    "sha": "a0137c9048252ea965a0436895c77d1c917dfe2a",
    "message": "LoongArch: Handle KCOV __init vs inline mismatches\n\nWhen the KCOV is enabled all functions get instrumented, unless\nthe __no_sanitize_coverage attribute is used. To prepare for\n__no_sanitize_coverage being applied to __init functions, we have to\nhandle differences in how GCC's inline optimizations get resolved.\nFor LoongArch this exposed several places where __init annotations\nwere missing but ended up being \"accidentally correct\". So fix these\ncases.\n\nSigned-off-by: Kees Cook <kees@kernel.org>\nSigned-off-by: Huacai Chen <chenhuacai@loongson.cn>",
    "author": "Kees Cook",
    "date": "2025-06-26T20:07:18+08:00",
    "files_changed": [
      "arch/loongarch/include/asm/smp.h",
      "arch/loongarch/kernel/time.c",
      "arch/loongarch/mm/ioremap.c"
    ],
    "diff": "diff --git a/arch/loongarch/include/asm/smp.h b/arch/loongarch/include/asm/smp.h\nindex ad0bd234a0f1..3a47f52959a8 100644\n--- a/arch/loongarch/include/asm/smp.h\n+++ b/arch/loongarch/include/asm/smp.h\n@@ -39,7 +39,7 @@ int loongson_cpu_disable(void);\n void loongson_cpu_die(unsigned int cpu);\n #endif\n \n-static inline void plat_smp_setup(void)\n+static inline void __init plat_smp_setup(void)\n {\n \tloongson_smp_setup();\n }\ndiff --git a/arch/loongarch/kernel/time.c b/arch/loongarch/kernel/time.c\nindex bc75a3a69fc8..367906b10f81 100644\n--- a/arch/loongarch/kernel/time.c\n+++ b/arch/loongarch/kernel/time.c\n@@ -102,7 +102,7 @@ static int constant_timer_next_event(unsigned long delta, struct clock_event_dev\n \treturn 0;\n }\n \n-static unsigned long __init get_loops_per_jiffy(void)\n+static unsigned long get_loops_per_jiffy(void)\n {\n \tunsigned long lpj = (unsigned long)const_clock_freq;\n \ndiff --git a/arch/loongarch/mm/ioremap.c b/arch/loongarch/mm/ioremap.c\nindex 70ca73019811..df949a3d0f34 100644\n--- a/arch/loongarch/mm/ioremap.c\n+++ b/arch/loongarch/mm/ioremap.c\n@@ -16,12 +16,12 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)\n \n }\n \n-void *early_memremap_ro(resource_size_t phys_addr, unsigned long size)\n+void * __init early_memremap_ro(resource_size_t phys_addr, unsigned long size)\n {\n \treturn early_memremap(phys_addr, size);\n }\n \n-void *early_memremap_prot(resource_size_t phys_addr, unsigned long size,\n+void * __init early_memremap_prot(resource_size_t phys_addr, unsigned long size,\n \t\t    unsigned long prot_val)\n {\n \treturn early_memremap(phys_addr, size);",
    "stats": {
      "insertions": 4,
      "deletions": 4,
      "files": 3
    }
  },
  {
    "sha": "f46d273449ba65afd53f3dd8fe0182c9df877e08",
    "message": "nvme: fix atomic write size validation\n\nDon't mix the namespace and controller values, and validate the\nper-controller limit when probing the controller.  This avoid spurious\nfailures for controllers with namespaces that have different namespaces\nwith different logical block sizes, or report the per-namespace values\nonly for some namespaces.\n\nIt also fixes a missing queue_limits_cancel_update in an error path by\nremoving that error path.\n\nFixes: 8695f060a029 (\"nvme: all namespaces in a subsystem must adhere to a common atomic write size\")\nReported-by: Yi Zhang <yi.zhang@redhat.com>\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Luis Chamberlain <mcgrof@kernel.org>\nReviewed-by: John Garry <john.g.garry@oracle.com>\nTested-by: Yi Zhang <yi.zhang@redhat.com>",
    "author": "Christoph Hellwig",
    "date": "2025-06-26T13:04:37+02:00",
    "files_changed": [
      "drivers/nvme/host/core.c",
      "drivers/nvme/host/nvme.h"
    ],
    "diff": "diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c\nindex 520fb5f1e214..e533d791955d 100644\n--- a/drivers/nvme/host/core.c\n+++ b/drivers/nvme/host/core.c\n@@ -2041,17 +2041,7 @@ static u32 nvme_configure_atomic_write(struct nvme_ns *ns,\n \t\t * no clear language in the specification prohibiting different\n \t\t * values for different controllers in the subsystem.\n \t\t */\n-\t\tatomic_bs = (1 + ns->ctrl->awupf) * bs;\n-\t}\n-\n-\tif (!ns->ctrl->subsys->atomic_bs) {\n-\t\tns->ctrl->subsys->atomic_bs = atomic_bs;\n-\t} else if (ns->ctrl->subsys->atomic_bs != atomic_bs) {\n-\t\tdev_err_ratelimited(ns->ctrl->device,\n-\t\t\t\"%s: Inconsistent Atomic Write Size, Namespace will not be added: Subsystem=%d bytes, Controller/Namespace=%d bytes\\n\",\n-\t\t\tns->disk ? ns->disk->disk_name : \"?\",\n-\t\t\tns->ctrl->subsys->atomic_bs,\n-\t\t\tatomic_bs);\n+\t\tatomic_bs = (1 + ns->ctrl->subsys->awupf) * bs;\n \t}\n \n \tlim->atomic_write_hw_max = atomic_bs;\n@@ -2386,16 +2376,6 @@ static int nvme_update_ns_info_block(struct nvme_ns *ns,\n \tif (!nvme_update_disk_info(ns, id, &lim))\n \t\tcapacity = 0;\n \n-\t/*\n-\t * Validate the max atomic write size fits within the subsystem's\n-\t * atomic write capabilities.\n-\t */\n-\tif (lim.atomic_write_hw_max > ns->ctrl->subsys->atomic_bs) {\n-\t\tblk_mq_unfreeze_queue(ns->disk->queue, memflags);\n-\t\tret = -ENXIO;\n-\t\tgoto out;\n-\t}\n-\n \tnvme_config_discard(ns, &lim);\n \tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED) &&\n \t    ns->head->ids.csi == NVME_CSI_ZNS)\n@@ -3219,6 +3199,7 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)\n \tmemcpy(subsys->model, id->mn, sizeof(subsys->model));\n \tsubsys->vendor_id = le16_to_cpu(id->vid);\n \tsubsys->cmic = id->cmic;\n+\tsubsys->awupf = le16_to_cpu(id->awupf);\n \n \t/* Versions prior to 1.4 don't necessarily report a valid type */\n \tif (id->cntrltype == NVME_CTRL_DISC ||\n@@ -3556,6 +3537,15 @@ static int nvme_init_identify(struct nvme_ctrl *ctrl)\n \t\tif (ret)\n \t\t\tgoto out_free;\n \t}\n+\n+\tif (le16_to_cpu(id->awupf) != ctrl->subsys->awupf) {\n+\t\tdev_err_ratelimited(ctrl->device,\n+\t\t\t\"inconsistent AWUPF, controller not added (%u/%u).\\n\",\n+\t\t\tle16_to_cpu(id->awupf), ctrl->subsys->awupf);\n+\t\tret = -EINVAL;\n+\t\tgoto out_free;\n+\t}\n+\n \tmemcpy(ctrl->subsys->firmware_rev, id->fr,\n \t       sizeof(ctrl->subsys->firmware_rev));\n \n@@ -3651,7 +3641,6 @@ static int nvme_init_identify(struct nvme_ctrl *ctrl)\n \t\tdev_pm_qos_expose_latency_tolerance(ctrl->device);\n \telse if (!ctrl->apst_enabled && prev_apst_enabled)\n \t\tdev_pm_qos_hide_latency_tolerance(ctrl->device);\n-\tctrl->awupf = le16_to_cpu(id->awupf);\n out_free:\n \tkfree(id);\n \treturn ret;\ndiff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h\nindex a468cdc5b5cb..7df2ea21851f 100644\n--- a/drivers/nvme/host/nvme.h\n+++ b/drivers/nvme/host/nvme.h\n@@ -410,7 +410,6 @@ struct nvme_ctrl {\n \n \tenum nvme_ctrl_type cntrltype;\n \tenum nvme_dctype dctype;\n-\tu16 awupf; /* 0's based value. */\n };\n \n static inline enum nvme_ctrl_state nvme_ctrl_state(struct nvme_ctrl *ctrl)\n@@ -443,11 +442,11 @@ struct nvme_subsystem {\n \tu8\t\t\tcmic;\n \tenum nvme_subsys_type\tsubtype;\n \tu16\t\t\tvendor_id;\n+\tu16\t\t\tawupf; /* 0's based value. */\n \tstruct ida\t\tns_ida;\n #ifdef CONFIG_NVME_MULTIPATH\n \tenum nvme_iopolicy\tiopolicy;\n #endif\n-\tu32\t\t\tatomic_bs;\n };\n \n /*",
    "stats": {
      "insertions": 12,
      "deletions": 24,
      "files": 2
    }
  },
  {
    "sha": "0e02219f9cf4f0c0aa3dbf3c820e6612bf3f0c8c",
    "message": "KVM: arm64: Don't free hyp pages with pKVM on GICv2\n\nMarc reported that enabling protected mode on a device with GICv2\ndoesn't fail gracefully as one would expect, and leads to a host\nkernel crash.\n\nAs it turns out, the first half of pKVM init happens before the vgic\nprobe, and so by the time we find out we have a GICv2 we're already\ncommitted to keeping the pKVM vectors installed at EL2 -- pKVM rejects\nstub HVCs for obvious security reasons. However, the error path on KVM\ninit leads to teardown_hyp_mode() which unconditionally frees hypervisor\nallocations (including the EL2 stacks and per-cpu pages) under the\nassumption that a previous cpu_hyp_uninit() execution has reset the\nvectors back to the stubs, which is false with pKVM.\n\nInterestingly, host stage-2 protection is not enabled yet at this point,\nso this use-after-free may go unnoticed for a while. The issue becomes\nmore obvious after the finalize_pkvm() call.\n\nFix this by keeping track of the CPUs on which pKVM is initialized in\nthe kvm_hyp_initialized per-cpu variable, and use it from\nteardown_hyp_mode() to skip freeing pages that are in fact used.\n\nFixes: a770ee80e662 (\"KVM: arm64: pkvm: Disable GICv2 support\")\nReported-by: Marc Zyngier <maz@kernel.org>\nSigned-off-by: Quentin Perret <qperret@google.com>\nLink: https://lore.kernel.org/r/20250626101014.1519345-1-qperret@google.com\nSigned-off-by: Marc Zyngier <maz@kernel.org>",
    "author": "Quentin Perret",
    "date": "2025-06-26T11:39:15+01:00",
    "files_changed": [
      "arch/arm64/kvm/arm.c"
    ],
    "diff": "diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c\nindex 6bdf79bc5d95..b223d21c063c 100644\n--- a/arch/arm64/kvm/arm.c\n+++ b/arch/arm64/kvm/arm.c\n@@ -2129,7 +2129,7 @@ static void cpu_hyp_init(void *discard)\n \n static void cpu_hyp_uninit(void *discard)\n {\n-\tif (__this_cpu_read(kvm_hyp_initialized)) {\n+\tif (!is_protected_kvm_enabled() && __this_cpu_read(kvm_hyp_initialized)) {\n \t\tcpu_hyp_reset();\n \t\t__this_cpu_write(kvm_hyp_initialized, 0);\n \t}\n@@ -2345,6 +2345,9 @@ static void __init teardown_hyp_mode(void)\n \n \tfree_hyp_pgds();\n \tfor_each_possible_cpu(cpu) {\n+\t\tif (per_cpu(kvm_hyp_initialized, cpu))\n+\t\t\tcontinue;\n+\n \t\tfree_pages(per_cpu(kvm_arm_hyp_stack_base, cpu), NVHE_STACK_SHIFT - PAGE_SHIFT);\n \n \t\tif (!kvm_nvhe_sym(kvm_arm_hyp_percpu_base)[cpu])",
    "stats": {
      "insertions": 4,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "1476b218327b89bbb64c14619a2d34f0c320f2c3",
    "message": "perf/aux: Fix pending disable flow when the AUX ring buffer overruns\n\nIf an AUX event overruns, the event core layer intends to disable the\nevent by setting the 'pending_disable' flag. Unfortunately, the event\nis not actually disabled afterwards.\n\nIn commit:\n\n  ca6c21327c6a (\"perf: Fix missing SIGTRAPs\")\n\nthe 'pending_disable' flag was changed to a boolean. However, the\nAUX event code was not updated accordingly. The flag ends up holding a\nCPU number. If this number is zero, the flag is taken as false and the\nIRQ work is never triggered.\n\nLater, with commit:\n\n  2b84def990d3 (\"perf: Split __perf_pending_irq() out of perf_pending_irq()\")\n\na new IRQ work 'pending_disable_irq' was introduced to handle event\ndisabling. The AUX event path was not updated to kick off the work queue.\n\nTo fix this bug, when an AUX ring buffer overrun is detected, call\nperf_event_disable_inatomic() to initiate the pending disable flow.\n\nAlso update the outdated comment for setting the flag, to reflect the\nboolean values (0 or 1).\n\nFixes: 2b84def990d3 (\"perf: Split __perf_pending_irq() out of perf_pending_irq()\")\nFixes: ca6c21327c6a (\"perf: Fix missing SIGTRAPs\")\nSigned-off-by: Leo Yan <leo.yan@arm.com>\nSigned-off-by: Ingo Molnar <mingo@kernel.org>\nReviewed-by: James Clark <james.clark@linaro.org>\nReviewed-by: Yeoreum Yun <yeoreum.yun@arm.com>\nCc: Adrian Hunter <adrian.hunter@intel.com>\nCc: Alexander Shishkin <alexander.shishkin@linux.intel.com>\nCc: Arnaldo Carvalho de Melo <acme@kernel.org>\nCc: Ian Rogers <irogers@google.com>\nCc: Jiri Olsa <jolsa@redhat.com>\nCc: Liang Kan <kan.liang@linux.intel.com>\nCc: Marco Elver <elver@google.com>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Namhyung Kim <namhyung@kernel.org>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>\nCc: linux-perf-users@vger.kernel.org\nLink: https://lore.kernel.org/r/20250625170737.2918295-1-leo.yan@arm.com",
    "author": "Leo Yan",
    "date": "2025-06-26T10:50:37+02:00",
    "files_changed": [
      "kernel/events/core.c",
      "kernel/events/ring_buffer.c"
    ],
    "diff": "diff --git a/kernel/events/core.c b/kernel/events/core.c\nindex 1f746469fda5..7281230044d0 100644\n--- a/kernel/events/core.c\n+++ b/kernel/events/core.c\n@@ -7251,15 +7251,15 @@ static void __perf_pending_disable(struct perf_event *event)\n \t *  CPU-A\t\t\tCPU-B\n \t *\n \t *  perf_event_disable_inatomic()\n-\t *    @pending_disable = CPU-A;\n+\t *    @pending_disable = 1;\n \t *    irq_work_queue();\n \t *\n \t *  sched-out\n-\t *    @pending_disable = -1;\n+\t *    @pending_disable = 0;\n \t *\n \t *\t\t\t\tsched-in\n \t *\t\t\t\tperf_event_disable_inatomic()\n-\t *\t\t\t\t  @pending_disable = CPU-B;\n+\t *\t\t\t\t  @pending_disable = 1;\n \t *\t\t\t\t  irq_work_queue(); // FAILS\n \t *\n \t *  irq_work_run()\ndiff --git a/kernel/events/ring_buffer.c b/kernel/events/ring_buffer.c\nindex d2aef87c7e9f..aa9a759e824f 100644\n--- a/kernel/events/ring_buffer.c\n+++ b/kernel/events/ring_buffer.c\n@@ -441,7 +441,7 @@ void *perf_aux_output_begin(struct perf_output_handle *handle,\n \t\t * store that will be enabled on successful return\n \t\t */\n \t\tif (!handle->size) { /* A, matches D */\n-\t\t\tevent->pending_disable = smp_processor_id();\n+\t\t\tperf_event_disable_inatomic(handle->event);\n \t\t\tperf_output_wakeup(handle);\n \t\t\tWRITE_ONCE(rb->aux_nest, 0);\n \t\t\tgoto err_put;\n@@ -526,7 +526,7 @@ void perf_aux_output_end(struct perf_output_handle *handle, unsigned long size)\n \n \tif (wakeup) {\n \t\tif (handle->aux_flags & PERF_AUX_FLAG_TRUNCATED)\n-\t\t\thandle->event->pending_disable = smp_processor_id();\n+\t\t\tperf_event_disable_inatomic(handle->event);\n \t\tperf_output_wakeup(handle);\n \t}\n ",
    "stats": {
      "insertions": 5,
      "deletions": 5,
      "files": 2
    }
  }
]
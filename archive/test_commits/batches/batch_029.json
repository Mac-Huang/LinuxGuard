[
  {
    "sha": "35e261cd95ddc741d8664f5ac897bbd0d384bbd0",
    "message": "Merge tag 'acpi-6.16-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm\n\nPull ACPI fix from Rafael Wysocki:\n \"Revert a commit that attempted to fix a memory leak in an error code\n  path and introduced a different issue (Zhe Qiao)\"\n\n* tag 'acpi-6.16-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:\n  Revert \"PCI/ACPI: Fix allocated memory release on error in pci_acpi_scan_root()\"",
    "author": "Linus Torvalds",
    "date": "2025-06-27T12:08:36-07:00",
    "files_changed": [
      "drivers/pci/pci-acpi.c"
    ],
    "diff": "diff --git a/drivers/pci/pci-acpi.c b/drivers/pci/pci-acpi.c\nindex b78e0e417324..af370628e583 100644\n--- a/drivers/pci/pci-acpi.c\n+++ b/drivers/pci/pci-acpi.c\n@@ -1676,19 +1676,24 @@ struct pci_bus *pci_acpi_scan_root(struct acpi_pci_root *root)\n \t\treturn NULL;\n \n \troot_ops = kzalloc(sizeof(*root_ops), GFP_KERNEL);\n-\tif (!root_ops)\n-\t\tgoto free_ri;\n+\tif (!root_ops) {\n+\t\tkfree(ri);\n+\t\treturn NULL;\n+\t}\n \n \tri->cfg = pci_acpi_setup_ecam_mapping(root);\n-\tif (!ri->cfg)\n-\t\tgoto free_root_ops;\n+\tif (!ri->cfg) {\n+\t\tkfree(ri);\n+\t\tkfree(root_ops);\n+\t\treturn NULL;\n+\t}\n \n \troot_ops->release_info = pci_acpi_generic_release_info;\n \troot_ops->prepare_resources = pci_acpi_root_prepare_resources;\n \troot_ops->pci_ops = (struct pci_ops *)&ri->cfg->ops->pci_ops;\n \tbus = acpi_pci_root_create(root, root_ops, &ri->common, ri->cfg);\n \tif (!bus)\n-\t\tgoto free_cfg;\n+\t\treturn NULL;\n \n \t/* If we must preserve the resource configuration, claim now */\n \thost = pci_find_host_bridge(bus);\n@@ -1705,14 +1710,6 @@ struct pci_bus *pci_acpi_scan_root(struct acpi_pci_root *root)\n \t\tpcie_bus_configure_settings(child);\n \n \treturn bus;\n-\n-free_cfg:\n-\tpci_ecam_free(ri->cfg);\n-free_root_ops:\n-\tkfree(root_ops);\n-free_ri:\n-\tkfree(ri);\n-\treturn NULL;\n }\n \n void pcibios_add_bus(struct pci_bus *bus)",
    "stats": {
      "insertions": 10,
      "deletions": 13,
      "files": 1
    }
  },
  {
    "sha": "5f61b961599acbd2bed028d3089105a1f7d224b8",
    "message": "btrfs: fix inode lookup error handling during log replay\n\nWhen replaying log trees we use read_one_inode() to get an inode, which is\njust a wrapper around btrfs_iget_logging(), which in turn is a wrapper for\nbtrfs_iget(). But read_one_inode() always returns NULL for any error\nthat btrfs_iget_logging() / btrfs_iget() may return and this is a problem\nbecause:\n\n1) In many callers of read_one_inode() we convert the NULL into -EIO,\n   which is not accurate since btrfs_iget() may return -ENOMEM and -ENOENT\n   for example, besides -EIO and other errors. So during log replay we\n   may end up reporting a false -EIO, which is confusing since we may\n   not have had any IO error at all;\n\n2) When replaying directory deletes, at replay_dir_deletes(), we assume\n   the NULL returned from read_one_inode() means that the inode doesn't\n   exist and then proceed as if no error had happened. This is wrong\n   because unless btrfs_iget() returned ERR_PTR(-ENOENT), we had an\n   actual error and the target inode may exist in the target subvolume\n   root - this may later result in the log replay code failing at a\n   later stage (if we are \"lucky\") or succeed but leaving some\n   inconsistency in the filesystem.\n\nSo fix this by not ignoring errors from btrfs_iget_logging() and as\na consequence remove the read_one_inode() wrapper and just use\nbtrfs_iget_logging() directly. Also since btrfs_iget_logging() is\nsupposed to be called only against subvolume roots, just like\nread_one_inode() which had a comment about it, add an assertion to\nbtrfs_iget_logging() to check that the target root corresponds to a\nsubvolume root.\n\nFixes: 5d4f98a28c7d (\"Btrfs: Mixed back reference  (FORWARD ROLLING FORMAT CHANGE)\")\nReviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>\nReviewed-by: Qu Wenruo <wqu@suse.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
    "author": "Filipe Manana",
    "date": "2025-06-27T19:57:06+02:00",
    "files_changed": [
      "fs/btrfs/tree-log.c"
    ],
    "diff": "diff --git a/fs/btrfs/tree-log.c b/fs/btrfs/tree-log.c\nindex d514bf531b3b..8cf5e5ae593c 100644\n--- a/fs/btrfs/tree-log.c\n+++ b/fs/btrfs/tree-log.c\n@@ -143,6 +143,9 @@ static struct btrfs_inode *btrfs_iget_logging(u64 objectid, struct btrfs_root *r\n \tunsigned int nofs_flag;\n \tstruct btrfs_inode *inode;\n \n+\t/* Only meant to be called for subvolume roots and not for log roots. */\n+\tASSERT(is_fstree(btrfs_root_id(root)));\n+\n \t/*\n \t * We're holding a transaction handle whether we are logging or\n \t * replaying a log tree, so we must make sure NOFS semantics apply\n@@ -604,21 +607,6 @@ static int read_alloc_one_name(struct extent_buffer *eb, void *start, int len,\n \treturn 0;\n }\n \n-/*\n- * simple helper to read an inode off the disk from a given root\n- * This can only be called for subvolume roots and not for the log\n- */\n-static noinline struct btrfs_inode *read_one_inode(struct btrfs_root *root,\n-\t\t\t\t\t\t   u64 objectid)\n-{\n-\tstruct btrfs_inode *inode;\n-\n-\tinode = btrfs_iget_logging(objectid, root);\n-\tif (IS_ERR(inode))\n-\t\treturn NULL;\n-\treturn inode;\n-}\n-\n /* replays a single extent in 'eb' at 'slot' with 'key' into the\n  * subvolume 'root'.  path is released on entry and should be released\n  * on exit.\n@@ -674,9 +662,9 @@ static noinline int replay_one_extent(struct btrfs_trans_handle *trans,\n \t\treturn -EUCLEAN;\n \t}\n \n-\tinode = read_one_inode(root, key->objectid);\n-\tif (!inode)\n-\t\treturn -EIO;\n+\tinode = btrfs_iget_logging(key->objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n \t/*\n \t * first check to see if we already have this extent in the\n@@ -948,9 +936,10 @@ static noinline int drop_one_dir_item(struct btrfs_trans_handle *trans,\n \n \tbtrfs_release_path(path);\n \n-\tinode = read_one_inode(root, location.objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(location.objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -1169,10 +1158,10 @@ static inline int __add_inode_ref(struct btrfs_trans_handle *trans,\n \t\t\t\tkfree(victim_name.name);\n \t\t\t\treturn ret;\n \t\t\t} else if (!ret) {\n-\t\t\t\tret = -ENOENT;\n-\t\t\t\tvictim_parent = read_one_inode(root,\n-\t\t\t\t\t\tparent_objectid);\n-\t\t\t\tif (victim_parent) {\n+\t\t\t\tvictim_parent = btrfs_iget_logging(parent_objectid, root);\n+\t\t\t\tif (IS_ERR(victim_parent)) {\n+\t\t\t\t\tret = PTR_ERR(victim_parent);\n+\t\t\t\t} else {\n \t\t\t\t\tinc_nlink(&inode->vfs_inode);\n \t\t\t\t\tbtrfs_release_path(path);\n \n@@ -1317,9 +1306,9 @@ static int unlink_old_inode_refs(struct btrfs_trans_handle *trans,\n \t\t\tstruct btrfs_inode *dir;\n \n \t\t\tbtrfs_release_path(path);\n-\t\t\tdir = read_one_inode(root, parent_id);\n-\t\t\tif (!dir) {\n-\t\t\t\tret = -ENOENT;\n+\t\t\tdir = btrfs_iget_logging(parent_id, root);\n+\t\t\tif (IS_ERR(dir)) {\n+\t\t\t\tret = PTR_ERR(dir);\n \t\t\t\tkfree(name.name);\n \t\t\t\tgoto out;\n \t\t\t}\n@@ -1391,15 +1380,17 @@ static noinline int add_inode_ref(struct btrfs_trans_handle *trans,\n \t * copy the back ref in.  The link count fixup code will take\n \t * care of the rest\n \t */\n-\tdir = read_one_inode(root, parent_objectid);\n-\tif (!dir) {\n-\t\tret = -ENOENT;\n+\tdir = btrfs_iget_logging(parent_objectid, root);\n+\tif (IS_ERR(dir)) {\n+\t\tret = PTR_ERR(dir);\n+\t\tdir = NULL;\n \t\tgoto out;\n \t}\n \n-\tinode = read_one_inode(root, inode_objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(inode_objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -1411,11 +1402,13 @@ static noinline int add_inode_ref(struct btrfs_trans_handle *trans,\n \t\t\t * parent object can change from one array\n \t\t\t * item to another.\n \t\t\t */\n-\t\t\tif (!dir)\n-\t\t\t\tdir = read_one_inode(root, parent_objectid);\n \t\t\tif (!dir) {\n-\t\t\t\tret = -ENOENT;\n-\t\t\t\tgoto out;\n+\t\t\t\tdir = btrfs_iget_logging(parent_objectid, root);\n+\t\t\t\tif (IS_ERR(dir)) {\n+\t\t\t\t\tret = PTR_ERR(dir);\n+\t\t\t\t\tdir = NULL;\n+\t\t\t\t\tgoto out;\n+\t\t\t\t}\n \t\t\t}\n \t\t} else {\n \t\t\tret = ref_get_fields(eb, ref_ptr, &name, &ref_index);\n@@ -1684,9 +1677,9 @@ static noinline int fixup_inode_link_counts(struct btrfs_trans_handle *trans,\n \t\t\tbreak;\n \n \t\tbtrfs_release_path(path);\n-\t\tinode = read_one_inode(root, key.offset);\n-\t\tif (!inode) {\n-\t\t\tret = -EIO;\n+\t\tinode = btrfs_iget_logging(key.offset, root);\n+\t\tif (IS_ERR(inode)) {\n+\t\t\tret = PTR_ERR(inode);\n \t\t\tbreak;\n \t\t}\n \n@@ -1722,9 +1715,9 @@ static noinline int link_to_fixup_dir(struct btrfs_trans_handle *trans,\n \tstruct btrfs_inode *inode;\n \tstruct inode *vfs_inode;\n \n-\tinode = read_one_inode(root, objectid);\n-\tif (!inode)\n-\t\treturn -EIO;\n+\tinode = btrfs_iget_logging(objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n \tvfs_inode = &inode->vfs_inode;\n \tkey.objectid = BTRFS_TREE_LOG_FIXUP_OBJECTID;\n@@ -1763,14 +1756,14 @@ static noinline int insert_one_name(struct btrfs_trans_handle *trans,\n \tstruct btrfs_inode *dir;\n \tint ret;\n \n-\tinode = read_one_inode(root, location->objectid);\n-\tif (!inode)\n-\t\treturn -ENOENT;\n+\tinode = btrfs_iget_logging(location->objectid, root);\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n-\tdir = read_one_inode(root, dirid);\n-\tif (!dir) {\n+\tdir = btrfs_iget_logging(dirid, root);\n+\tif (IS_ERR(dir)) {\n \t\tiput(&inode->vfs_inode);\n-\t\treturn -EIO;\n+\t\treturn PTR_ERR(dir);\n \t}\n \n \tret = btrfs_add_link(trans, dir, inode, name, 1, index);\n@@ -1847,9 +1840,9 @@ static noinline int replay_one_name(struct btrfs_trans_handle *trans,\n \tbool update_size = true;\n \tbool name_added = false;\n \n-\tdir = read_one_inode(root, key->objectid);\n-\tif (!dir)\n-\t\treturn -EIO;\n+\tdir = btrfs_iget_logging(key->objectid, root);\n+\tif (IS_ERR(dir))\n+\t\treturn PTR_ERR(dir);\n \n \tret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name);\n \tif (ret)\n@@ -2149,9 +2142,10 @@ static noinline int check_item_in_log(struct btrfs_trans_handle *trans,\n \tbtrfs_dir_item_key_to_cpu(eb, di, &location);\n \tbtrfs_release_path(path);\n \tbtrfs_release_path(log_path);\n-\tinode = read_one_inode(root, location.objectid);\n-\tif (!inode) {\n-\t\tret = -EIO;\n+\tinode = btrfs_iget_logging(location.objectid, root);\n+\tif (IS_ERR(inode)) {\n+\t\tret = PTR_ERR(inode);\n+\t\tinode = NULL;\n \t\tgoto out;\n \t}\n \n@@ -2303,14 +2297,17 @@ static noinline int replay_dir_deletes(struct btrfs_trans_handle *trans,\n \tif (!log_path)\n \t\treturn -ENOMEM;\n \n-\tdir = read_one_inode(root, dirid);\n-\t/* it isn't an error if the inode isn't there, that can happen\n-\t * because we replay the deletes before we copy in the inode item\n-\t * from the log\n+\tdir = btrfs_iget_logging(dirid, root);\n+\t/*\n+\t * It isn't an error if the inode isn't there, that can happen because\n+\t * we replay the deletes before we copy in the inode item from the log.\n \t */\n-\tif (!dir) {\n+\tif (IS_ERR(dir)) {\n \t\tbtrfs_free_path(log_path);\n-\t\treturn 0;\n+\t\tret = PTR_ERR(dir);\n+\t\tif (ret == -ENOENT)\n+\t\t\tret = 0;\n+\t\treturn ret;\n \t}\n \n \trange_start = 0;\n@@ -2469,9 +2466,9 @@ static int replay_one_buffer(struct btrfs_root *log, struct extent_buffer *eb,\n \t\t\t\tstruct btrfs_inode *inode;\n \t\t\t\tu64 from;\n \n-\t\t\t\tinode = read_one_inode(root, key.objectid);\n-\t\t\t\tif (!inode) {\n-\t\t\t\t\tret = -EIO;\n+\t\t\t\tinode = btrfs_iget_logging(key.objectid, root);\n+\t\t\t\tif (IS_ERR(inode)) {\n+\t\t\t\t\tret = PTR_ERR(inode);\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t\tfrom = ALIGN(i_size_read(&inode->vfs_inode),",
    "stats": {
      "insertions": 62,
      "deletions": 65,
      "files": 1
    }
  },
  {
    "sha": "6561a40ceced9082f50c374a22d5966cf9fc5f5c",
    "message": "btrfs: fix missing error handling when searching for inode refs during log replay\n\nDuring log replay, at __add_inode_ref(), when we are searching for inode\nref keys we totally ignore if btrfs_search_slot() returns an error. This\nmay make a log replay succeed when there was an actual error and leave\nsome metadata inconsistency in a subvolume tree. Fix this by checking if\nan error was returned from btrfs_search_slot() and if so, return it to\nthe caller.\n\nFixes: e02119d5a7b4 (\"Btrfs: Add a write ahead tree log to optimize synchronous operations\")\nReviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>\nReviewed-by: Qu Wenruo <wqu@suse.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
    "author": "Filipe Manana",
    "date": "2025-06-27T19:56:35+02:00",
    "files_changed": [
      "fs/btrfs/tree-log.c"
    ],
    "diff": "diff --git a/fs/btrfs/tree-log.c b/fs/btrfs/tree-log.c\nindex 858b609e292c..8b66173d9023 100644\n--- a/fs/btrfs/tree-log.c\n+++ b/fs/btrfs/tree-log.c\n@@ -1073,7 +1073,9 @@ static inline int __add_inode_ref(struct btrfs_trans_handle *trans,\n \tsearch_key.type = BTRFS_INODE_REF_KEY;\n \tsearch_key.offset = parent_objectid;\n \tret = btrfs_search_slot(NULL, root, &search_key, path, 0, 0);\n-\tif (ret == 0) {\n+\tif (ret < 0) {\n+\t\treturn ret;\n+\t} else if (ret == 0) {\n \t\tstruct btrfs_inode_ref *victim_ref;\n \t\tunsigned long ptr;\n \t\tunsigned long ptr_end;",
    "stats": {
      "insertions": 3,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "e540341508ce2f6e27810106253d5de194b66750",
    "message": "Merge tag 'block-6.16-20250626' of git://git.kernel.dk/linux\n\nPull block fixes from Jens Axboe:\n\n - Fixes for ublk:\n      - fix C++ narrowing warnings in the uapi header\n      - update/improve UBLK_F_SUPPORT_ZERO_COPY comment in uapi header\n      - fix for the ublk ->queue_rqs() implementation, limiting a batch\n        to just the specific task AND ring\n      - ublk_get_data() error handling fix\n      - sanity check more arguments in ublk_ctrl_add_dev()\n      - selftest addition\n\n - NVMe pull request via Christoph:\n      - reset delayed remove_work after reconnect\n      - fix atomic write size validation\n\n - Fix for a warning introduced in bdev_count_inflight_rw() in this\n   merge window\n\n* tag 'block-6.16-20250626' of git://git.kernel.dk/linux:\n  block: fix false warning in bdev_count_inflight_rw()\n  ublk: sanity check add_dev input for underflow\n  nvme: fix atomic write size validation\n  nvme: refactor the atomic write unit detection\n  nvme: reset delayed remove_work after reconnect\n  ublk: setup ublk_io correctly in case of ublk_get_data() failure\n  ublk: update UBLK_F_SUPPORT_ZERO_COPY comment in UAPI header\n  ublk: fix narrowing warnings in UAPI header\n  selftests: ublk: don't take same backing file for more than one ublk devices\n  ublk: build batch from IOs in same io_ring_ctx and io task",
    "author": "Linus Torvalds",
    "date": "2025-06-27T09:02:33-07:00",
    "files_changed": [
      "block/genhd.c",
      "drivers/block/ublk_drv.c",
      "drivers/nvme/host/core.c",
      "drivers/nvme/host/multipath.c",
      "drivers/nvme/host/nvme.h",
      "include/uapi/linux/ublk_cmd.h"
    ],
    "diff": "diff --git a/block/genhd.c b/block/genhd.c\nindex 8171a6bc3210..c26733f6324b 100644\n--- a/block/genhd.c\n+++ b/block/genhd.c\n@@ -128,23 +128,27 @@ static void part_stat_read_all(struct block_device *part,\n static void bdev_count_inflight_rw(struct block_device *part,\n \t\tunsigned int inflight[2], bool mq_driver)\n {\n+\tint write = 0;\n+\tint read = 0;\n \tint cpu;\n \n \tif (mq_driver) {\n \t\tblk_mq_in_driver_rw(part, inflight);\n-\t} else {\n-\t\tfor_each_possible_cpu(cpu) {\n-\t\t\tinflight[READ] += part_stat_local_read_cpu(\n-\t\t\t\t\t\tpart, in_flight[READ], cpu);\n-\t\t\tinflight[WRITE] += part_stat_local_read_cpu(\n-\t\t\t\t\t\tpart, in_flight[WRITE], cpu);\n-\t\t}\n+\t\treturn;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tread += part_stat_local_read_cpu(part, in_flight[READ], cpu);\n+\t\twrite += part_stat_local_read_cpu(part, in_flight[WRITE], cpu);\n \t}\n \n-\tif (WARN_ON_ONCE((int)inflight[READ] < 0))\n-\t\tinflight[READ] = 0;\n-\tif (WARN_ON_ONCE((int)inflight[WRITE] < 0))\n-\t\tinflight[WRITE] = 0;\n+\t/*\n+\t * While iterating all CPUs, some IOs may be issued from a CPU already\n+\t * traversed and complete on a CPU that has not yet been traversed,\n+\t * causing the inflight number to be negative.\n+\t */\n+\tinflight[READ] = read > 0 ? read : 0;\n+\tinflight[WRITE] = write > 0 ? write : 0;\n }\n \n /**\ndiff --git a/drivers/block/ublk_drv.c b/drivers/block/ublk_drv.c\nindex d36f44f5ee80..c3e3c3b65a6d 100644\n--- a/drivers/block/ublk_drv.c\n+++ b/drivers/block/ublk_drv.c\n@@ -1148,8 +1148,8 @@ static inline void __ublk_complete_rq(struct request *req)\n \tblk_mq_end_request(req, res);\n }\n \n-static void ublk_complete_io_cmd(struct ublk_io *io, struct request *req,\n-\t\t\t\t int res, unsigned issue_flags)\n+static struct io_uring_cmd *__ublk_prep_compl_io_cmd(struct ublk_io *io,\n+\t\t\t\t\t\t     struct request *req)\n {\n \t/* read cmd first because req will overwrite it */\n \tstruct io_uring_cmd *cmd = io->cmd;\n@@ -1164,6 +1164,13 @@ static void ublk_complete_io_cmd(struct ublk_io *io, struct request *req,\n \tio->flags &= ~UBLK_IO_FLAG_ACTIVE;\n \n \tio->req = req;\n+\treturn cmd;\n+}\n+\n+static void ublk_complete_io_cmd(struct ublk_io *io, struct request *req,\n+\t\t\t\t int res, unsigned issue_flags)\n+{\n+\tstruct io_uring_cmd *cmd = __ublk_prep_compl_io_cmd(io, req);\n \n \t/* tell ublksrv one io request is coming */\n \tio_uring_cmd_done(cmd, res, 0, issue_flags);\n@@ -1416,6 +1423,14 @@ static blk_status_t ublk_queue_rq(struct blk_mq_hw_ctx *hctx,\n \treturn BLK_STS_OK;\n }\n \n+static inline bool ublk_belong_to_same_batch(const struct ublk_io *io,\n+\t\t\t\t\t     const struct ublk_io *io2)\n+{\n+\treturn (io_uring_cmd_ctx_handle(io->cmd) ==\n+\t\tio_uring_cmd_ctx_handle(io2->cmd)) &&\n+\t\t(io->task == io2->task);\n+}\n+\n static void ublk_queue_rqs(struct rq_list *rqlist)\n {\n \tstruct rq_list requeue_list = { };\n@@ -1427,7 +1442,8 @@ static void ublk_queue_rqs(struct rq_list *rqlist)\n \t\tstruct ublk_queue *this_q = req->mq_hctx->driver_data;\n \t\tstruct ublk_io *this_io = &this_q->ios[req->tag];\n \n-\t\tif (io && io->task != this_io->task && !rq_list_empty(&submit_list))\n+\t\tif (io && !ublk_belong_to_same_batch(io, this_io) &&\n+\t\t\t\t!rq_list_empty(&submit_list))\n \t\t\tublk_queue_cmd_list(io, &submit_list);\n \t\tio = this_io;\n \n@@ -2148,10 +2164,9 @@ static int ublk_commit_and_fetch(const struct ublk_queue *ubq,\n \treturn 0;\n }\n \n-static bool ublk_get_data(const struct ublk_queue *ubq, struct ublk_io *io)\n+static bool ublk_get_data(const struct ublk_queue *ubq, struct ublk_io *io,\n+\t\t\t  struct request *req)\n {\n-\tstruct request *req = io->req;\n-\n \t/*\n \t * We have handled UBLK_IO_NEED_GET_DATA command,\n \t * so clear UBLK_IO_FLAG_NEED_GET_DATA now and just\n@@ -2178,6 +2193,7 @@ static int __ublk_ch_uring_cmd(struct io_uring_cmd *cmd,\n \tu32 cmd_op = cmd->cmd_op;\n \tunsigned tag = ub_cmd->tag;\n \tint ret = -EINVAL;\n+\tstruct request *req;\n \n \tpr_devel(\"%s: received: cmd op %d queue %d tag %d result %d\\n\",\n \t\t\t__func__, cmd->cmd_op, ub_cmd->q_id, tag,\n@@ -2236,11 +2252,19 @@ static int __ublk_ch_uring_cmd(struct io_uring_cmd *cmd,\n \t\t\tgoto out;\n \t\tbreak;\n \tcase UBLK_IO_NEED_GET_DATA:\n-\t\tio->addr = ub_cmd->addr;\n-\t\tif (!ublk_get_data(ubq, io))\n-\t\t\treturn -EIOCBQUEUED;\n-\n-\t\treturn UBLK_IO_RES_OK;\n+\t\t/*\n+\t\t * ublk_get_data() may fail and fallback to requeue, so keep\n+\t\t * uring_cmd active first and prepare for handling new requeued\n+\t\t * request\n+\t\t */\n+\t\treq = io->req;\n+\t\tublk_fill_io_cmd(io, cmd, ub_cmd->addr);\n+\t\tio->flags &= ~UBLK_IO_FLAG_OWNED_BY_SRV;\n+\t\tif (likely(ublk_get_data(ubq, io, req))) {\n+\t\t\t__ublk_prep_compl_io_cmd(io, req);\n+\t\t\treturn UBLK_IO_RES_OK;\n+\t\t}\n+\t\tbreak;\n \tdefault:\n \t\tgoto out;\n \t}\n@@ -2825,7 +2849,8 @@ static int ublk_ctrl_add_dev(const struct ublksrv_ctrl_cmd *header)\n \tif (copy_from_user(&info, argp, sizeof(info)))\n \t\treturn -EFAULT;\n \n-\tif (info.queue_depth > UBLK_MAX_QUEUE_DEPTH || info.nr_hw_queues > UBLK_MAX_NR_QUEUES)\n+\tif (info.queue_depth > UBLK_MAX_QUEUE_DEPTH || !info.queue_depth ||\n+\t    info.nr_hw_queues > UBLK_MAX_NR_QUEUES || !info.nr_hw_queues)\n \t\treturn -EINVAL;\n \n \tif (capable(CAP_SYS_ADMIN))\ndiff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c\nindex 92697f98c601..e533d791955d 100644\n--- a/drivers/nvme/host/core.c\n+++ b/drivers/nvme/host/core.c\n@@ -2015,21 +2015,41 @@ static void nvme_configure_metadata(struct nvme_ctrl *ctrl,\n }\n \n \n-static void nvme_update_atomic_write_disk_info(struct nvme_ns *ns,\n-\t\t\tstruct nvme_id_ns *id, struct queue_limits *lim,\n-\t\t\tu32 bs, u32 atomic_bs)\n+static u32 nvme_configure_atomic_write(struct nvme_ns *ns,\n+\t\tstruct nvme_id_ns *id, struct queue_limits *lim, u32 bs)\n {\n-\tunsigned int boundary = 0;\n+\tu32 atomic_bs, boundary = 0;\n \n-\tif (id->nsfeat & NVME_NS_FEAT_ATOMICS && id->nawupf) {\n-\t\tif (le16_to_cpu(id->nabspf))\n+\t/*\n+\t * We do not support an offset for the atomic boundaries.\n+\t */\n+\tif (id->nabo)\n+\t\treturn bs;\n+\n+\tif ((id->nsfeat & NVME_NS_FEAT_ATOMICS) && id->nawupf) {\n+\t\t/*\n+\t\t * Use the per-namespace atomic write unit when available.\n+\t\t */\n+\t\tatomic_bs = (1 + le16_to_cpu(id->nawupf)) * bs;\n+\t\tif (id->nabspf)\n \t\t\tboundary = (le16_to_cpu(id->nabspf) + 1) * bs;\n+\t} else {\n+\t\t/*\n+\t\t * Use the controller wide atomic write unit.  This sucks\n+\t\t * because the limit is defined in terms of logical blocks while\n+\t\t * namespaces can have different formats, and because there is\n+\t\t * no clear language in the specification prohibiting different\n+\t\t * values for different controllers in the subsystem.\n+\t\t */\n+\t\tatomic_bs = (1 + ns->ctrl->subsys->awupf) * bs;\n \t}\n+\n \tlim->atomic_write_hw_max = atomic_bs;\n \tlim->atomic_write_hw_boundary = boundary;\n \tlim->atomic_write_hw_unit_min = bs;\n \tlim->atomic_write_hw_unit_max = rounddown_pow_of_two(atomic_bs);\n \tlim->features |= BLK_FEAT_ATOMIC_WRITES;\n+\treturn atomic_bs;\n }\n \n static u32 nvme_max_drv_segments(struct nvme_ctrl *ctrl)\n@@ -2067,34 +2087,8 @@ static bool nvme_update_disk_info(struct nvme_ns *ns, struct nvme_id_ns *id,\n \t\tvalid = false;\n \t}\n \n-\tatomic_bs = phys_bs = bs;\n-\tif (id->nabo == 0) {\n-\t\t/*\n-\t\t * Bit 1 indicates whether NAWUPF is defined for this namespace\n-\t\t * and whether it should be used instead of AWUPF. If NAWUPF ==\n-\t\t * 0 then AWUPF must be used instead.\n-\t\t */\n-\t\tif (id->nsfeat & NVME_NS_FEAT_ATOMICS && id->nawupf)\n-\t\t\tatomic_bs = (1 + le16_to_cpu(id->nawupf)) * bs;\n-\t\telse\n-\t\t\tatomic_bs = (1 + ns->ctrl->awupf) * bs;\n-\n-\t\t/*\n-\t\t * Set subsystem atomic bs.\n-\t\t */\n-\t\tif (ns->ctrl->subsys->atomic_bs) {\n-\t\t\tif (atomic_bs != ns->ctrl->subsys->atomic_bs) {\n-\t\t\t\tdev_err_ratelimited(ns->ctrl->device,\n-\t\t\t\t\t\"%s: Inconsistent Atomic Write Size, Namespace will not be added: Subsystem=%d bytes, Controller/Namespace=%d bytes\\n\",\n-\t\t\t\t\tns->disk ? ns->disk->disk_name : \"?\",\n-\t\t\t\t\tns->ctrl->subsys->atomic_bs,\n-\t\t\t\t\tatomic_bs);\n-\t\t\t}\n-\t\t} else\n-\t\t\tns->ctrl->subsys->atomic_bs = atomic_bs;\n-\n-\t\tnvme_update_atomic_write_disk_info(ns, id, lim, bs, atomic_bs);\n-\t}\n+\tphys_bs = bs;\n+\tatomic_bs = nvme_configure_atomic_write(ns, id, lim, bs);\n \n \tif (id->nsfeat & NVME_NS_FEAT_IO_OPT) {\n \t\t/* NPWG = Namespace Preferred Write Granularity */\n@@ -2382,16 +2376,6 @@ static int nvme_update_ns_info_block(struct nvme_ns *ns,\n \tif (!nvme_update_disk_info(ns, id, &lim))\n \t\tcapacity = 0;\n \n-\t/*\n-\t * Validate the max atomic write size fits within the subsystem's\n-\t * atomic write capabilities.\n-\t */\n-\tif (lim.atomic_write_hw_max > ns->ctrl->subsys->atomic_bs) {\n-\t\tblk_mq_unfreeze_queue(ns->disk->queue, memflags);\n-\t\tret = -ENXIO;\n-\t\tgoto out;\n-\t}\n-\n \tnvme_config_discard(ns, &lim);\n \tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED) &&\n \t    ns->head->ids.csi == NVME_CSI_ZNS)\n@@ -3215,6 +3199,7 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)\n \tmemcpy(subsys->model, id->mn, sizeof(subsys->model));\n \tsubsys->vendor_id = le16_to_cpu(id->vid);\n \tsubsys->cmic = id->cmic;\n+\tsubsys->awupf = le16_to_cpu(id->awupf);\n \n \t/* Versions prior to 1.4 don't necessarily report a valid type */\n \tif (id->cntrltype == NVME_CTRL_DISC ||\n@@ -3552,6 +3537,15 @@ static int nvme_init_identify(struct nvme_ctrl *ctrl)\n \t\tif (ret)\n \t\t\tgoto out_free;\n \t}\n+\n+\tif (le16_to_cpu(id->awupf) != ctrl->subsys->awupf) {\n+\t\tdev_err_ratelimited(ctrl->device,\n+\t\t\t\"inconsistent AWUPF, controller not added (%u/%u).\\n\",\n+\t\t\tle16_to_cpu(id->awupf), ctrl->subsys->awupf);\n+\t\tret = -EINVAL;\n+\t\tgoto out_free;\n+\t}\n+\n \tmemcpy(ctrl->subsys->firmware_rev, id->fr,\n \t       sizeof(ctrl->subsys->firmware_rev));\n \n@@ -3647,7 +3641,6 @@ static int nvme_init_identify(struct nvme_ctrl *ctrl)\n \t\tdev_pm_qos_expose_latency_tolerance(ctrl->device);\n \telse if (!ctrl->apst_enabled && prev_apst_enabled)\n \t\tdev_pm_qos_hide_latency_tolerance(ctrl->device);\n-\tctrl->awupf = le16_to_cpu(id->awupf);\n out_free:\n \tkfree(id);\n \treturn ret;\n@@ -4036,6 +4029,10 @@ static int nvme_init_ns_head(struct nvme_ns *ns, struct nvme_ns_info *info)\n \tlist_add_tail_rcu(&ns->siblings, &head->list);\n \tns->head = head;\n \tmutex_unlock(&ctrl->subsys->lock);\n+\n+#ifdef CONFIG_NVME_MULTIPATH\n+\tcancel_delayed_work(&head->remove_work);\n+#endif\n \treturn 0;\n \n out_put_ns_head:\ndiff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c\nindex e040e467f9fa..316a269842fa 100644\n--- a/drivers/nvme/host/multipath.c\n+++ b/drivers/nvme/host/multipath.c\n@@ -1311,7 +1311,7 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)\n \t\t */\n \t\tif (!try_module_get(THIS_MODULE))\n \t\t\tgoto out;\n-\t\tqueue_delayed_work(nvme_wq, &head->remove_work,\n+\t\tmod_delayed_work(nvme_wq, &head->remove_work,\n \t\t\t\thead->delayed_removal_secs * HZ);\n \t} else {\n \t\tlist_del_init(&head->entry);\ndiff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h\nindex a468cdc5b5cb..7df2ea21851f 100644\n--- a/drivers/nvme/host/nvme.h\n+++ b/drivers/nvme/host/nvme.h\n@@ -410,7 +410,6 @@ struct nvme_ctrl {\n \n \tenum nvme_ctrl_type cntrltype;\n \tenum nvme_dctype dctype;\n-\tu16 awupf; /* 0's based value. */\n };\n \n static inline enum nvme_ctrl_state nvme_ctrl_state(struct nvme_ctrl *ctrl)\n@@ -443,11 +442,11 @@ struct nvme_subsystem {\n \tu8\t\t\tcmic;\n \tenum nvme_subsys_type\tsubtype;\n \tu16\t\t\tvendor_id;\n+\tu16\t\t\tawupf; /* 0's based value. */\n \tstruct ida\t\tns_ida;\n #ifdef CONFIG_NVME_MULTIPATH\n \tenum nvme_iopolicy\tiopolicy;\n #endif\n-\tu32\t\t\tatomic_bs;\n };\n \n /*\ndiff --git a/include/uapi/linux/ublk_cmd.h b/include/uapi/linux/ublk_cmd.h\nindex 77d9d6af46da..c9751bdfd937 100644\n--- a/include/uapi/linux/ublk_cmd.h\n+++ b/include/uapi/linux/ublk_cmd.h\n@@ -135,8 +135,28 @@\n #define UBLKSRV_IO_BUF_TOTAL_SIZE\t(1ULL << UBLKSRV_IO_BUF_TOTAL_BITS)\n \n /*\n- * zero copy requires 4k block size, and can remap ublk driver's io\n- * request into ublksrv's vm space\n+ * ublk server can register data buffers for incoming I/O requests with a sparse\n+ * io_uring buffer table. The request buffer can then be used as the data buffer\n+ * for io_uring operations via the fixed buffer index.\n+ * Note that the ublk server can never directly access the request data memory.\n+ *\n+ * To use this feature, the ublk server must first register a sparse buffer\n+ * table on an io_uring instance.\n+ * When an incoming ublk request is received, the ublk server submits a\n+ * UBLK_U_IO_REGISTER_IO_BUF command to that io_uring instance. The\n+ * ublksrv_io_cmd's q_id and tag specify the request whose buffer to register\n+ * and addr is the index in the io_uring's buffer table to install the buffer.\n+ * SQEs can now be submitted to the io_uring to read/write the request's buffer\n+ * by enabling fixed buffers (e.g. using IORING_OP_{READ,WRITE}_FIXED or\n+ * IORING_URING_CMD_FIXED) and passing the registered buffer index in buf_index.\n+ * Once the last io_uring operation using the request's buffer has completed,\n+ * the ublk server submits a UBLK_U_IO_UNREGISTER_IO_BUF command with q_id, tag,\n+ * and addr again specifying the request buffer to unregister.\n+ * The ublk request is completed when its buffer is unregistered from all\n+ * io_uring instances and the ublk server issues UBLK_U_IO_COMMIT_AND_FETCH_REQ.\n+ *\n+ * Not available for UBLK_F_UNPRIVILEGED_DEV, as a ublk server can leak\n+ * uninitialized kernel memory by not reading into the full request buffer.\n  */\n #define UBLK_F_SUPPORT_ZERO_COPY\t(1ULL << 0)\n \n@@ -450,10 +470,10 @@ static inline struct ublk_auto_buf_reg ublk_sqe_addr_to_auto_buf_reg(\n \t\t__u64 sqe_addr)\n {\n \tstruct ublk_auto_buf_reg reg = {\n-\t\t.index = sqe_addr & 0xffff,\n-\t\t.flags = (sqe_addr >> 16) & 0xff,\n-\t\t.reserved0 = (sqe_addr >> 24) & 0xff,\n-\t\t.reserved1 = sqe_addr >> 32,\n+\t\t.index = (__u16)sqe_addr,\n+\t\t.flags = (__u8)(sqe_addr >> 16),\n+\t\t.reserved0 = (__u8)(sqe_addr >> 24),\n+\t\t.reserved1 = (__u32)(sqe_addr >> 32),\n \t};\n \n \treturn reg;\ndiff --git a/tools/testing/selftests/ublk/test_stress_03.sh b/tools/testing/selftests/ublk/test_stress_03.sh\nindex 6eef282d569f..3ed4c9b2d8c0 100755\n--- a/tools/testing/selftests/ublk/test_stress_03.sh\n+++ b/tools/testing/selftests/ublk/test_stress_03.sh\n@@ -32,22 +32,23 @@ _create_backfile 2 128M\n ublk_io_and_remove 8G -t null -q 4 -z &\n ublk_io_and_remove 256M -t loop -q 4 -z \"${UBLK_BACKFILES[0]}\" &\n ublk_io_and_remove 256M -t stripe -q 4 -z \"${UBLK_BACKFILES[1]}\" \"${UBLK_BACKFILES[2]}\" &\n+wait\n \n if _have_feature \"AUTO_BUF_REG\"; then\n \tublk_io_and_remove 8G -t null -q 4 --auto_zc &\n \tublk_io_and_remove 256M -t loop -q 4 --auto_zc \"${UBLK_BACKFILES[0]}\" &\n \tublk_io_and_remove 256M -t stripe -q 4 --auto_zc \"${UBLK_BACKFILES[1]}\" \"${UBLK_BACKFILES[2]}\" &\n \tublk_io_and_remove 8G -t null -q 4 -z --auto_zc --auto_zc_fallback &\n+\twait\n fi\n-wait\n \n if _have_feature \"PER_IO_DAEMON\"; then\n \tublk_io_and_remove 8G -t null -q 4 --auto_zc --nthreads 8 --per_io_tasks &\n \tublk_io_and_remove 256M -t loop -q 4 --auto_zc --nthreads 8 --per_io_tasks \"${UBLK_BACKFILES[0]}\" &\n \tublk_io_and_remove 256M -t stripe -q 4 --auto_zc --nthreads 8 --per_io_tasks \"${UBLK_BACKFILES[1]}\" \"${UBLK_BACKFILES[2]}\" &\n \tublk_io_and_remove 8G -t null -q 4 -z --auto_zc --auto_zc_fallback --nthreads 8 --per_io_tasks &\n+\twait\n fi\n-wait\n \n _cleanup_test \"stress\"\n _show_result $TID $ERR_CODE",
    "stats": {
      "insertions": 125,
      "deletions": 79,
      "files": 7
    }
  },
  {
    "sha": "0a47e02d8a283a99592876556b9d42e087525828",
    "message": "Merge tag 'io_uring-6.16-20250626' of git://git.kernel.dk/linux\n\nPull io_uring fixes from Jens Axboe:\n\n - Two tweaks for a recent fix: fixing a memory leak if multiple iovecs\n   were initially mapped but only the first was used and hence turned\n   into a UBUF rathan than an IOVEC iterator, and catching a case where\n   a retry would be done even if the previous segment wasn't full\n\n - Small series fixing an issue making the vm unhappy if debugging is\n   turned on, hitting a VM_BUG_ON_PAGE()\n\n - Fix a resource leak in io_import_dmabuf() in the error handling case,\n   which is a regression in this merge window\n\n - Mark fallocate as needing to be write serialized, as is already done\n   for truncate and buffered writes\n\n* tag 'io_uring-6.16-20250626' of git://git.kernel.dk/linux:\n  io_uring/kbuf: flag partial buffer mappings\n  io_uring/net: mark iov as dynamically allocated even for single segments\n  io_uring: fix resource leak in io_import_dmabuf()\n  io_uring: don't assume uaddr alignment in io_vec_fill_bvec\n  io_uring/rsrc: don't rely on user vaddr alignment\n  io_uring/rsrc: fix folio unpinning\n  io_uring: make fallocate be hashed work",
    "author": "Linus Torvalds",
    "date": "2025-06-27T08:55:57-07:00",
    "files_changed": [
      "io_uring/kbuf.c",
      "io_uring/kbuf.h",
      "io_uring/net.c",
      "io_uring/opdef.c",
      "io_uring/rsrc.c",
      "io_uring/rsrc.h",
      "io_uring/zcrx.c"
    ],
    "diff": "diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c\nindex ce95e3af44a9..f2d2cc319faa 100644\n--- a/io_uring/kbuf.c\n+++ b/io_uring/kbuf.c\n@@ -271,6 +271,7 @@ static int io_ring_buffers_peek(struct io_kiocb *req, struct buf_sel_arg *arg,\n \t\tif (len > arg->max_len) {\n \t\t\tlen = arg->max_len;\n \t\t\tif (!(bl->flags & IOBL_INC)) {\n+\t\t\t\targ->partial_map = 1;\n \t\t\t\tif (iov != arg->iovs)\n \t\t\t\t\tbreak;\n \t\t\t\tbuf->len = len;\ndiff --git a/io_uring/kbuf.h b/io_uring/kbuf.h\nindex 5d83c7adc739..723d0361898e 100644\n--- a/io_uring/kbuf.h\n+++ b/io_uring/kbuf.h\n@@ -58,7 +58,8 @@ struct buf_sel_arg {\n \tsize_t max_len;\n \tunsigned short nr_iovs;\n \tunsigned short mode;\n-\tunsigned buf_group;\n+\tunsigned short buf_group;\n+\tunsigned short partial_map;\n };\n \n void __user *io_buffer_select(struct io_kiocb *req, size_t *len,\ndiff --git a/io_uring/net.c b/io_uring/net.c\nindex 9550d4c8f866..43a43522f406 100644\n--- a/io_uring/net.c\n+++ b/io_uring/net.c\n@@ -75,12 +75,17 @@ struct io_sr_msg {\n \tu16\t\t\t\tflags;\n \t/* initialised and used only by !msg send variants */\n \tu16\t\t\t\tbuf_group;\n-\tbool\t\t\t\tretry;\n+\tunsigned short\t\t\tretry_flags;\n \tvoid __user\t\t\t*msg_control;\n \t/* used only for send zerocopy */\n \tstruct io_kiocb \t\t*notif;\n };\n \n+enum sr_retry_flags {\n+\tIO_SR_MSG_RETRY\t\t= 1,\n+\tIO_SR_MSG_PARTIAL_MAP\t= 2,\n+};\n+\n /*\n  * Number of times we'll try and do receives if there's more data. If we\n  * exceed this limit, then add us to the back of the queue and retry from\n@@ -187,7 +192,7 @@ static inline void io_mshot_prep_retry(struct io_kiocb *req,\n \n \treq->flags &= ~REQ_F_BL_EMPTY;\n \tsr->done_io = 0;\n-\tsr->retry = false;\n+\tsr->retry_flags = 0;\n \tsr->len = 0; /* get from the provided buffer */\n }\n \n@@ -397,7 +402,7 @@ int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n \tstruct io_sr_msg *sr = io_kiocb_to_cmd(req, struct io_sr_msg);\n \n \tsr->done_io = 0;\n-\tsr->retry = false;\n+\tsr->retry_flags = 0;\n \tsr->len = READ_ONCE(sqe->len);\n \tsr->flags = READ_ONCE(sqe->ioprio);\n \tif (sr->flags & ~SENDMSG_FLAGS)\n@@ -751,7 +756,7 @@ int io_recvmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n \tstruct io_sr_msg *sr = io_kiocb_to_cmd(req, struct io_sr_msg);\n \n \tsr->done_io = 0;\n-\tsr->retry = false;\n+\tsr->retry_flags = 0;\n \n \tif (unlikely(sqe->file_index || sqe->addr2))\n \t\treturn -EINVAL;\n@@ -823,7 +828,7 @@ static inline bool io_recv_finish(struct io_kiocb *req, int *ret,\n \n \t\tcflags |= io_put_kbufs(req, this_ret, io_bundle_nbufs(kmsg, this_ret),\n \t\t\t\t      issue_flags);\n-\t\tif (sr->retry)\n+\t\tif (sr->retry_flags & IO_SR_MSG_RETRY)\n \t\t\tcflags = req->cqe.flags | (cflags & CQE_F_MASK);\n \t\t/* bundle with no more immediate buffers, we're done */\n \t\tif (req->flags & REQ_F_BL_EMPTY)\n@@ -832,12 +837,12 @@ static inline bool io_recv_finish(struct io_kiocb *req, int *ret,\n \t\t * If more is available AND it was a full transfer, retry and\n \t\t * append to this one\n \t\t */\n-\t\tif (!sr->retry && kmsg->msg.msg_inq > 1 && this_ret > 0 &&\n+\t\tif (!sr->retry_flags && kmsg->msg.msg_inq > 1 && this_ret > 0 &&\n \t\t    !iov_iter_count(&kmsg->msg.msg_iter)) {\n \t\t\treq->cqe.flags = cflags & ~CQE_F_MASK;\n \t\t\tsr->len = kmsg->msg.msg_inq;\n \t\t\tsr->done_io += this_ret;\n-\t\t\tsr->retry = true;\n+\t\t\tsr->retry_flags |= IO_SR_MSG_RETRY;\n \t\t\treturn false;\n \t\t}\n \t} else {\n@@ -1077,6 +1082,14 @@ static int io_recv_buf_select(struct io_kiocb *req, struct io_async_msghdr *kmsg\n \t\tif (unlikely(ret < 0))\n \t\t\treturn ret;\n \n+\t\tif (arg.iovs != &kmsg->fast_iov && arg.iovs != kmsg->vec.iovec) {\n+\t\t\tkmsg->vec.nr = ret;\n+\t\t\tkmsg->vec.iovec = arg.iovs;\n+\t\t\treq->flags |= REQ_F_NEED_CLEANUP;\n+\t\t}\n+\t\tif (arg.partial_map)\n+\t\t\tsr->retry_flags |= IO_SR_MSG_PARTIAL_MAP;\n+\n \t\t/* special case 1 vec, can be a fast path */\n \t\tif (ret == 1) {\n \t\t\tsr->buf = arg.iovs[0].iov_base;\n@@ -1085,11 +1098,6 @@ static int io_recv_buf_select(struct io_kiocb *req, struct io_async_msghdr *kmsg\n \t\t}\n \t\tiov_iter_init(&kmsg->msg.msg_iter, ITER_DEST, arg.iovs, ret,\n \t\t\t\targ.out_len);\n-\t\tif (arg.iovs != &kmsg->fast_iov && arg.iovs != kmsg->vec.iovec) {\n-\t\t\tkmsg->vec.nr = ret;\n-\t\t\tkmsg->vec.iovec = arg.iovs;\n-\t\t\treq->flags |= REQ_F_NEED_CLEANUP;\n-\t\t}\n \t} else {\n \t\tvoid __user *buf;\n \n@@ -1275,7 +1283,7 @@ int io_send_zc_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n \tint ret;\n \n \tzc->done_io = 0;\n-\tzc->retry = false;\n+\tzc->retry_flags = 0;\n \n \tif (unlikely(READ_ONCE(sqe->__pad2[0]) || READ_ONCE(sqe->addr3)))\n \t\treturn -EINVAL;\ndiff --git a/io_uring/opdef.c b/io_uring/opdef.c\nindex 6e0882b051f9..6de6229207a8 100644\n--- a/io_uring/opdef.c\n+++ b/io_uring/opdef.c\n@@ -216,6 +216,7 @@ const struct io_issue_def io_issue_defs[] = {\n \t},\n \t[IORING_OP_FALLOCATE] = {\n \t\t.needs_file\t\t= 1,\n+\t\t.hash_reg_file          = 1,\n \t\t.prep\t\t\t= io_fallocate_prep,\n \t\t.issue\t\t\t= io_fallocate,\n \t},\ndiff --git a/io_uring/rsrc.c b/io_uring/rsrc.c\nindex d724602697e7..f2b31fb68992 100644\n--- a/io_uring/rsrc.c\n+++ b/io_uring/rsrc.c\n@@ -112,8 +112,11 @@ static void io_release_ubuf(void *priv)\n \tstruct io_mapped_ubuf *imu = priv;\n \tunsigned int i;\n \n-\tfor (i = 0; i < imu->nr_bvecs; i++)\n-\t\tunpin_user_page(imu->bvec[i].bv_page);\n+\tfor (i = 0; i < imu->nr_bvecs; i++) {\n+\t\tstruct folio *folio = page_folio(imu->bvec[i].bv_page);\n+\n+\t\tunpin_user_folio(folio, 1);\n+\t}\n }\n \n static struct io_mapped_ubuf *io_alloc_imu(struct io_ring_ctx *ctx,\n@@ -731,6 +734,7 @@ bool io_check_coalesce_buffer(struct page **page_array, int nr_pages,\n \n \tdata->nr_pages_mid = folio_nr_pages(folio);\n \tdata->folio_shift = folio_shift(folio);\n+\tdata->first_folio_page_idx = folio_page_idx(folio, page_array[0]);\n \n \t/*\n \t * Check if pages are contiguous inside a folio, and all folios have\n@@ -824,7 +828,11 @@ static struct io_rsrc_node *io_sqe_buffer_register(struct io_ring_ctx *ctx,\n \tif (coalesced)\n \t\timu->folio_shift = data.folio_shift;\n \trefcount_set(&imu->refs, 1);\n-\toff = (unsigned long) iov->iov_base & ((1UL << imu->folio_shift) - 1);\n+\n+\toff = (unsigned long)iov->iov_base & ~PAGE_MASK;\n+\tif (coalesced)\n+\t\toff += data.first_folio_page_idx << PAGE_SHIFT;\n+\n \tnode->buf = imu;\n \tret = 0;\n \n@@ -840,8 +848,10 @@ static struct io_rsrc_node *io_sqe_buffer_register(struct io_ring_ctx *ctx,\n \tif (ret) {\n \t\tif (imu)\n \t\t\tio_free_imu(ctx, imu);\n-\t\tif (pages)\n-\t\t\tunpin_user_pages(pages, nr_pages);\n+\t\tif (pages) {\n+\t\t\tfor (i = 0; i < nr_pages; i++)\n+\t\t\t\tunpin_user_folio(page_folio(pages[i]), 1);\n+\t\t}\n \t\tio_cache_free(&ctx->node_cache, node);\n \t\tnode = ERR_PTR(ret);\n \t}\n@@ -1329,7 +1339,6 @@ static int io_vec_fill_bvec(int ddir, struct iov_iter *iter,\n {\n \tunsigned long folio_size = 1 << imu->folio_shift;\n \tunsigned long folio_mask = folio_size - 1;\n-\tu64 folio_addr = imu->ubuf & ~folio_mask;\n \tstruct bio_vec *res_bvec = vec->bvec;\n \tsize_t total_len = 0;\n \tunsigned bvec_idx = 0;\n@@ -1351,8 +1360,13 @@ static int io_vec_fill_bvec(int ddir, struct iov_iter *iter,\n \t\tif (unlikely(check_add_overflow(total_len, iov_len, &total_len)))\n \t\t\treturn -EOVERFLOW;\n \n-\t\t/* by using folio address it also accounts for bvec offset */\n-\t\toffset = buf_addr - folio_addr;\n+\t\toffset = buf_addr - imu->ubuf;\n+\t\t/*\n+\t\t * Only the first bvec can have non zero bv_offset, account it\n+\t\t * here and work with full folios below.\n+\t\t */\n+\t\toffset += imu->bvec[0].bv_offset;\n+\n \t\tsrc_bvec = imu->bvec + (offset >> imu->folio_shift);\n \t\toffset &= folio_mask;\n \ndiff --git a/io_uring/rsrc.h b/io_uring/rsrc.h\nindex 0d2138f16322..25e7e998dcfd 100644\n--- a/io_uring/rsrc.h\n+++ b/io_uring/rsrc.h\n@@ -49,6 +49,7 @@ struct io_imu_folio_data {\n \tunsigned int\tnr_pages_mid;\n \tunsigned int\tfolio_shift;\n \tunsigned int\tnr_folios;\n+\tunsigned long\tfirst_folio_page_idx;\n };\n \n bool io_rsrc_cache_init(struct io_ring_ctx *ctx);\ndiff --git a/io_uring/zcrx.c b/io_uring/zcrx.c\nindex 797247a34cb7..085eeed8cd50 100644\n--- a/io_uring/zcrx.c\n+++ b/io_uring/zcrx.c\n@@ -106,8 +106,10 @@ static int io_import_dmabuf(struct io_zcrx_ifq *ifq,\n \tfor_each_sgtable_dma_sg(mem->sgt, sg, i)\n \t\ttotal_size += sg_dma_len(sg);\n \n-\tif (total_size < off + len)\n-\t\treturn -EINVAL;\n+\tif (total_size < off + len) {\n+\t\tret = -EINVAL;\n+\t\tgoto err;\n+\t}\n \n \tmem->dmabuf_offset = off;\n \tmem->size = len;",
    "stats": {
      "insertions": 52,
      "deletions": 24,
      "files": 7
    }
  }
]
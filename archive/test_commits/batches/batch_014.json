[
  {
    "sha": "482deed9dfa065cf3f68372dadac857541c7d504",
    "message": "Merge tag 'bcachefs-2025-07-03' of git://evilpiepirate.org/bcachefs\n\nPull bcachefs fixes from Kent Overstreet:\n \"The 'opts.casefold_disabled' patch is non critical, but would be a\n  6.15 backport; it's to address the casefolding + overlayfs\n  incompatibility that was discovvered late.\n\n  It's late because I was hoping that this would be addressed on the\n  overlayfs side (and will be in 6.17), but user reports keep coming in\n  on this one (lots of people are using docker these days)\"\n\n* tag 'bcachefs-2025-07-03' of git://evilpiepirate.org/bcachefs:\n  bcachefs: opts.casefold_disabled\n  bcachefs: Work around deadlock to btree node rewrites in journal replay\n  bcachefs: Fix incorrect transaction restart handling\n  bcachefs: fix btree_trans_peek_prev_journal()\n  bcachefs: mark invalid_btree_id autofix",
    "author": "Linus Torvalds",
    "date": "2025-07-04T09:29:22-07:00",
    "files_changed": [
      "fs/bcachefs/bcachefs.h",
      "fs/bcachefs/btree_io.c",
      "fs/bcachefs/btree_iter.c",
      "fs/bcachefs/dirent.c",
      "fs/bcachefs/dirent.h",
      "fs/bcachefs/fs.c",
      "fs/bcachefs/fsck.c",
      "fs/bcachefs/inode.c",
      "fs/bcachefs/opts.h",
      "fs/bcachefs/sb-errors_format.h",
      "fs/bcachefs/str_hash.c",
      "fs/bcachefs/str_hash.h",
      "fs/bcachefs/super.c"
    ],
    "diff": "diff --git a/fs/bcachefs/bcachefs.h b/fs/bcachefs/bcachefs.h\nindex 8043943cdf6a..ddfacad0f70c 100644\n--- a/fs/bcachefs/bcachefs.h\n+++ b/fs/bcachefs/bcachefs.h\n@@ -863,9 +863,7 @@ struct bch_fs {\n \tDARRAY(enum bcachefs_metadata_version)\n \t\t\t\tincompat_versions_requested;\n \n-#ifdef CONFIG_UNICODE\n \tstruct unicode_map\t*cf_encoding;\n-#endif\n \n \tstruct bch_sb_handle\tdisk_sb;\n \n@@ -1285,4 +1283,13 @@ static inline bool bch2_discard_opt_enabled(struct bch_fs *c, struct bch_dev *ca\n \t\t: ca->mi.discard;\n }\n \n+static inline bool bch2_fs_casefold_enabled(struct bch_fs *c)\n+{\n+#ifdef CONFIG_UNICODE\n+\treturn !c->opts.casefold_disabled;\n+#else\n+\treturn false;\n+#endif\n+}\n+\n #endif /* _BCACHEFS_H */\ndiff --git a/fs/bcachefs/btree_io.c b/fs/bcachefs/btree_io.c\nindex 08b22bddd747..e874a4357f64 100644\n--- a/fs/bcachefs/btree_io.c\n+++ b/fs/bcachefs/btree_io.c\n@@ -1337,15 +1337,42 @@ int bch2_btree_node_read_done(struct bch_fs *c, struct bch_dev *ca,\n \n \tbtree_node_reset_sib_u64s(b);\n \n-\tscoped_guard(rcu)\n-\t\tbkey_for_each_ptr(bch2_bkey_ptrs(bkey_i_to_s(&b->key)), ptr) {\n-\t\t\tstruct bch_dev *ca2 = bch2_dev_rcu(c, ptr->dev);\n-\n-\t\t\tif (!ca2 || ca2->mi.state != BCH_MEMBER_STATE_rw) {\n-\t\t\t\tset_btree_node_need_rewrite(b);\n-\t\t\t\tset_btree_node_need_rewrite_degraded(b);\n+\t/*\n+\t * XXX:\n+\t *\n+\t * We deadlock if too many btree updates require node rewrites while\n+\t * we're still in journal replay.\n+\t *\n+\t * This is because btree node rewrites generate more updates for the\n+\t * interior updates (alloc, backpointers), and if those updates touch\n+\t * new nodes and generate more rewrites - well, you see the problem.\n+\t *\n+\t * The biggest cause is that we don't use the btree write buffer (for\n+\t * the backpointer updates - this needs some real thought on locking in\n+\t * order to fix.\n+\t *\n+\t * The problem with this workaround (not doing the rewrite for degraded\n+\t * nodes in journal replay) is that those degraded nodes persist, and we\n+\t * don't want that (this is a real bug when a btree node write completes\n+\t * with fewer replicas than we wanted and leaves a degraded node due to\n+\t * device _removal_, i.e. the device went away mid write).\n+\t *\n+\t * It's less of a bug here, but still a problem because we don't yet\n+\t * have a way of tracking degraded data - we another index (all\n+\t * extents/btree nodes, by replicas entry) in order to fix properly\n+\t * (re-replicate degraded data at the earliest possible time).\n+\t */\n+\tif (c->recovery.passes_complete & BIT_ULL(BCH_RECOVERY_PASS_journal_replay)) {\n+\t\tscoped_guard(rcu)\n+\t\t\tbkey_for_each_ptr(bch2_bkey_ptrs(bkey_i_to_s(&b->key)), ptr) {\n+\t\t\t\tstruct bch_dev *ca2 = bch2_dev_rcu(c, ptr->dev);\n+\n+\t\t\t\tif (!ca2 || ca2->mi.state != BCH_MEMBER_STATE_rw) {\n+\t\t\t\t\tset_btree_node_need_rewrite(b);\n+\t\t\t\t\tset_btree_node_need_rewrite_degraded(b);\n+\t\t\t\t}\n \t\t\t}\n-\t\t}\n+\t}\n \n \tif (!ptr_written) {\n \t\tset_btree_node_need_rewrite(b);\ndiff --git a/fs/bcachefs/btree_iter.c b/fs/bcachefs/btree_iter.c\nindex 352f9cd2634f..f8829b667ad3 100644\n--- a/fs/bcachefs/btree_iter.c\n+++ b/fs/bcachefs/btree_iter.c\n@@ -2189,7 +2189,7 @@ void btree_trans_peek_prev_journal(struct btree_trans *trans,\n \tstruct btree_path *path = btree_iter_path(trans, iter);\n \tstruct bkey_i *next_journal =\n \t\tbch2_btree_journal_peek_prev(trans, iter, search_key,\n-\t\t\t\tk->k ? k->k->p : path_l(path)->b->key.k.p);\n+\t\t\t\tk->k ? k->k->p : path_l(path)->b->data->min_key);\n \n \tif (next_journal) {\n \t\titer->k = next_journal->k;\ndiff --git a/fs/bcachefs/dirent.c b/fs/bcachefs/dirent.c\nindex 300f7cc8abdf..a18d0f78704d 100644\n--- a/fs/bcachefs/dirent.c\n+++ b/fs/bcachefs/dirent.c\n@@ -18,7 +18,9 @@ int bch2_casefold(struct btree_trans *trans, const struct bch_hash_info *info,\n {\n \t*out_cf = (struct qstr) QSTR_INIT(NULL, 0);\n \n-#ifdef CONFIG_UNICODE\n+\tif (!bch2_fs_casefold_enabled(trans->c))\n+\t\treturn -EOPNOTSUPP;\n+\n \tunsigned char *buf = bch2_trans_kmalloc(trans, BCH_NAME_MAX + 1);\n \tint ret = PTR_ERR_OR_ZERO(buf);\n \tif (ret)\n@@ -30,9 +32,6 @@ int bch2_casefold(struct btree_trans *trans, const struct bch_hash_info *info,\n \n \t*out_cf = (struct qstr) QSTR_INIT(buf, ret);\n \treturn 0;\n-#else\n-\treturn -EOPNOTSUPP;\n-#endif\n }\n \n static unsigned bch2_dirent_name_bytes(struct bkey_s_c_dirent d)\n@@ -231,7 +230,8 @@ void bch2_dirent_to_text(struct printbuf *out, struct bch_fs *c, struct bkey_s_c\n \tprt_printf(out, \" type %s\", bch2_d_type_str(d.v->d_type));\n }\n \n-int bch2_dirent_init_name(struct bkey_i_dirent *dirent,\n+int bch2_dirent_init_name(struct bch_fs *c,\n+\t\t\t  struct bkey_i_dirent *dirent,\n \t\t\t  const struct bch_hash_info *hash_info,\n \t\t\t  const struct qstr *name,\n \t\t\t  const struct qstr *cf_name)\n@@ -251,7 +251,9 @@ int bch2_dirent_init_name(struct bkey_i_dirent *dirent,\n \t\t       offsetof(struct bch_dirent, d_name) -\n \t\t       name->len);\n \t} else {\n-#ifdef CONFIG_UNICODE\n+\t\tif (!bch2_fs_casefold_enabled(c))\n+\t\t\treturn -EOPNOTSUPP;\n+\n \t\tmemcpy(&dirent->v.d_cf_name_block.d_names[0], name->name, name->len);\n \n \t\tchar *cf_out = &dirent->v.d_cf_name_block.d_names[name->len];\n@@ -277,9 +279,6 @@ int bch2_dirent_init_name(struct bkey_i_dirent *dirent,\n \t\tdirent->v.d_cf_name_block.d_cf_name_len = cpu_to_le16(cf_len);\n \n \t\tEBUG_ON(bch2_dirent_get_casefold_name(dirent_i_to_s_c(dirent)).len != cf_len);\n-#else\n-\treturn -EOPNOTSUPP;\n-#endif\n \t}\n \n \tunsigned u64s = dirent_val_u64s(name->len, cf_len);\n@@ -313,7 +312,7 @@ struct bkey_i_dirent *bch2_dirent_create_key(struct btree_trans *trans,\n \tdirent->v.d_type = type;\n \tdirent->v.d_unused = 0;\n \n-\tint ret = bch2_dirent_init_name(dirent, hash_info, name, cf_name);\n+\tint ret = bch2_dirent_init_name(trans->c, dirent, hash_info, name, cf_name);\n \tif (ret)\n \t\treturn ERR_PTR(ret);\n \ndiff --git a/fs/bcachefs/dirent.h b/fs/bcachefs/dirent.h\nindex 70fb0b581221..1e17199cc5c7 100644\n--- a/fs/bcachefs/dirent.h\n+++ b/fs/bcachefs/dirent.h\n@@ -59,7 +59,8 @@ static inline void dirent_copy_target(struct bkey_i_dirent *dst,\n \tdst->v.d_type = src.v->d_type;\n }\n \n-int bch2_dirent_init_name(struct bkey_i_dirent *,\n+int bch2_dirent_init_name(struct bch_fs *,\n+\t\t\t  struct bkey_i_dirent *,\n \t\t\t  const struct bch_hash_info *,\n \t\t\t  const struct qstr *,\n \t\t\t  const struct qstr *);\ndiff --git a/fs/bcachefs/fs.c b/fs/bcachefs/fs.c\nindex db24a76563f8..e54e4f255b22 100644\n--- a/fs/bcachefs/fs.c\n+++ b/fs/bcachefs/fs.c\n@@ -722,7 +722,6 @@ static struct dentry *bch2_lookup(struct inode *vdir, struct dentry *dentry,\n \tif (IS_ERR(inode))\n \t\tinode = NULL;\n \n-#ifdef CONFIG_UNICODE\n \tif (!inode && IS_CASEFOLDED(vdir)) {\n \t\t/*\n \t\t * Do not cache a negative dentry in casefolded directories\n@@ -737,7 +736,6 @@ static struct dentry *bch2_lookup(struct inode *vdir, struct dentry *dentry,\n \t\t */\n \t\treturn NULL;\n \t}\n-#endif\n \n \treturn d_splice_alias(&inode->v, dentry);\n }\n@@ -2566,9 +2564,10 @@ static int bch2_fs_get_tree(struct fs_context *fc)\n \tsb->s_shrink->seeks = 0;\n \n #ifdef CONFIG_UNICODE\n-\tsb->s_encoding = c->cf_encoding;\n-#endif\n+\tif (bch2_fs_casefold_enabled(c))\n+\t\tsb->s_encoding = c->cf_encoding;\n \tgeneric_set_sb_d_ops(sb);\n+#endif\n \n \tvinode = bch2_vfs_inode_get(c, BCACHEFS_ROOT_SUBVOL_INUM);\n \tret = PTR_ERR_OR_ZERO(vinode);\ndiff --git a/fs/bcachefs/fsck.c b/fs/bcachefs/fsck.c\nindex 9920f1affc5b..dbf161e4311a 100644\n--- a/fs/bcachefs/fsck.c\n+++ b/fs/bcachefs/fsck.c\n@@ -2302,9 +2302,7 @@ static int check_dirent(struct btree_trans *trans, struct btree_iter *iter,\n \t\t*hash_info = bch2_hash_info_init(c, &i->inode);\n \tdir->first_this_inode = false;\n \n-#ifdef CONFIG_UNICODE\n \thash_info->cf_encoding = bch2_inode_casefold(c, &i->inode) ? c->cf_encoding : NULL;\n-#endif\n \n \tret = bch2_str_hash_check_key(trans, s, &bch2_dirent_hash_desc, hash_info,\n \t\t\t\t      iter, k, need_second_pass);\n@@ -2819,7 +2817,7 @@ static int check_path_loop(struct btree_trans *trans, struct bkey_s_c inode_k)\n \t\t\t\tret = remove_backpointer(trans, &inode);\n \t\t\t\tbch_err_msg(c, ret, \"removing dirent\");\n \t\t\t\tif (ret)\n-\t\t\t\t\tbreak;\n+\t\t\t\t\tgoto out;\n \n \t\t\t\tret = reattach_inode(trans, &inode);\n \t\t\t\tbch_err_msg(c, ret, \"reattaching inode %llu\", inode.bi_inum);\ndiff --git a/fs/bcachefs/inode.c b/fs/bcachefs/inode.c\nindex 53e5dc1f6ac1..ef4cc7395b86 100644\n--- a/fs/bcachefs/inode.c\n+++ b/fs/bcachefs/inode.c\n@@ -1265,7 +1265,14 @@ int bch2_inode_set_casefold(struct btree_trans *trans, subvol_inum inum,\n {\n \tstruct bch_fs *c = trans->c;\n \n-#ifdef CONFIG_UNICODE\n+#ifndef CONFIG_UNICODE\n+\tbch_err(c, \"Cannot use casefolding on a kernel without CONFIG_UNICODE\");\n+\treturn -EOPNOTSUPP;\n+#endif\n+\n+\tif (c->opts.casefold_disabled)\n+\t\treturn -EOPNOTSUPP;\n+\n \tint ret = 0;\n \t/* Not supported on individual files. */\n \tif (!S_ISDIR(bi->bi_mode))\n@@ -1289,10 +1296,6 @@ int bch2_inode_set_casefold(struct btree_trans *trans, subvol_inum inum,\n \tbi->bi_fields_set |= BIT(Inode_opt_casefold);\n \n \treturn bch2_maybe_propagate_has_case_insensitive(trans, inum, bi);\n-#else\n-\tbch_err(c, \"Cannot use casefolding on a kernel without CONFIG_UNICODE\");\n-\treturn -EOPNOTSUPP;\n-#endif\n }\n \n static noinline int __bch2_inode_rm_snapshot(struct btree_trans *trans, u64 inum, u32 snapshot)\ndiff --git a/fs/bcachefs/opts.h b/fs/bcachefs/opts.h\nindex b0a76bd6d6f5..63f8e254495c 100644\n--- a/fs/bcachefs/opts.h\n+++ b/fs/bcachefs/opts.h\n@@ -234,6 +234,11 @@ enum fsck_err_opts {\n \t  OPT_BOOL(),\t\t\t\t\t\t\t\\\n \t  BCH_SB_CASEFOLD,\t\tfalse,\t\t\t\t\\\n \t  NULL,\t\t\"Dirent lookups are casefolded\")\t\t\\\n+\tx(casefold_disabled,\t\t\tu8,\t\t\t\\\n+\t  OPT_FS|OPT_MOUNT,\t\t\t\t\t\t\\\n+\t  OPT_BOOL(),\t\t\t\t\t\t\t\\\n+\t  BCH2_NO_SB_OPT,\t\tfalse,\t\t\t\t\\\n+\t  NULL,\t\t\"Disable casefolding filesystem wide\")\t\t\\\n \tx(inodes_32bit,\t\t\tu8,\t\t\t\t\\\n \t  OPT_FS|OPT_INODE|OPT_FORMAT|OPT_MOUNT|OPT_RUNTIME,\t\t\\\n \t  OPT_BOOL(),\t\t\t\t\t\t\t\\\ndiff --git a/fs/bcachefs/sb-errors_format.h b/fs/bcachefs/sb-errors_format.h\nindex 0641fb634bd4..d154b7651d28 100644\n--- a/fs/bcachefs/sb-errors_format.h\n+++ b/fs/bcachefs/sb-errors_format.h\n@@ -314,7 +314,7 @@ enum bch_fsck_flags {\n \tx(accounting_mismatch,\t\t\t\t\t272,\tFSCK_AUTOFIX)\t\\\n \tx(accounting_replicas_not_marked,\t\t\t273,\t0)\t\t\\\n \tx(accounting_to_invalid_device,\t\t\t\t289,\t0)\t\t\\\n-\tx(invalid_btree_id,\t\t\t\t\t274,\t0)\t\t\\\n+\tx(invalid_btree_id,\t\t\t\t\t274,\tFSCK_AUTOFIX)\t\t\\\n \tx(alloc_key_io_time_bad,\t\t\t\t275,\t0)\t\t\\\n \tx(alloc_key_fragmentation_lru_wrong,\t\t\t276,\tFSCK_AUTOFIX)\t\\\n \tx(accounting_key_junk_at_end,\t\t\t\t277,\tFSCK_AUTOFIX)\t\\\ndiff --git a/fs/bcachefs/str_hash.c b/fs/bcachefs/str_hash.c\nindex 71b735a85026..3e9f59226bdf 100644\n--- a/fs/bcachefs/str_hash.c\n+++ b/fs/bcachefs/str_hash.c\n@@ -38,6 +38,7 @@ static int bch2_fsck_rename_dirent(struct btree_trans *trans,\n \t\t\t\t   struct bkey_s_c_dirent old,\n \t\t\t\t   bool *updated_before_k_pos)\n {\n+\tstruct bch_fs *c = trans->c;\n \tstruct qstr old_name = bch2_dirent_get_name(old);\n \tstruct bkey_i_dirent *new = bch2_trans_kmalloc(trans, BKEY_U64s_MAX * sizeof(u64));\n \tint ret = PTR_ERR_OR_ZERO(new);\n@@ -60,7 +61,7 @@ static int bch2_fsck_rename_dirent(struct btree_trans *trans,\n \t\t\t\t\tsprintf(renamed_buf, \"%.*s.fsck_renamed-%u\",\n \t\t\t\t\t\told_name.len, old_name.name, i));\n \n-\t\tret = bch2_dirent_init_name(new, hash_info, &renamed_name, NULL);\n+\t\tret = bch2_dirent_init_name(c, new, hash_info, &renamed_name, NULL);\n \t\tif (ret)\n \t\t\treturn ret;\n \n@@ -79,7 +80,7 @@ static int bch2_fsck_rename_dirent(struct btree_trans *trans,\n \t}\n \n \tret = ret ?: bch2_fsck_update_backpointers(trans, s, desc, hash_info, &new->k_i);\n-\tbch_err_fn(trans->c, ret);\n+\tbch_err_fn(c, ret);\n \treturn ret;\n }\n \ndiff --git a/fs/bcachefs/str_hash.h b/fs/bcachefs/str_hash.h\nindex 79d51aef70aa..8979ac2d7a3b 100644\n--- a/fs/bcachefs/str_hash.h\n+++ b/fs/bcachefs/str_hash.h\n@@ -48,9 +48,7 @@ bch2_hash_info_init(struct bch_fs *c, const struct bch_inode_unpacked *bi)\n \tstruct bch_hash_info info = {\n \t\t.inum_snapshot\t= bi->bi_snapshot,\n \t\t.type\t\t= INODE_STR_HASH(bi),\n-#ifdef CONFIG_UNICODE\n \t\t.cf_encoding\t= bch2_inode_casefold(c, bi) ? c->cf_encoding : NULL,\n-#endif\n \t\t.siphash_key\t= { .k0 = bi->bi_hash_seed }\n \t};\n \ndiff --git a/fs/bcachefs/super.c b/fs/bcachefs/super.c\nindex 69c097ff54e7..c46b1053a02c 100644\n--- a/fs/bcachefs/super.c\n+++ b/fs/bcachefs/super.c\n@@ -1025,15 +1025,17 @@ static struct bch_fs *bch2_fs_alloc(struct bch_sb *sb, struct bch_opts *opts,\n \t}\n \n #ifdef CONFIG_UNICODE\n-\t/* Default encoding until we can potentially have more as an option. */\n-\tc->cf_encoding = utf8_load(BCH_FS_DEFAULT_UTF8_ENCODING);\n-\tif (IS_ERR(c->cf_encoding)) {\n-\t\tprintk(KERN_ERR \"Cannot load UTF-8 encoding for filesystem. Version: %u.%u.%u\",\n-\t\t\tunicode_major(BCH_FS_DEFAULT_UTF8_ENCODING),\n-\t\t\tunicode_minor(BCH_FS_DEFAULT_UTF8_ENCODING),\n-\t\t\tunicode_rev(BCH_FS_DEFAULT_UTF8_ENCODING));\n-\t\tret = -EINVAL;\n-\t\tgoto err;\n+\tif (bch2_fs_casefold_enabled(c)) {\n+\t\t/* Default encoding until we can potentially have more as an option. */\n+\t\tc->cf_encoding = utf8_load(BCH_FS_DEFAULT_UTF8_ENCODING);\n+\t\tif (IS_ERR(c->cf_encoding)) {\n+\t\t\tprintk(KERN_ERR \"Cannot load UTF-8 encoding for filesystem. Version: %u.%u.%u\",\n+\t\t\t       unicode_major(BCH_FS_DEFAULT_UTF8_ENCODING),\n+\t\t\t       unicode_minor(BCH_FS_DEFAULT_UTF8_ENCODING),\n+\t\t\t       unicode_rev(BCH_FS_DEFAULT_UTF8_ENCODING));\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n \t}\n #else\n \tif (c->sb.features & BIT_ULL(BCH_FEATURE_casefolding)) {\n@@ -1160,12 +1162,11 @@ int bch2_fs_start(struct bch_fs *c)\n \n \tprint_mount_opts(c);\n \n-#ifdef CONFIG_UNICODE\n-\tbch_info(c, \"Using encoding defined by superblock: utf8-%u.%u.%u\",\n-\t\t unicode_major(BCH_FS_DEFAULT_UTF8_ENCODING),\n-\t\t unicode_minor(BCH_FS_DEFAULT_UTF8_ENCODING),\n-\t\t unicode_rev(BCH_FS_DEFAULT_UTF8_ENCODING));\n-#endif\n+\tif (c->cf_encoding)\n+\t\tbch_info(c, \"Using encoding defined by superblock: utf8-%u.%u.%u\",\n+\t\t\t unicode_major(BCH_FS_DEFAULT_UTF8_ENCODING),\n+\t\t\t unicode_minor(BCH_FS_DEFAULT_UTF8_ENCODING),\n+\t\t\t unicode_rev(BCH_FS_DEFAULT_UTF8_ENCODING));\n \n \tif (!bch2_fs_may_start(c))\n \t\treturn bch_err_throw(c, insufficient_devices_to_start);",
    "stats": {
      "insertions": 93,
      "deletions": 54,
      "files": 13
    }
  },
  {
    "sha": "2eb7f03acf4ac5db937974e99e75dac4c2c5a83d",
    "message": "Merge tag 'vfs-6.16-rc5.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs\n\nPull vfs fixes from Christian Brauner:\n\n - Fix a regression caused by the anonymous inode rework. Making them\n   regular files causes various places in the kernel to tip over\n   starting with io_uring.\n\n   Revert to the former status quo and port our assertion to be based on\n   checking the inode so we don't lose the valuable VFS_*_ON_*()\n   assertions that have already helped discover weird behavior our\n   outright bugs.\n\n - Fix the the upper bound calculation in fuse_fill_write_pages()\n\n - Fix priority inversion issues in the eventpoll code\n\n - Make secretmen use anon_inode_make_secure_inode() to avoid bypassing\n   the LSM layer\n\n - Fix a netfs hang due to missing case in final DIO read result\n   collection\n\n - Fix a double put of the netfs_io_request struct\n\n - Provide some helpers to abstract out NETFS_RREQ_IN_PROGRESS flag\n   wrangling\n\n - Fix infinite looping in netfs_wait_for_pause/request()\n\n - Fix a netfs ref leak on an extra subrequest inserted into a request's\n   list of subreqs\n\n - Fix various cifs RPC callbacks to set NETFS_SREQ_NEED_RETRY if a\n   subrequest fails retriably\n\n - Fix a cifs warning in the workqueue code when reconnecting a channel\n\n - Fix the updating of i_size in netfs to avoid a race between testing\n   if we should have extended the file with a DIO write and changing\n   i_size\n\n - Merge the places in netfs that update i_size on write\n\n - Fix coredump socket selftests\n\n* tag 'vfs-6.16-rc5.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  anon_inode: rework assertions\n  netfs: Update tracepoints in a number of ways\n  netfs: Renumber the NETFS_RREQ_* flags to make traces easier to read\n  netfs: Merge i_size update functions\n  netfs: Fix i_size updating\n  smb: client: set missing retry flag in cifs_writev_callback()\n  smb: client: set missing retry flag in cifs_readv_callback()\n  smb: client: set missing retry flag in smb2_writev_callback()\n  netfs: Fix ref leak on inserted extra subreq in write retry\n  netfs: Fix looping in wait functions\n  netfs: Provide helpers to perform NETFS_RREQ_IN_PROGRESS flag wangling\n  netfs: Fix double put of request\n  netfs: Fix hang due to missing case in final DIO read result collection\n  eventpoll: Fix priority inversion problem\n  fuse: fix fuse_fill_write_pages() upper bound calculation\n  fs: export anon_inode_make_secure_inode() and fix secretmem LSM bypass\n  selftests/coredump: Fix \"socket_detect_userspace_client\" test failure",
    "author": "Linus Torvalds",
    "date": "2025-07-04T09:06:49-07:00",
    "files_changed": [
      "fs/anon_inodes.c",
      "fs/eventpoll.c",
      "fs/exec.c",
      "fs/fuse/file.c",
      "fs/libfs.c",
      "fs/namei.c",
      "fs/netfs/buffered_write.c",
      "fs/netfs/direct_write.c",
      "fs/netfs/internal.h",
      "fs/netfs/main.c",
      "fs/netfs/misc.c",
      "fs/netfs/read_collect.c",
      "fs/netfs/write_collect.c",
      "fs/netfs/write_retry.c",
      "fs/smb/client/cifssmb.c",
      "fs/smb/client/smb2pdu.c",
      "include/linux/fs.h",
      "include/linux/netfs.h",
      "include/trace/events/netfs.h",
      "mm/secretmem.c",
      "tools/testing/selftests/coredump/stackdump_test.c"
    ],
    "diff": "diff --git a/fs/anon_inodes.c b/fs/anon_inodes.c\nindex e51e7d88980a..1d847a939f29 100644\n--- a/fs/anon_inodes.c\n+++ b/fs/anon_inodes.c\n@@ -98,14 +98,25 @@ static struct file_system_type anon_inode_fs_type = {\n \t.kill_sb\t= kill_anon_super,\n };\n \n-static struct inode *anon_inode_make_secure_inode(\n-\tconst char *name,\n-\tconst struct inode *context_inode)\n+/**\n+ * anon_inode_make_secure_inode - allocate an anonymous inode with security context\n+ * @sb:\t\t[in]\tSuperblock to allocate from\n+ * @name:\t[in]\tName of the class of the newfile (e.g., \"secretmem\")\n+ * @context_inode:\n+ *\t\t[in]\tOptional parent inode for security inheritance\n+ *\n+ * The function ensures proper security initialization through the LSM hook\n+ * security_inode_init_security_anon().\n+ *\n+ * Return:\tPointer to new inode on success, ERR_PTR on failure.\n+ */\n+struct inode *anon_inode_make_secure_inode(struct super_block *sb, const char *name,\n+\t\t\t\t\t   const struct inode *context_inode)\n {\n \tstruct inode *inode;\n \tint error;\n \n-\tinode = alloc_anon_inode(anon_inode_mnt->mnt_sb);\n+\tinode = alloc_anon_inode(sb);\n \tif (IS_ERR(inode))\n \t\treturn inode;\n \tinode->i_flags &= ~S_PRIVATE;\n@@ -118,6 +129,7 @@ static struct inode *anon_inode_make_secure_inode(\n \t}\n \treturn inode;\n }\n+EXPORT_SYMBOL_GPL_FOR_MODULES(anon_inode_make_secure_inode, \"kvm\");\n \n static struct file *__anon_inode_getfile(const char *name,\n \t\t\t\t\t const struct file_operations *fops,\n@@ -132,7 +144,8 @@ static struct file *__anon_inode_getfile(const char *name,\n \t\treturn ERR_PTR(-ENOENT);\n \n \tif (make_inode) {\n-\t\tinode =\tanon_inode_make_secure_inode(name, context_inode);\n+\t\tinode =\tanon_inode_make_secure_inode(anon_inode_mnt->mnt_sb,\n+\t\t\t\t\t\t     name, context_inode);\n \t\tif (IS_ERR(inode)) {\n \t\t\tfile = ERR_CAST(inode);\n \t\t\tgoto err;\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex d4dbffdedd08..a97a771a459c 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -137,13 +137,7 @@ struct epitem {\n \t};\n \n \t/* List header used to link this structure to the eventpoll ready list */\n-\tstruct list_head rdllink;\n-\n-\t/*\n-\t * Works together \"struct eventpoll\"->ovflist in keeping the\n-\t * single linked chain of items.\n-\t */\n-\tstruct epitem *next;\n+\tstruct llist_node rdllink;\n \n \t/* The file descriptor information this item refers to */\n \tstruct epoll_filefd ffd;\n@@ -191,22 +185,15 @@ struct eventpoll {\n \t/* Wait queue used by file->poll() */\n \twait_queue_head_t poll_wait;\n \n-\t/* List of ready file descriptors */\n-\tstruct list_head rdllist;\n-\n-\t/* Lock which protects rdllist and ovflist */\n-\trwlock_t lock;\n+\t/*\n+\t * List of ready file descriptors. Adding to this list is lockless. Items can be removed\n+\t * only with eventpoll::mtx\n+\t */\n+\tstruct llist_head rdllist;\n \n \t/* RB tree root used to store monitored fd structs */\n \tstruct rb_root_cached rbr;\n \n-\t/*\n-\t * This is a single linked list that chains all the \"struct epitem\" that\n-\t * happened while transferring ready events to userspace w/out\n-\t * holding ->lock.\n-\t */\n-\tstruct epitem *ovflist;\n-\n \t/* wakeup_source used when ep_send_events or __ep_eventpoll_poll is running */\n \tstruct wakeup_source *ws;\n \n@@ -361,10 +348,14 @@ static inline int ep_cmp_ffd(struct epoll_filefd *p1,\n \t        (p1->file < p2->file ? -1 : p1->fd - p2->fd));\n }\n \n-/* Tells us if the item is currently linked */\n-static inline int ep_is_linked(struct epitem *epi)\n+/*\n+ * Add the item to its container eventpoll's rdllist; do nothing if the item is already on rdllist.\n+ */\n+static void epitem_ready(struct epitem *epi)\n {\n-\treturn !list_empty(&epi->rdllink);\n+\tif (&epi->rdllink == cmpxchg(&epi->rdllink.next, &epi->rdllink, NULL))\n+\t\tllist_add(&epi->rdllink, &epi->ep->rdllist);\n+\n }\n \n static inline struct eppoll_entry *ep_pwq_from_wait(wait_queue_entry_t *p)\n@@ -383,13 +374,26 @@ static inline struct epitem *ep_item_from_wait(wait_queue_entry_t *p)\n  *\n  * @ep: Pointer to the eventpoll context.\n  *\n- * Return: a value different than %zero if ready events are available,\n- *          or %zero otherwise.\n+ * Return: true if ready events might be available, false otherwise.\n  */\n-static inline int ep_events_available(struct eventpoll *ep)\n+static inline bool ep_events_available(struct eventpoll *ep)\n {\n-\treturn !list_empty_careful(&ep->rdllist) ||\n-\t\tREAD_ONCE(ep->ovflist) != EP_UNACTIVE_PTR;\n+\tbool available;\n+\tint locked;\n+\n+\tlocked = mutex_trylock(&ep->mtx);\n+\tif (!locked) {\n+\t\t/*\n+\t\t * The lock held and someone might have removed all items while inspecting it. The\n+\t\t * llist_empty() check in this case is futile. Assume that something is enqueued and\n+\t\t * let ep_try_send_events() figure it out.\n+\t\t */\n+\t\treturn true;\n+\t}\n+\n+\tavailable = !llist_empty(&ep->rdllist);\n+\tmutex_unlock(&ep->mtx);\n+\treturn available;\n }\n \n #ifdef CONFIG_NET_RX_BUSY_POLL\n@@ -724,77 +728,6 @@ static inline void ep_pm_stay_awake_rcu(struct epitem *epi)\n \trcu_read_unlock();\n }\n \n-\n-/*\n- * ep->mutex needs to be held because we could be hit by\n- * eventpoll_release_file() and epoll_ctl().\n- */\n-static void ep_start_scan(struct eventpoll *ep, struct list_head *txlist)\n-{\n-\t/*\n-\t * Steal the ready list, and re-init the original one to the\n-\t * empty list. Also, set ep->ovflist to NULL so that events\n-\t * happening while looping w/out locks, are not lost. We cannot\n-\t * have the poll callback to queue directly on ep->rdllist,\n-\t * because we want the \"sproc\" callback to be able to do it\n-\t * in a lockless way.\n-\t */\n-\tlockdep_assert_irqs_enabled();\n-\twrite_lock_irq(&ep->lock);\n-\tlist_splice_init(&ep->rdllist, txlist);\n-\tWRITE_ONCE(ep->ovflist, NULL);\n-\twrite_unlock_irq(&ep->lock);\n-}\n-\n-static void ep_done_scan(struct eventpoll *ep,\n-\t\t\t struct list_head *txlist)\n-{\n-\tstruct epitem *epi, *nepi;\n-\n-\twrite_lock_irq(&ep->lock);\n-\t/*\n-\t * During the time we spent inside the \"sproc\" callback, some\n-\t * other events might have been queued by the poll callback.\n-\t * We re-insert them inside the main ready-list here.\n-\t */\n-\tfor (nepi = READ_ONCE(ep->ovflist); (epi = nepi) != NULL;\n-\t     nepi = epi->next, epi->next = EP_UNACTIVE_PTR) {\n-\t\t/*\n-\t\t * We need to check if the item is already in the list.\n-\t\t * During the \"sproc\" callback execution time, items are\n-\t\t * queued into ->ovflist but the \"txlist\" might already\n-\t\t * contain them, and the list_splice() below takes care of them.\n-\t\t */\n-\t\tif (!ep_is_linked(epi)) {\n-\t\t\t/*\n-\t\t\t * ->ovflist is LIFO, so we have to reverse it in order\n-\t\t\t * to keep in FIFO.\n-\t\t\t */\n-\t\t\tlist_add(&epi->rdllink, &ep->rdllist);\n-\t\t\tep_pm_stay_awake(epi);\n-\t\t}\n-\t}\n-\t/*\n-\t * We need to set back ep->ovflist to EP_UNACTIVE_PTR, so that after\n-\t * releasing the lock, events will be queued in the normal way inside\n-\t * ep->rdllist.\n-\t */\n-\tWRITE_ONCE(ep->ovflist, EP_UNACTIVE_PTR);\n-\n-\t/*\n-\t * Quickly re-inject items left on \"txlist\".\n-\t */\n-\tlist_splice(txlist, &ep->rdllist);\n-\t__pm_relax(ep->ws);\n-\n-\tif (!list_empty(&ep->rdllist)) {\n-\t\tif (waitqueue_active(&ep->wq))\n-\t\t\twake_up(&ep->wq);\n-\t}\n-\n-\twrite_unlock_irq(&ep->lock);\n-}\n-\n static void ep_get(struct eventpoll *ep)\n {\n \trefcount_inc(&ep->refcount);\n@@ -832,10 +765,12 @@ static void ep_free(struct eventpoll *ep)\n static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n {\n \tstruct file *file = epi->ffd.file;\n+\tstruct llist_node *put_back_last;\n \tstruct epitems_head *to_free;\n \tstruct hlist_head *head;\n+\tLLIST_HEAD(put_back);\n \n-\tlockdep_assert_irqs_enabled();\n+\tlockdep_assert_held(&ep->mtx);\n \n \t/*\n \t * Removes poll wait queue hooks.\n@@ -867,10 +802,20 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n \n \trb_erase_cached(&epi->rbn, &ep->rbr);\n \n-\twrite_lock_irq(&ep->lock);\n-\tif (ep_is_linked(epi))\n-\t\tlist_del_init(&epi->rdllink);\n-\twrite_unlock_irq(&ep->lock);\n+\tif (llist_on_list(&epi->rdllink)) {\n+\t\tput_back_last = NULL;\n+\t\twhile (true) {\n+\t\t\tstruct llist_node *n = llist_del_first(&ep->rdllist);\n+\n+\t\t\tif (&epi->rdllink == n || WARN_ON(!n))\n+\t\t\t\tbreak;\n+\t\t\tif (!put_back_last)\n+\t\t\t\tput_back_last = n;\n+\t\t\t__llist_add(n, &put_back);\n+\t\t}\n+\t\tif (put_back_last)\n+\t\t\tllist_add_batch(put_back.first, put_back_last, &ep->rdllist);\n+\t}\n \n \twakeup_source_unregister(ep_wakeup_source(epi));\n \t/*\n@@ -974,8 +919,9 @@ static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt, int depth\n static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int depth)\n {\n \tstruct eventpoll *ep = file->private_data;\n-\tLIST_HEAD(txlist);\n-\tstruct epitem *epi, *tmp;\n+\tstruct wakeup_source *ws;\n+\tstruct llist_node *n;\n+\tstruct epitem *epi;\n \tpoll_table pt;\n \t__poll_t res = 0;\n \n@@ -989,22 +935,39 @@ static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int dep\n \t * the ready list.\n \t */\n \tmutex_lock_nested(&ep->mtx, depth);\n-\tep_start_scan(ep, &txlist);\n-\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n+\twhile (true) {\n+\t\tn = llist_del_first_init(&ep->rdllist);\n+\t\tif (!n)\n+\t\t\tbreak;\n+\n+\t\tepi = llist_entry(n, struct epitem, rdllink);\n+\n \t\tif (ep_item_poll(epi, &pt, depth + 1)) {\n \t\t\tres = EPOLLIN | EPOLLRDNORM;\n+\t\t\tepitem_ready(epi);\n \t\t\tbreak;\n \t\t} else {\n \t\t\t/*\n-\t\t\t * Item has been dropped into the ready list by the poll\n-\t\t\t * callback, but it's not actually ready, as far as\n-\t\t\t * caller requested events goes. We can remove it here.\n+\t\t\t * We need to activate ep before deactivating epi, to prevent autosuspend\n+\t\t\t * just in case epi becomes active after ep_item_poll() above.\n+\t\t\t *\n+\t\t\t * This is similar to ep_send_events().\n \t\t\t */\n+\t\t\tws = ep_wakeup_source(epi);\n+\t\t\tif (ws) {\n+\t\t\t\tif (ws->active)\n+\t\t\t\t\t__pm_stay_awake(ep->ws);\n+\t\t\t\t__pm_relax(ws);\n+\t\t\t}\n \t\t\t__pm_relax(ep_wakeup_source(epi));\n-\t\t\tlist_del_init(&epi->rdllink);\n+\n+\t\t\t/* Just in case epi becomes active right before __pm_relax() */\n+\t\t\tif (unlikely(ep_item_poll(epi, &pt, depth + 1)))\n+\t\t\t\tep_pm_stay_awake(epi);\n+\n+\t\t\t__pm_relax(ep->ws);\n \t\t}\n \t}\n-\tep_done_scan(ep, &txlist);\n \tmutex_unlock(&ep->mtx);\n \treturn res;\n }\n@@ -1153,12 +1116,10 @@ static int ep_alloc(struct eventpoll **pep)\n \t\treturn -ENOMEM;\n \n \tmutex_init(&ep->mtx);\n-\trwlock_init(&ep->lock);\n \tinit_waitqueue_head(&ep->wq);\n \tinit_waitqueue_head(&ep->poll_wait);\n-\tINIT_LIST_HEAD(&ep->rdllist);\n+\tinit_llist_head(&ep->rdllist);\n \tep->rbr = RB_ROOT_CACHED;\n-\tep->ovflist = EP_UNACTIVE_PTR;\n \tep->user = get_current_user();\n \trefcount_set(&ep->refcount, 1);\n \n@@ -1240,94 +1201,11 @@ struct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,\n }\n #endif /* CONFIG_KCMP */\n \n-/*\n- * Adds a new entry to the tail of the list in a lockless way, i.e.\n- * multiple CPUs are allowed to call this function concurrently.\n- *\n- * Beware: it is necessary to prevent any other modifications of the\n- *         existing list until all changes are completed, in other words\n- *         concurrent list_add_tail_lockless() calls should be protected\n- *         with a read lock, where write lock acts as a barrier which\n- *         makes sure all list_add_tail_lockless() calls are fully\n- *         completed.\n- *\n- *        Also an element can be locklessly added to the list only in one\n- *        direction i.e. either to the tail or to the head, otherwise\n- *        concurrent access will corrupt the list.\n- *\n- * Return: %false if element has been already added to the list, %true\n- * otherwise.\n- */\n-static inline bool list_add_tail_lockless(struct list_head *new,\n-\t\t\t\t\t  struct list_head *head)\n-{\n-\tstruct list_head *prev;\n-\n-\t/*\n-\t * This is simple 'new->next = head' operation, but cmpxchg()\n-\t * is used in order to detect that same element has been just\n-\t * added to the list from another CPU: the winner observes\n-\t * new->next == new.\n-\t */\n-\tif (!try_cmpxchg(&new->next, &new, head))\n-\t\treturn false;\n-\n-\t/*\n-\t * Initially ->next of a new element must be updated with the head\n-\t * (we are inserting to the tail) and only then pointers are atomically\n-\t * exchanged.  XCHG guarantees memory ordering, thus ->next should be\n-\t * updated before pointers are actually swapped and pointers are\n-\t * swapped before prev->next is updated.\n-\t */\n-\n-\tprev = xchg(&head->prev, new);\n-\n-\t/*\n-\t * It is safe to modify prev->next and new->prev, because a new element\n-\t * is added only to the tail and new->next is updated before XCHG.\n-\t */\n-\n-\tprev->next = new;\n-\tnew->prev = prev;\n-\n-\treturn true;\n-}\n-\n-/*\n- * Chains a new epi entry to the tail of the ep->ovflist in a lockless way,\n- * i.e. multiple CPUs are allowed to call this function concurrently.\n- *\n- * Return: %false if epi element has been already chained, %true otherwise.\n- */\n-static inline bool chain_epi_lockless(struct epitem *epi)\n-{\n-\tstruct eventpoll *ep = epi->ep;\n-\n-\t/* Fast preliminary check */\n-\tif (epi->next != EP_UNACTIVE_PTR)\n-\t\treturn false;\n-\n-\t/* Check that the same epi has not been just chained from another CPU */\n-\tif (cmpxchg(&epi->next, EP_UNACTIVE_PTR, NULL) != EP_UNACTIVE_PTR)\n-\t\treturn false;\n-\n-\t/* Atomically exchange tail */\n-\tepi->next = xchg(&ep->ovflist, epi);\n-\n-\treturn true;\n-}\n-\n /*\n  * This is the callback that is passed to the wait queue wakeup\n  * mechanism. It is called by the stored file descriptors when they\n  * have events to report.\n  *\n- * This callback takes a read lock in order not to contend with concurrent\n- * events from another file descriptor, thus all modifications to ->rdllist\n- * or ->ovflist are lockless.  Read lock is paired with the write lock from\n- * ep_start/done_scan(), which stops all list modifications and guarantees\n- * that lists state is seen correctly.\n- *\n  * Another thing worth to mention is that ep_poll_callback() can be called\n  * concurrently for the same @epi from different CPUs if poll table was inited\n  * with several wait queues entries.  Plural wakeup from different CPUs of a\n@@ -1337,15 +1215,11 @@ static inline bool chain_epi_lockless(struct epitem *epi)\n  */\n static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n {\n-\tint pwake = 0;\n \tstruct epitem *epi = ep_item_from_wait(wait);\n \tstruct eventpoll *ep = epi->ep;\n \t__poll_t pollflags = key_to_poll(key);\n-\tunsigned long flags;\n \tint ewake = 0;\n \n-\tread_lock_irqsave(&ep->lock, flags);\n-\n \tep_set_busy_poll_napi_id(epi);\n \n \t/*\n@@ -1355,7 +1229,7 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * until the next EPOLL_CTL_MOD will be issued.\n \t */\n \tif (!(epi->event.events & ~EP_PRIVATE_BITS))\n-\t\tgoto out_unlock;\n+\t\tgoto out;\n \n \t/*\n \t * Check the events coming with the callback. At this stage, not\n@@ -1364,22 +1238,10 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * test for \"key\" != NULL before the event match test.\n \t */\n \tif (pollflags && !(pollflags & epi->event.events))\n-\t\tgoto out_unlock;\n+\t\tgoto out;\n \n-\t/*\n-\t * If we are transferring events to userspace, we can hold no locks\n-\t * (because we're accessing user memory, and because of linux f_op->poll()\n-\t * semantics). All the events that happen during that period of time are\n-\t * chained in ep->ovflist and requeued later on.\n-\t */\n-\tif (READ_ONCE(ep->ovflist) != EP_UNACTIVE_PTR) {\n-\t\tif (chain_epi_lockless(epi))\n-\t\t\tep_pm_stay_awake_rcu(epi);\n-\t} else if (!ep_is_linked(epi)) {\n-\t\t/* In the usual case, add event to ready list. */\n-\t\tif (list_add_tail_lockless(&epi->rdllink, &ep->rdllist))\n-\t\t\tep_pm_stay_awake_rcu(epi);\n-\t}\n+\tep_pm_stay_awake_rcu(epi);\n+\tepitem_ready(epi);\n \n \t/*\n \t * Wake up ( if active ) both the eventpoll wait list and the ->poll()\n@@ -1408,15 +1270,9 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t\t\twake_up(&ep->wq);\n \t}\n \tif (waitqueue_active(&ep->poll_wait))\n-\t\tpwake++;\n-\n-out_unlock:\n-\tread_unlock_irqrestore(&ep->lock, flags);\n-\n-\t/* We have to call this outside the lock */\n-\tif (pwake)\n \t\tep_poll_safewake(ep, epi, pollflags & EPOLL_URING_WAKE);\n \n+out:\n \tif (!(epi->event.events & EPOLLEXCLUSIVE))\n \t\tewake = 1;\n \n@@ -1661,8 +1517,6 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \tif (is_file_epoll(tfile))\n \t\ttep = tfile->private_data;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tif (unlikely(percpu_counter_compare(&ep->user->epoll_watches,\n \t\t\t\t\t    max_user_watches) >= 0))\n \t\treturn -ENOSPC;\n@@ -1674,11 +1528,10 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t}\n \n \t/* Item initialization follow here ... */\n-\tINIT_LIST_HEAD(&epi->rdllink);\n+\tinit_llist_node(&epi->rdllink);\n \tepi->ep = ep;\n \tep_set_ffd(&epi->ffd, tfile, fd);\n \tepi->event = *event;\n-\tepi->next = EP_UNACTIVE_PTR;\n \n \tif (tep)\n \t\tmutex_lock_nested(&tep->mtx, 1);\n@@ -1745,16 +1598,13 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\treturn -ENOMEM;\n \t}\n \n-\t/* We have to drop the new item inside our item list to keep track of it */\n-\twrite_lock_irq(&ep->lock);\n-\n \t/* record NAPI ID of new item if present */\n \tep_set_busy_poll_napi_id(epi);\n \n \t/* If the file is already \"ready\" we drop it inside the ready list */\n-\tif (revents && !ep_is_linked(epi)) {\n-\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n+\tif (revents) {\n \t\tep_pm_stay_awake(epi);\n+\t\tepitem_ready(epi);\n \n \t\t/* Notify waiting tasks that events are available */\n \t\tif (waitqueue_active(&ep->wq))\n@@ -1763,8 +1613,6 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\t\tpwake++;\n \t}\n \n-\twrite_unlock_irq(&ep->lock);\n-\n \t/* We have to call this outside the lock */\n \tif (pwake)\n \t\tep_poll_safewake(ep, NULL, 0);\n@@ -1779,11 +1627,8 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t\t     const struct epoll_event *event)\n {\n-\tint pwake = 0;\n \tpoll_table pt;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tinit_poll_funcptr(&pt, NULL);\n \n \t/*\n@@ -1827,24 +1672,16 @@ static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t * list, push it inside.\n \t */\n \tif (ep_item_poll(epi, &pt, 1)) {\n-\t\twrite_lock_irq(&ep->lock);\n-\t\tif (!ep_is_linked(epi)) {\n-\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n-\t\t\tep_pm_stay_awake(epi);\n+\t\tep_pm_stay_awake(epi);\n+\t\tepitem_ready(epi);\n \n-\t\t\t/* Notify waiting tasks that events are available */\n-\t\t\tif (waitqueue_active(&ep->wq))\n-\t\t\t\twake_up(&ep->wq);\n-\t\t\tif (waitqueue_active(&ep->poll_wait))\n-\t\t\t\tpwake++;\n-\t\t}\n-\t\twrite_unlock_irq(&ep->lock);\n+\t\t/* Notify waiting tasks that events are available */\n+\t\tif (waitqueue_active(&ep->wq))\n+\t\t\twake_up(&ep->wq);\n+\t\tif (waitqueue_active(&ep->poll_wait))\n+\t\t\tep_poll_safewake(ep, NULL, 0);\n \t}\n \n-\t/* We have to call this outside the lock */\n-\tif (pwake)\n-\t\tep_poll_safewake(ep, NULL, 0);\n-\n \treturn 0;\n }\n \n@@ -1852,7 +1689,7 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t  struct epoll_event __user *events, int maxevents)\n {\n \tstruct epitem *epi, *tmp;\n-\tLIST_HEAD(txlist);\n+\tLLIST_HEAD(txlist);\n \tpoll_table pt;\n \tint res = 0;\n \n@@ -1867,19 +1704,18 @@ static int ep_send_events(struct eventpoll *ep,\n \tinit_poll_funcptr(&pt, NULL);\n \n \tmutex_lock(&ep->mtx);\n-\tep_start_scan(ep, &txlist);\n \n-\t/*\n-\t * We can loop without lock because we are passed a task private list.\n-\t * Items cannot vanish during the loop we are holding ep->mtx.\n-\t */\n-\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n+\twhile (res < maxevents) {\n \t\tstruct wakeup_source *ws;\n+\t\tstruct llist_node *n;\n \t\t__poll_t revents;\n \n-\t\tif (res >= maxevents)\n+\t\tn = llist_del_first(&ep->rdllist);\n+\t\tif (!n)\n \t\t\tbreak;\n \n+\t\tepi = llist_entry(n, struct epitem, rdllink);\n+\n \t\t/*\n \t\t * Activate ep->ws before deactivating epi->ws to prevent\n \t\t * triggering auto-suspend here (in case we reactive epi->ws\n@@ -1896,21 +1732,30 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t__pm_relax(ws);\n \t\t}\n \n-\t\tlist_del_init(&epi->rdllink);\n-\n \t\t/*\n \t\t * If the event mask intersect the caller-requested one,\n \t\t * deliver the event to userspace. Again, we are holding ep->mtx,\n \t\t * so no operations coming from userspace can change the item.\n \t\t */\n \t\trevents = ep_item_poll(epi, &pt, 1);\n-\t\tif (!revents)\n+\t\tif (!revents) {\n+\t\t\tinit_llist_node(n);\n+\n+\t\t\t/*\n+\t\t\t * Just in case epi becomes ready after ep_item_poll() above, but before\n+\t\t\t * init_llist_node(). Make sure to add it to the ready list, otherwise an\n+\t\t\t * event may be lost.\n+\t\t\t */\n+\t\t\tif (unlikely(ep_item_poll(epi, &pt, 1))) {\n+\t\t\t\tep_pm_stay_awake(epi);\n+\t\t\t\tepitem_ready(epi);\n+\t\t\t}\n \t\t\tcontinue;\n+\t\t}\n \n \t\tevents = epoll_put_uevent(revents, epi->event.data, events);\n \t\tif (!events) {\n-\t\t\tlist_add(&epi->rdllink, &txlist);\n-\t\t\tep_pm_stay_awake(epi);\n+\t\t\tllist_add(&epi->rdllink, &ep->rdllist);\n \t\t\tif (!res)\n \t\t\t\tres = -EFAULT;\n \t\t\tbreak;\n@@ -1918,25 +1763,31 @@ static int ep_send_events(struct eventpoll *ep,\n \t\tres++;\n \t\tif (epi->event.events & EPOLLONESHOT)\n \t\t\tepi->event.events &= EP_PRIVATE_BITS;\n-\t\telse if (!(epi->event.events & EPOLLET)) {\n+\t\t__llist_add(n, &txlist);\n+\t}\n+\n+\tllist_for_each_entry_safe(epi, tmp, txlist.first, rdllink) {\n+\t\tinit_llist_node(&epi->rdllink);\n+\n+\t\tif (!(epi->event.events & EPOLLET)) {\n \t\t\t/*\n-\t\t\t * If this file has been added with Level\n-\t\t\t * Trigger mode, we need to insert back inside\n-\t\t\t * the ready list, so that the next call to\n-\t\t\t * epoll_wait() will check again the events\n-\t\t\t * availability. At this point, no one can insert\n-\t\t\t * into ep->rdllist besides us. The epoll_ctl()\n-\t\t\t * callers are locked out by\n-\t\t\t * ep_send_events() holding \"mtx\" and the\n-\t\t\t * poll callback will queue them in ep->ovflist.\n+\t\t\t * If this file has been added with Level Trigger mode, we need to insert\n+\t\t\t * back inside the ready list, so that the next call to epoll_wait() will\n+\t\t\t * check again the events availability.\n \t\t\t */\n-\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n \t\t\tep_pm_stay_awake(epi);\n+\t\t\tepitem_ready(epi);\n \t\t}\n \t}\n-\tep_done_scan(ep, &txlist);\n+\n+\t__pm_relax(ep->ws);\n \tmutex_unlock(&ep->mtx);\n \n+\tif (!llist_empty(&ep->rdllist)) {\n+\t\tif (waitqueue_active(&ep->wq))\n+\t\t\twake_up(&ep->wq);\n+\t}\n+\n \treturn res;\n }\n \n@@ -2029,8 +1880,6 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \twait_queue_entry_t wait;\n \tktime_t expires, *to = NULL;\n \n-\tlockdep_assert_irqs_enabled();\n-\n \tif (timeout && (timeout->tv_sec | timeout->tv_nsec)) {\n \t\tslack = select_estimate_accuracy(timeout);\n \t\tto = &expires;\n@@ -2090,54 +1939,15 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \t\tinit_wait(&wait);\n \t\twait.func = ep_autoremove_wake_function;\n \n-\t\twrite_lock_irq(&ep->lock);\n-\t\t/*\n-\t\t * Barrierless variant, waitqueue_active() is called under\n-\t\t * the same lock on wakeup ep_poll_callback() side, so it\n-\t\t * is safe to avoid an explicit barrier.\n-\t\t */\n-\t\t__set_current_state(TASK_INTERRUPTIBLE);\n+\t\tprepare_to_wait_exclusive(&ep->wq, &wait, TASK_INTERRUPTIBLE);\n \n-\t\t/*\n-\t\t * Do the final check under the lock. ep_start/done_scan()\n-\t\t * plays with two lists (->rdllist and ->ovflist) and there\n-\t\t * is always a race when both lists are empty for short\n-\t\t * period of time although events are pending, so lock is\n-\t\t * important.\n-\t\t */\n-\t\teavail = ep_events_available(ep);\n-\t\tif (!eavail)\n-\t\t\t__add_wait_queue_exclusive(&ep->wq, &wait);\n-\n-\t\twrite_unlock_irq(&ep->lock);\n-\n-\t\tif (!eavail)\n+\t\tif (!ep_events_available(ep))\n \t\t\ttimed_out = !ep_schedule_timeout(to) ||\n \t\t\t\t!schedule_hrtimeout_range(to, slack,\n \t\t\t\t\t\t\t  HRTIMER_MODE_ABS);\n-\t\t__set_current_state(TASK_RUNNING);\n-\n-\t\t/*\n-\t\t * We were woken up, thus go and try to harvest some events.\n-\t\t * If timed out and still on the wait queue, recheck eavail\n-\t\t * carefully under lock, below.\n-\t\t */\n-\t\teavail = 1;\n \n-\t\tif (!list_empty_careful(&wait.entry)) {\n-\t\t\twrite_lock_irq(&ep->lock);\n-\t\t\t/*\n-\t\t\t * If the thread timed out and is not on the wait queue,\n-\t\t\t * it means that the thread was woken up after its\n-\t\t\t * timeout expired before it could reacquire the lock.\n-\t\t\t * Thus, when wait.entry is empty, it needs to harvest\n-\t\t\t * events.\n-\t\t\t */\n-\t\t\tif (timed_out)\n-\t\t\t\teavail = list_empty(&wait.entry);\n-\t\t\t__remove_wait_queue(&ep->wq, &wait);\n-\t\t\twrite_unlock_irq(&ep->lock);\n-\t\t}\n+\t\tfinish_wait(&ep->wq, &wait);\n+\t\teavail = ep_events_available(ep);\n \t}\n }\n \ndiff --git a/fs/exec.c b/fs/exec.c\nindex 1f5fdd2e096e..ba400aafd640 100644\n--- a/fs/exec.c\n+++ b/fs/exec.c\n@@ -114,6 +114,9 @@ static inline void put_binfmt(struct linux_binfmt * fmt)\n \n bool path_noexec(const struct path *path)\n {\n+\t/* If it's an anonymous inode make sure that we catch any shenanigans. */\n+\tVFS_WARN_ON_ONCE(IS_ANON_FILE(d_inode(path->dentry)) &&\n+\t\t\t !(path->mnt->mnt_sb->s_iflags & SB_I_NOEXEC));\n \treturn (path->mnt->mnt_flags & MNT_NOEXEC) ||\n \t       (path->mnt->mnt_sb->s_iflags & SB_I_NOEXEC);\n }\n@@ -781,13 +784,15 @@ static struct file *do_open_execat(int fd, struct filename *name, int flags)\n \tif (IS_ERR(file))\n \t\treturn file;\n \n+\tif (path_noexec(&file->f_path))\n+\t\treturn ERR_PTR(-EACCES);\n+\n \t/*\n \t * In the past the regular type check was here. It moved to may_open() in\n \t * 633fb6ac3980 (\"exec: move S_ISREG() check earlier\"). Since then it is\n \t * an invariant that all non-regular files error out before we get here.\n \t */\n-\tif (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)) ||\n-\t    path_noexec(&file->f_path))\n+\tif (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)))\n \t\treturn ERR_PTR(-EACCES);\n \n \terr = exe_file_deny_write_access(file);\ndiff --git a/fs/fuse/file.c b/fs/fuse/file.c\nindex f102afc03359..47006d0753f1 100644\n--- a/fs/fuse/file.c\n+++ b/fs/fuse/file.c\n@@ -1147,7 +1147,7 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n static ssize_t fuse_fill_write_pages(struct fuse_io_args *ia,\n \t\t\t\t     struct address_space *mapping,\n \t\t\t\t     struct iov_iter *ii, loff_t pos,\n-\t\t\t\t     unsigned int max_pages)\n+\t\t\t\t     unsigned int max_folios)\n {\n \tstruct fuse_args_pages *ap = &ia->ap;\n \tstruct fuse_conn *fc = get_fuse_conn(mapping->host);\n@@ -1157,12 +1157,11 @@ static ssize_t fuse_fill_write_pages(struct fuse_io_args *ia,\n \tint err = 0;\n \n \tnum = min(iov_iter_count(ii), fc->max_write);\n-\tnum = min(num, max_pages << PAGE_SHIFT);\n \n \tap->args.in_pages = true;\n \tap->descs[0].offset = offset;\n \n-\twhile (num) {\n+\twhile (num && ap->num_folios < max_folios) {\n \t\tsize_t tmp;\n \t\tstruct folio *folio;\n \t\tpgoff_t index = pos >> PAGE_SHIFT;\ndiff --git a/fs/libfs.c b/fs/libfs.c\nindex 9ea0ecc325a8..6f487fc6be34 100644\n--- a/fs/libfs.c\n+++ b/fs/libfs.c\n@@ -1649,12 +1649,10 @@ struct inode *alloc_anon_inode(struct super_block *s)\n \t */\n \tinode->i_state = I_DIRTY;\n \t/*\n-\t * Historically anonymous inodes didn't have a type at all and\n-\t * userspace has come to rely on this. Internally they're just\n-\t * regular files but S_IFREG is masked off when reporting\n-\t * information to userspace.\n+\t * Historically anonymous inodes don't have a type at all and\n+\t * userspace has come to rely on this.\n \t */\n-\tinode->i_mode = S_IFREG | S_IRUSR | S_IWUSR;\n+\tinode->i_mode = S_IRUSR | S_IWUSR;\n \tinode->i_uid = current_fsuid();\n \tinode->i_gid = current_fsgid();\n \tinode->i_flags |= S_PRIVATE | S_ANON_INODE;\ndiff --git a/fs/namei.c b/fs/namei.c\nindex f761cafaeaad..c26a7ee42184 100644\n--- a/fs/namei.c\n+++ b/fs/namei.c\n@@ -3480,7 +3480,7 @@ static int may_open(struct mnt_idmap *idmap, const struct path *path,\n \t\t\treturn -EACCES;\n \t\tbreak;\n \tdefault:\n-\t\tVFS_BUG_ON_INODE(1, inode);\n+\t\tVFS_BUG_ON_INODE(!IS_ANON_FILE(inode), inode);\n \t}\n \n \terror = inode_permission(idmap, inode, MAY_OPEN | acc_mode);\ndiff --git a/fs/netfs/buffered_write.c b/fs/netfs/buffered_write.c\nindex 72a3e6db2524..f27ea5099a68 100644\n--- a/fs/netfs/buffered_write.c\n+++ b/fs/netfs/buffered_write.c\n@@ -53,30 +53,40 @@ static struct folio *netfs_grab_folio_for_write(struct address_space *mapping,\n  * data written into the pagecache until we can find out from the server what\n  * the values actually are.\n  */\n-static void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n-\t\t\t\tloff_t i_size, loff_t pos, size_t copied)\n+void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n+\t\t\t loff_t pos, size_t copied)\n {\n+\tloff_t i_size, end = pos + copied;\n \tblkcnt_t add;\n \tsize_t gap;\n \n+\tif (end <= i_size_read(inode))\n+\t\treturn;\n+\n \tif (ctx->ops->update_i_size) {\n-\t\tctx->ops->update_i_size(inode, pos);\n+\t\tctx->ops->update_i_size(inode, end);\n \t\treturn;\n \t}\n \n-\ti_size_write(inode, pos);\n+\tspin_lock(&inode->i_lock);\n+\n+\ti_size = i_size_read(inode);\n+\tif (end > i_size) {\n+\t\ti_size_write(inode, end);\n #if IS_ENABLED(CONFIG_FSCACHE)\n-\tfscache_update_cookie(ctx->cache, NULL, &pos);\n+\t\tfscache_update_cookie(ctx->cache, NULL, &end);\n #endif\n \n-\tgap = SECTOR_SIZE - (i_size & (SECTOR_SIZE - 1));\n-\tif (copied > gap) {\n-\t\tadd = DIV_ROUND_UP(copied - gap, SECTOR_SIZE);\n+\t\tgap = SECTOR_SIZE - (i_size & (SECTOR_SIZE - 1));\n+\t\tif (copied > gap) {\n+\t\t\tadd = DIV_ROUND_UP(copied - gap, SECTOR_SIZE);\n \n-\t\tinode->i_blocks = min_t(blkcnt_t,\n-\t\t\t\t\tDIV_ROUND_UP(pos, SECTOR_SIZE),\n-\t\t\t\t\tinode->i_blocks + add);\n+\t\t\tinode->i_blocks = min_t(blkcnt_t,\n+\t\t\t\t\t\tDIV_ROUND_UP(end, SECTOR_SIZE),\n+\t\t\t\t\t\tinode->i_blocks + add);\n+\t\t}\n \t}\n+\tspin_unlock(&inode->i_lock);\n }\n \n /**\n@@ -111,7 +121,7 @@ ssize_t netfs_perform_write(struct kiocb *iocb, struct iov_iter *iter,\n \tstruct folio *folio = NULL, *writethrough = NULL;\n \tunsigned int bdp_flags = (iocb->ki_flags & IOCB_NOWAIT) ? BDP_ASYNC : 0;\n \tssize_t written = 0, ret, ret2;\n-\tloff_t i_size, pos = iocb->ki_pos;\n+\tloff_t pos = iocb->ki_pos;\n \tsize_t max_chunk = mapping_max_folio_size(mapping);\n \tbool maybe_trouble = false;\n \n@@ -344,10 +354,8 @@ ssize_t netfs_perform_write(struct kiocb *iocb, struct iov_iter *iter,\n \t\tflush_dcache_folio(folio);\n \n \t\t/* Update the inode size if we moved the EOF marker */\n+\t\tnetfs_update_i_size(ctx, inode, pos, copied);\n \t\tpos += copied;\n-\t\ti_size = i_size_read(inode);\n-\t\tif (pos > i_size)\n-\t\t\tnetfs_update_i_size(ctx, inode, i_size, pos, copied);\n \t\twritten += copied;\n \n \t\tif (likely(!wreq)) {\ndiff --git a/fs/netfs/direct_write.c b/fs/netfs/direct_write.c\nindex fa9a5bf3c6d5..a16660ab7f83 100644\n--- a/fs/netfs/direct_write.c\n+++ b/fs/netfs/direct_write.c\n@@ -9,20 +9,6 @@\n #include <linux/uio.h>\n #include \"internal.h\"\n \n-static void netfs_cleanup_dio_write(struct netfs_io_request *wreq)\n-{\n-\tstruct inode *inode = wreq->inode;\n-\tunsigned long long end = wreq->start + wreq->transferred;\n-\n-\tif (!wreq->error &&\n-\t    i_size_read(inode) < end) {\n-\t\tif (wreq->netfs_ops->update_i_size)\n-\t\t\twreq->netfs_ops->update_i_size(inode, end);\n-\t\telse\n-\t\t\ti_size_write(inode, end);\n-\t}\n-}\n-\n /*\n  * Perform an unbuffered write where we may have to do an RMW operation on an\n  * encrypted file.  This can also be used for direct I/O writes.\n@@ -98,7 +84,6 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \tif (async)\n \t\twreq->iocb = iocb;\n \twreq->len = iov_iter_count(&wreq->buffer.iter);\n-\twreq->cleanup = netfs_cleanup_dio_write;\n \tret = netfs_unbuffered_write(wreq, is_sync_kiocb(iocb), wreq->len);\n \tif (ret < 0) {\n \t\t_debug(\"begin = %zd\", ret);\n@@ -106,7 +91,6 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t}\n \n \tif (!async) {\n-\t\ttrace_netfs_rreq(wreq, netfs_rreq_trace_wait_ip);\n \t\tret = netfs_wait_for_write(wreq);\n \t\tif (ret > 0)\n \t\t\tiocb->ki_pos += ret;\ndiff --git a/fs/netfs/internal.h b/fs/netfs/internal.h\nindex e2ee9183392b..d4f16fefd965 100644\n--- a/fs/netfs/internal.h\n+++ b/fs/netfs/internal.h\n@@ -27,6 +27,12 @@ void netfs_cache_read_terminated(void *priv, ssize_t transferred_or_error);\n int netfs_prefetch_for_write(struct file *file, struct folio *folio,\n \t\t\t     size_t offset, size_t len);\n \n+/*\n+ * buffered_write.c\n+ */\n+void netfs_update_i_size(struct netfs_inode *ctx, struct inode *inode,\n+\t\t\t loff_t pos, size_t copied);\n+\n /*\n  * main.c\n  */\n@@ -267,13 +273,31 @@ static inline void netfs_wake_rreq_flag(struct netfs_io_request *rreq,\n \t\t\t\t\tenum netfs_rreq_trace trace)\n {\n \tif (test_bit(rreq_flag, &rreq->flags)) {\n-\t\ttrace_netfs_rreq(rreq, trace);\n \t\tclear_bit_unlock(rreq_flag, &rreq->flags);\n \t\tsmp_mb__after_atomic(); /* Set flag before task state */\n+\t\ttrace_netfs_rreq(rreq, trace);\n \t\twake_up(&rreq->waitq);\n \t}\n }\n \n+/*\n+ * Test the NETFS_RREQ_IN_PROGRESS flag, inserting an appropriate barrier.\n+ */\n+static inline bool netfs_check_rreq_in_progress(const struct netfs_io_request *rreq)\n+{\n+\t/* Order read of flags before read of anything else, such as error. */\n+\treturn test_bit_acquire(NETFS_RREQ_IN_PROGRESS, &rreq->flags);\n+}\n+\n+/*\n+ * Test the NETFS_SREQ_IN_PROGRESS flag, inserting an appropriate barrier.\n+ */\n+static inline bool netfs_check_subreq_in_progress(const struct netfs_io_subrequest *subreq)\n+{\n+\t/* Order read of flags before read of anything else, such as error. */\n+\treturn test_bit_acquire(NETFS_SREQ_IN_PROGRESS, &subreq->flags);\n+}\n+\n /*\n  * fscache-cache.c\n  */\ndiff --git a/fs/netfs/main.c b/fs/netfs/main.c\nindex 3db401d269e7..73da6c9f5777 100644\n--- a/fs/netfs/main.c\n+++ b/fs/netfs/main.c\n@@ -58,15 +58,15 @@ static int netfs_requests_seq_show(struct seq_file *m, void *v)\n \n \tif (v == &netfs_io_requests) {\n \t\tseq_puts(m,\n-\t\t\t \"REQUEST  OR REF FL ERR  OPS COVERAGE\\n\"\n-\t\t\t \"======== == === == ==== === =========\\n\"\n+\t\t\t \"REQUEST  OR REF FLAG ERR  OPS COVERAGE\\n\"\n+\t\t\t \"======== == === ==== ==== === =========\\n\"\n \t\t\t );\n \t\treturn 0;\n \t}\n \n \trreq = list_entry(v, struct netfs_io_request, proc_link);\n \tseq_printf(m,\n-\t\t   \"%08x %s %3d %2lx %4ld %3d @%04llx %llx/%llx\",\n+\t\t   \"%08x %s %3d %4lx %4ld %3d @%04llx %llx/%llx\",\n \t\t   rreq->debug_id,\n \t\t   netfs_origins[rreq->origin],\n \t\t   refcount_read(&rreq->ref),\ndiff --git a/fs/netfs/misc.c b/fs/netfs/misc.c\nindex 43b67a28a8fa..20748bcfbf59 100644\n--- a/fs/netfs/misc.c\n+++ b/fs/netfs/misc.c\n@@ -356,22 +356,22 @@ void netfs_wait_for_in_progress_stream(struct netfs_io_request *rreq,\n \tDEFINE_WAIT(myself);\n \n \tlist_for_each_entry(subreq, &stream->subrequests, rreq_link) {\n-\t\tif (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags))\n+\t\tif (!netfs_check_subreq_in_progress(subreq))\n \t\t\tcontinue;\n \n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_quiesce);\n \t\tfor (;;) {\n \t\t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n-\t\t\tif (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags))\n+\t\t\tif (!netfs_check_subreq_in_progress(subreq))\n \t\t\t\tbreak;\n \n \t\t\ttrace_netfs_sreq(subreq, netfs_sreq_trace_wait_for);\n \t\t\tschedule();\n-\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t\t}\n \t}\n \n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_quiesce);\n \tfinish_wait(&rreq->waitq, &myself);\n }\n \n@@ -381,7 +381,12 @@ void netfs_wait_for_in_progress_stream(struct netfs_io_request *rreq,\n static int netfs_collect_in_app(struct netfs_io_request *rreq,\n \t\t\t\tbool (*collector)(struct netfs_io_request *rreq))\n {\n-\tbool need_collect = false, inactive = true;\n+\tbool need_collect = false, inactive = true, done = true;\n+\n+\tif (!netfs_check_rreq_in_progress(rreq)) {\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_recollect);\n+\t\treturn 1; /* Done */\n+\t}\n \n \tfor (int i = 0; i < NR_IO_STREAMS; i++) {\n \t\tstruct netfs_io_subrequest *subreq;\n@@ -395,14 +400,16 @@ static int netfs_collect_in_app(struct netfs_io_request *rreq,\n \t\t\t\t\t\t  struct netfs_io_subrequest,\n \t\t\t\t\t\t  rreq_link);\n \t\tif (subreq &&\n-\t\t    (!test_bit(NETFS_SREQ_IN_PROGRESS, &subreq->flags) ||\n+\t\t    (!netfs_check_subreq_in_progress(subreq) ||\n \t\t     test_bit(NETFS_SREQ_MADE_PROGRESS, &subreq->flags))) {\n \t\t\tneed_collect = true;\n \t\t\tbreak;\n \t\t}\n+\t\tif (subreq || !test_bit(NETFS_RREQ_ALL_QUEUED, &rreq->flags))\n+\t\t\tdone = false;\n \t}\n \n-\tif (!need_collect && !inactive)\n+\tif (!need_collect && !inactive && !done)\n \t\treturn 0; /* Sleep */\n \n \t__set_current_state(TASK_RUNNING);\n@@ -423,14 +430,13 @@ static int netfs_collect_in_app(struct netfs_io_request *rreq,\n /*\n  * Wait for a request to complete, successfully or otherwise.\n  */\n-static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n-\t\t\t\t      bool (*collector)(struct netfs_io_request *rreq))\n+static ssize_t netfs_wait_for_in_progress(struct netfs_io_request *rreq,\n+\t\t\t\t\t  bool (*collector)(struct netfs_io_request *rreq))\n {\n \tDEFINE_WAIT(myself);\n \tssize_t ret;\n \n \tfor (;;) {\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n \t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n \t\tif (!test_bit(NETFS_RREQ_OFFLOAD_COLLECTION, &rreq->flags)) {\n@@ -440,18 +446,22 @@ static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n \t\t\tcase 1:\n \t\t\t\tgoto all_collected;\n \t\t\tcase 2:\n+\t\t\t\tif (!netfs_check_rreq_in_progress(rreq))\n+\t\t\t\t\tbreak;\n+\t\t\t\tcond_resched();\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n \n-\t\tif (!test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags))\n+\t\tif (!netfs_check_rreq_in_progress(rreq))\n \t\t\tbreak;\n \n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_ip);\n \t\tschedule();\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t}\n \n all_collected:\n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_ip);\n \tfinish_wait(&rreq->waitq, &myself);\n \n \tret = rreq->error;\n@@ -478,12 +488,12 @@ static ssize_t netfs_wait_for_request(struct netfs_io_request *rreq,\n \n ssize_t netfs_wait_for_read(struct netfs_io_request *rreq)\n {\n-\treturn netfs_wait_for_request(rreq, netfs_read_collection);\n+\treturn netfs_wait_for_in_progress(rreq, netfs_read_collection);\n }\n \n ssize_t netfs_wait_for_write(struct netfs_io_request *rreq)\n {\n-\treturn netfs_wait_for_request(rreq, netfs_write_collection);\n+\treturn netfs_wait_for_in_progress(rreq, netfs_write_collection);\n }\n \n /*\n@@ -494,10 +504,8 @@ static void netfs_wait_for_pause(struct netfs_io_request *rreq,\n {\n \tDEFINE_WAIT(myself);\n \n-\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_pause);\n-\n \tfor (;;) {\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_queue);\n+\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_wait_pause);\n \t\tprepare_to_wait(&rreq->waitq, &myself, TASK_UNINTERRUPTIBLE);\n \n \t\tif (!test_bit(NETFS_RREQ_OFFLOAD_COLLECTION, &rreq->flags)) {\n@@ -507,19 +515,23 @@ static void netfs_wait_for_pause(struct netfs_io_request *rreq,\n \t\t\tcase 1:\n \t\t\t\tgoto all_collected;\n \t\t\tcase 2:\n+\t\t\t\tif (!netfs_check_rreq_in_progress(rreq) ||\n+\t\t\t\t    !test_bit(NETFS_RREQ_PAUSE, &rreq->flags))\n+\t\t\t\t\tbreak;\n+\t\t\t\tcond_resched();\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n \n-\t\tif (!test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags) ||\n+\t\tif (!netfs_check_rreq_in_progress(rreq) ||\n \t\t    !test_bit(NETFS_RREQ_PAUSE, &rreq->flags))\n \t\t\tbreak;\n \n \t\tschedule();\n-\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_woke_queue);\n \t}\n \n all_collected:\n+\ttrace_netfs_rreq(rreq, netfs_rreq_trace_waited_pause);\n \tfinish_wait(&rreq->waitq, &myself);\n }\n \ndiff --git a/fs/netfs/read_collect.c b/fs/netfs/read_collect.c\nindex 96ee18af28ef..3e804da1e1eb 100644\n--- a/fs/netfs/read_collect.c\n+++ b/fs/netfs/read_collect.c\n@@ -218,7 +218,7 @@ static void netfs_collect_read_results(struct netfs_io_request *rreq)\n \t\t\tstream->collected_to = front->start;\n \t\t}\n \n-\t\tif (test_bit(NETFS_SREQ_IN_PROGRESS, &front->flags))\n+\t\tif (netfs_check_subreq_in_progress(front))\n \t\t\tnotes |= HIT_PENDING;\n \t\tsmp_rmb(); /* Read counters after IN_PROGRESS flag. */\n \t\ttransferred = READ_ONCE(front->transferred);\n@@ -293,7 +293,9 @@ static void netfs_collect_read_results(struct netfs_io_request *rreq)\n \t\tspin_lock(&rreq->lock);\n \n \t\tremove = front;\n-\t\ttrace_netfs_sreq(front, netfs_sreq_trace_discard);\n+\t\ttrace_netfs_sreq(front,\n+\t\t\t\t notes & ABANDON_SREQ ?\n+\t\t\t\t netfs_sreq_trace_abandoned : netfs_sreq_trace_consumed);\n \t\tlist_del_init(&front->rreq_link);\n \t\tfront = list_first_entry_or_null(&stream->subrequests,\n \t\t\t\t\t\t struct netfs_io_subrequest, rreq_link);\n@@ -353,9 +355,11 @@ static void netfs_rreq_assess_dio(struct netfs_io_request *rreq)\n \n \tif (rreq->iocb) {\n \t\trreq->iocb->ki_pos += rreq->transferred;\n-\t\tif (rreq->iocb->ki_complete)\n+\t\tif (rreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_ki_complete);\n \t\t\trreq->iocb->ki_complete(\n \t\t\t\trreq->iocb, rreq->error ? rreq->error : rreq->transferred);\n+\t\t}\n \t}\n \tif (rreq->netfs_ops->done)\n \t\trreq->netfs_ops->done(rreq);\n@@ -379,9 +383,11 @@ static void netfs_rreq_assess_single(struct netfs_io_request *rreq)\n \n \tif (rreq->iocb) {\n \t\trreq->iocb->ki_pos += rreq->transferred;\n-\t\tif (rreq->iocb->ki_complete)\n+\t\tif (rreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(rreq, netfs_rreq_trace_ki_complete);\n \t\t\trreq->iocb->ki_complete(\n \t\t\t\trreq->iocb, rreq->error ? rreq->error : rreq->transferred);\n+\t\t}\n \t}\n \tif (rreq->netfs_ops->done)\n \t\trreq->netfs_ops->done(rreq);\n@@ -445,7 +451,7 @@ void netfs_read_collection_worker(struct work_struct *work)\n \tstruct netfs_io_request *rreq = container_of(work, struct netfs_io_request, work);\n \n \tnetfs_see_request(rreq, netfs_rreq_trace_see_work);\n-\tif (test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags)) {\n+\tif (netfs_check_rreq_in_progress(rreq)) {\n \t\tif (netfs_read_collection(rreq))\n \t\t\t/* Drop the ref from the IN_PROGRESS flag. */\n \t\t\tnetfs_put_request(rreq, netfs_rreq_trace_put_work_ip);\ndiff --git a/fs/netfs/write_collect.c b/fs/netfs/write_collect.c\nindex e2b102ffb768..0f3a36852a4d 100644\n--- a/fs/netfs/write_collect.c\n+++ b/fs/netfs/write_collect.c\n@@ -240,7 +240,7 @@ static void netfs_collect_write_results(struct netfs_io_request *wreq)\n \t\t\t}\n \n \t\t\t/* Stall if the front is still undergoing I/O. */\n-\t\t\tif (test_bit(NETFS_SREQ_IN_PROGRESS, &front->flags)) {\n+\t\t\tif (netfs_check_subreq_in_progress(front)) {\n \t\t\t\tnotes |= HIT_PENDING;\n \t\t\t\tbreak;\n \t\t\t}\n@@ -393,8 +393,10 @@ bool netfs_write_collection(struct netfs_io_request *wreq)\n \t\tictx->ops->invalidate_cache(wreq);\n \t}\n \n-\tif (wreq->cleanup)\n-\t\twreq->cleanup(wreq);\n+\tif ((wreq->origin == NETFS_UNBUFFERED_WRITE ||\n+\t     wreq->origin == NETFS_DIO_WRITE) &&\n+\t    !wreq->error)\n+\t\tnetfs_update_i_size(ictx, &ictx->inode, wreq->start, wreq->transferred);\n \n \tif (wreq->origin == NETFS_DIO_WRITE &&\n \t    wreq->mapping->nrpages) {\n@@ -419,9 +421,11 @@ bool netfs_write_collection(struct netfs_io_request *wreq)\n \tif (wreq->iocb) {\n \t\tsize_t written = min(wreq->transferred, wreq->len);\n \t\twreq->iocb->ki_pos += written;\n-\t\tif (wreq->iocb->ki_complete)\n+\t\tif (wreq->iocb->ki_complete) {\n+\t\t\ttrace_netfs_rreq(wreq, netfs_rreq_trace_ki_complete);\n \t\t\twreq->iocb->ki_complete(\n \t\t\t\twreq->iocb, wreq->error ? wreq->error : written);\n+\t\t}\n \t\twreq->iocb = VFS_PTR_POISON;\n \t}\n \n@@ -434,7 +438,7 @@ void netfs_write_collection_worker(struct work_struct *work)\n \tstruct netfs_io_request *rreq = container_of(work, struct netfs_io_request, work);\n \n \tnetfs_see_request(rreq, netfs_rreq_trace_see_work);\n-\tif (test_bit(NETFS_RREQ_IN_PROGRESS, &rreq->flags)) {\n+\tif (netfs_check_rreq_in_progress(rreq)) {\n \t\tif (netfs_write_collection(rreq))\n \t\t\t/* Drop the ref from the IN_PROGRESS flag. */\n \t\t\tnetfs_put_request(rreq, netfs_rreq_trace_put_work_ip);\ndiff --git a/fs/netfs/write_retry.c b/fs/netfs/write_retry.c\nindex 9d1d8a8bab72..fc9c3e0d34d8 100644\n--- a/fs/netfs/write_retry.c\n+++ b/fs/netfs/write_retry.c\n@@ -146,14 +146,13 @@ static void netfs_retry_write_stream(struct netfs_io_request *wreq,\n \t\t\tsubreq = netfs_alloc_subrequest(wreq);\n \t\t\tsubreq->source\t\t= to->source;\n \t\t\tsubreq->start\t\t= start;\n-\t\t\tsubreq->debug_index\t= atomic_inc_return(&wreq->subreq_counter);\n \t\t\tsubreq->stream_nr\t= to->stream_nr;\n \t\t\tsubreq->retry_count\t= 1;\n \n \t\t\ttrace_netfs_sreq_ref(wreq->debug_id, subreq->debug_index,\n \t\t\t\t\t     refcount_read(&subreq->ref),\n \t\t\t\t\t     netfs_sreq_trace_new);\n-\t\t\tnetfs_get_subrequest(subreq, netfs_sreq_trace_get_resubmit);\n+\t\t\ttrace_netfs_sreq(subreq, netfs_sreq_trace_split);\n \n \t\t\tlist_add(&subreq->rreq_link, &to->rreq_link);\n \t\t\tto = list_next_entry(to, rreq_link);\ndiff --git a/fs/smb/client/cifssmb.c b/fs/smb/client/cifssmb.c\nindex 7216fcec79e8..75142f49d65d 100644\n--- a/fs/smb/client/cifssmb.c\n+++ b/fs/smb/client/cifssmb.c\n@@ -1334,7 +1334,12 @@ cifs_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\tgoto do_retry;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_retry_needed);\n+do_retry:\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &rdata->subreq.flags);\n \t\trdata->result = -EAGAIN;\n \t\tif (server->sign && rdata->got_bytes)\n \t\t\t/* reset bytes number since we can not check a sign */\n@@ -1343,8 +1348,14 @@ cifs_readv_callback(struct mid_q_entry *mid)\n \t\ttask_io_account_read(rdata->got_bytes);\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n+\tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_malformed);\n+\t\trdata->result = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_unknown);\n \t\trdata->result = -EIO;\n+\t\tbreak;\n \t}\n \n \tif (rdata->result == -ENODATA) {\n@@ -1713,10 +1724,21 @@ cifs_writev_callback(struct mid_q_entry *mid)\n \t\t}\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n+\t\tresult = -EAGAIN;\n+\t\tbreak;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_retry_needed);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n \t\tresult = -EAGAIN;\n \t\tbreak;\n+\tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_malformed);\n+\t\tresult = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_unknown);\n \t\tresult = -EIO;\n \t\tbreak;\n \t}\ndiff --git a/fs/smb/client/smb2pdu.c b/fs/smb/client/smb2pdu.c\nindex a717be1626a3..7f6186c2e60d 100644\n--- a/fs/smb/client/smb2pdu.c\n+++ b/fs/smb/client/smb2pdu.c\n@@ -4567,7 +4567,11 @@ smb2_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\tgoto do_retry;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_retry_needed);\n+do_retry:\n \t\t__set_bit(NETFS_SREQ_NEED_RETRY, &rdata->subreq.flags);\n \t\trdata->result = -EAGAIN;\n \t\tif (server->sign && rdata->got_bytes)\n@@ -4578,11 +4582,15 @@ smb2_readv_callback(struct mid_q_entry *mid)\n \t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n \t\tbreak;\n \tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_malformed);\n \t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n-\t\tfallthrough;\n+\t\trdata->result = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&rdata->subreq, netfs_sreq_trace_io_unknown);\n \t\trdata->result = -EIO;\n+\t\tbreak;\n \t}\n #ifdef CONFIG_CIFS_SMB_DIRECT\n \t/*\n@@ -4835,11 +4843,14 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \n \tswitch (mid->mid_state) {\n \tcase MID_RESPONSE_RECEIVED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_progress);\n \t\tcredits.value = le16_to_cpu(rsp->hdr.CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n \t\tresult = smb2_check_receive(mid, server, 0);\n-\t\tif (result != 0)\n+\t\tif (result != 0) {\n+\t\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_bad);\n \t\t\tbreak;\n+\t\t}\n \n \t\twritten = le32_to_cpu(rsp->DataLength);\n \t\t/*\n@@ -4861,14 +4872,23 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \t\t}\n \t\tbreak;\n \tcase MID_REQUEST_SUBMITTED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_req_submitted);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n+\t\tresult = -EAGAIN;\n+\t\tbreak;\n \tcase MID_RETRY_NEEDED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_retry_needed);\n+\t\t__set_bit(NETFS_SREQ_NEED_RETRY, &wdata->subreq.flags);\n \t\tresult = -EAGAIN;\n \t\tbreak;\n \tcase MID_RESPONSE_MALFORMED:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_malformed);\n \t\tcredits.value = le16_to_cpu(rsp->hdr.CreditRequest);\n \t\tcredits.instance = server->reconnect_instance;\n-\t\tfallthrough;\n+\t\tresult = -EIO;\n+\t\tbreak;\n \tdefault:\n+\t\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_unknown);\n \t\tresult = -EIO;\n \t\tbreak;\n \t}\n@@ -4908,7 +4928,6 @@ smb2_writev_callback(struct mid_q_entry *mid)\n \t\t\t      server->credits, server->in_flight,\n \t\t\t      0, cifs_trace_rw_credits_write_response_clear);\n \twdata->credits.value = 0;\n-\ttrace_netfs_sreq(&wdata->subreq, netfs_sreq_trace_io_progress);\n \tcifs_write_subrequest_terminated(wdata, result ?: written);\n \trelease_mid(mid);\n \ttrace_smb3_rw_credits(rreq_debug_id, subreq_debug_index, 0,\ndiff --git a/include/linux/fs.h b/include/linux/fs.h\nindex b085f161ed22..040c0036320f 100644\n--- a/include/linux/fs.h\n+++ b/include/linux/fs.h\n@@ -3608,6 +3608,8 @@ extern int simple_write_begin(struct file *file, struct address_space *mapping,\n extern const struct address_space_operations ram_aops;\n extern int always_delete_dentry(const struct dentry *);\n extern struct inode *alloc_anon_inode(struct super_block *);\n+struct inode *anon_inode_make_secure_inode(struct super_block *sb, const char *name,\n+\t\t\t\t\t   const struct inode *context_inode);\n extern int simple_nosetlease(struct file *, int, struct file_lease **, void **);\n extern const struct dentry_operations simple_dentry_operations;\n \ndiff --git a/include/linux/netfs.h b/include/linux/netfs.h\nindex 065c17385e53..f43f075852c0 100644\n--- a/include/linux/netfs.h\n+++ b/include/linux/netfs.h\n@@ -265,21 +265,20 @@ struct netfs_io_request {\n \tbool\t\t\tdirect_bv_unpin; /* T if direct_bv[] must be unpinned */\n \trefcount_t\t\tref;\n \tunsigned long\t\tflags;\n-#define NETFS_RREQ_OFFLOAD_COLLECTION\t0\t/* Offload collection to workqueue */\n-#define NETFS_RREQ_NO_UNLOCK_FOLIO\t2\t/* Don't unlock no_unlock_folio on completion */\n-#define NETFS_RREQ_FAILED\t\t4\t/* The request failed */\n-#define NETFS_RREQ_IN_PROGRESS\t\t5\t/* Unlocked when the request completes (has ref) */\n-#define NETFS_RREQ_FOLIO_COPY_TO_CACHE\t6\t/* Copy current folio to cache from read */\n-#define NETFS_RREQ_UPLOAD_TO_SERVER\t8\t/* Need to write to the server */\n-#define NETFS_RREQ_PAUSE\t\t11\t/* Pause subrequest generation */\n+#define NETFS_RREQ_IN_PROGRESS\t\t0\t/* Unlocked when the request completes (has ref) */\n+#define NETFS_RREQ_ALL_QUEUED\t\t1\t/* All subreqs are now queued */\n+#define NETFS_RREQ_PAUSE\t\t2\t/* Pause subrequest generation */\n+#define NETFS_RREQ_FAILED\t\t3\t/* The request failed */\n+#define NETFS_RREQ_RETRYING\t\t4\t/* Set if we're in the retry path */\n+#define NETFS_RREQ_SHORT_TRANSFER\t5\t/* Set if we have a short transfer */\n+#define NETFS_RREQ_OFFLOAD_COLLECTION\t8\t/* Offload collection to workqueue */\n+#define NETFS_RREQ_NO_UNLOCK_FOLIO\t9\t/* Don't unlock no_unlock_folio on completion */\n+#define NETFS_RREQ_FOLIO_COPY_TO_CACHE\t10\t/* Copy current folio to cache from read */\n+#define NETFS_RREQ_UPLOAD_TO_SERVER\t11\t/* Need to write to the server */\n #define NETFS_RREQ_USE_IO_ITER\t\t12\t/* Use ->io_iter rather than ->i_pages */\n-#define NETFS_RREQ_ALL_QUEUED\t\t13\t/* All subreqs are now queued */\n-#define NETFS_RREQ_RETRYING\t\t14\t/* Set if we're in the retry path */\n-#define NETFS_RREQ_SHORT_TRANSFER\t15\t/* Set if we have a short transfer */\n #define NETFS_RREQ_USE_PGPRIV2\t\t31\t/* [DEPRECATED] Use PG_private_2 to mark\n \t\t\t\t\t\t * write to cache on read */\n \tconst struct netfs_request_ops *netfs_ops;\n-\tvoid (*cleanup)(struct netfs_io_request *req);\n };\n \n /*\ndiff --git a/include/trace/events/netfs.h b/include/trace/events/netfs.h\nindex 333d2e38dd2c..73e96ccbe830 100644\n--- a/include/trace/events/netfs.h\n+++ b/include/trace/events/netfs.h\n@@ -50,12 +50,14 @@\n \n #define netfs_rreq_traces\t\t\t\t\t\\\n \tEM(netfs_rreq_trace_assess,\t\t\"ASSESS \")\t\\\n-\tEM(netfs_rreq_trace_copy,\t\t\"COPY   \")\t\\\n \tEM(netfs_rreq_trace_collect,\t\t\"COLLECT\")\t\\\n \tEM(netfs_rreq_trace_complete,\t\t\"COMPLET\")\t\\\n+\tEM(netfs_rreq_trace_copy,\t\t\"COPY   \")\t\\\n \tEM(netfs_rreq_trace_dirty,\t\t\"DIRTY  \")\t\\\n \tEM(netfs_rreq_trace_done,\t\t\"DONE   \")\t\\\n \tEM(netfs_rreq_trace_free,\t\t\"FREE   \")\t\\\n+\tEM(netfs_rreq_trace_ki_complete,\t\"KI-CMPL\")\t\\\n+\tEM(netfs_rreq_trace_recollect,\t\t\"RECLLCT\")\t\\\n \tEM(netfs_rreq_trace_redirty,\t\t\"REDIRTY\")\t\\\n \tEM(netfs_rreq_trace_resubmit,\t\t\"RESUBMT\")\t\\\n \tEM(netfs_rreq_trace_set_abandon,\t\"S-ABNDN\")\t\\\n@@ -63,13 +65,15 @@\n \tEM(netfs_rreq_trace_unlock,\t\t\"UNLOCK \")\t\\\n \tEM(netfs_rreq_trace_unlock_pgpriv2,\t\"UNLCK-2\")\t\\\n \tEM(netfs_rreq_trace_unmark,\t\t\"UNMARK \")\t\\\n+\tEM(netfs_rreq_trace_unpause,\t\t\"UNPAUSE\")\t\\\n \tEM(netfs_rreq_trace_wait_ip,\t\t\"WAIT-IP\")\t\\\n-\tEM(netfs_rreq_trace_wait_pause,\t\t\"WT-PAUS\")\t\\\n-\tEM(netfs_rreq_trace_wait_queue,\t\t\"WAIT-Q \")\t\\\n+\tEM(netfs_rreq_trace_wait_pause,\t\t\"--PAUSED--\")\t\\\n+\tEM(netfs_rreq_trace_wait_quiesce,\t\"WAIT-QUIESCE\")\t\\\n+\tEM(netfs_rreq_trace_waited_ip,\t\t\"DONE-IP\")\t\\\n+\tEM(netfs_rreq_trace_waited_pause,\t\"--UNPAUSED--\")\t\\\n+\tEM(netfs_rreq_trace_waited_quiesce,\t\"DONE-QUIESCE\")\t\\\n \tEM(netfs_rreq_trace_wake_ip,\t\t\"WAKE-IP\")\t\\\n \tEM(netfs_rreq_trace_wake_queue,\t\t\"WAKE-Q \")\t\\\n-\tEM(netfs_rreq_trace_woke_queue,\t\t\"WOKE-Q \")\t\\\n-\tEM(netfs_rreq_trace_unpause,\t\t\"UNPAUSE\")\t\\\n \tE_(netfs_rreq_trace_write_done,\t\t\"WR-DONE\")\n \n #define netfs_sreq_sources\t\t\t\t\t\\\n@@ -82,6 +86,7 @@\n \tE_(NETFS_WRITE_TO_CACHE,\t\t\"WRIT\")\n \n #define netfs_sreq_traces\t\t\t\t\t\\\n+\tEM(netfs_sreq_trace_abandoned,\t\t\"ABNDN\")\t\\\n \tEM(netfs_sreq_trace_add_donations,\t\"+DON \")\t\\\n \tEM(netfs_sreq_trace_added,\t\t\"ADD  \")\t\\\n \tEM(netfs_sreq_trace_cache_nowrite,\t\"CA-NW\")\t\\\n@@ -89,6 +94,7 @@\n \tEM(netfs_sreq_trace_cache_write,\t\"CA-WR\")\t\\\n \tEM(netfs_sreq_trace_cancel,\t\t\"CANCL\")\t\\\n \tEM(netfs_sreq_trace_clear,\t\t\"CLEAR\")\t\\\n+\tEM(netfs_sreq_trace_consumed,\t\t\"CONSM\")\t\\\n \tEM(netfs_sreq_trace_discard,\t\t\"DSCRD\")\t\\\n \tEM(netfs_sreq_trace_donate_to_prev,\t\"DON-P\")\t\\\n \tEM(netfs_sreq_trace_donate_to_next,\t\"DON-N\")\t\\\n@@ -96,7 +102,12 @@\n \tEM(netfs_sreq_trace_fail,\t\t\"FAIL \")\t\\\n \tEM(netfs_sreq_trace_free,\t\t\"FREE \")\t\\\n \tEM(netfs_sreq_trace_hit_eof,\t\t\"EOF  \")\t\\\n-\tEM(netfs_sreq_trace_io_progress,\t\"IO   \")\t\\\n+\tEM(netfs_sreq_trace_io_bad,\t\t\"I-BAD\")\t\\\n+\tEM(netfs_sreq_trace_io_malformed,\t\"I-MLF\")\t\\\n+\tEM(netfs_sreq_trace_io_unknown,\t\t\"I-UNK\")\t\\\n+\tEM(netfs_sreq_trace_io_progress,\t\"I-OK \")\t\\\n+\tEM(netfs_sreq_trace_io_req_submitted,\t\"I-RSB\")\t\\\n+\tEM(netfs_sreq_trace_io_retry_needed,\t\"I-RTR\")\t\\\n \tEM(netfs_sreq_trace_limited,\t\t\"LIMIT\")\t\\\n \tEM(netfs_sreq_trace_need_clear,\t\t\"N-CLR\")\t\\\n \tEM(netfs_sreq_trace_partial_read,\t\"PARTR\")\t\\\n@@ -142,8 +153,8 @@\n \n #define netfs_sreq_ref_traces\t\t\t\t\t\\\n \tEM(netfs_sreq_trace_get_copy_to_cache,\t\"GET COPY2C \")\t\\\n-\tEM(netfs_sreq_trace_get_resubmit,\t\"GET RESUBMIT\")\t\\\n-\tEM(netfs_sreq_trace_get_submit,\t\t\"GET SUBMIT\")\t\\\n+\tEM(netfs_sreq_trace_get_resubmit,\t\"GET RESUBMT\")\t\\\n+\tEM(netfs_sreq_trace_get_submit,\t\t\"GET SUBMIT \")\t\\\n \tEM(netfs_sreq_trace_get_short_read,\t\"GET SHORTRD\")\t\\\n \tEM(netfs_sreq_trace_new,\t\t\"NEW        \")\t\\\n \tEM(netfs_sreq_trace_put_abandon,\t\"PUT ABANDON\")\t\\\n@@ -366,7 +377,7 @@ TRACE_EVENT(netfs_sreq,\n \t\t    __entry->slot\t= sreq->io_iter.folioq_slot;\n \t\t\t   ),\n \n-\t    TP_printk(\"R=%08x[%x] %s %s f=%02x s=%llx %zx/%zx s=%u e=%d\",\n+\t    TP_printk(\"R=%08x[%x] %s %s f=%03x s=%llx %zx/%zx s=%u e=%d\",\n \t\t      __entry->rreq, __entry->index,\n \t\t      __print_symbolic(__entry->source, netfs_sreq_sources),\n \t\t      __print_symbolic(__entry->what, netfs_sreq_traces),\ndiff --git a/mm/secretmem.c b/mm/secretmem.c\nindex 589b26c2d553..9a11a38a6770 100644\n--- a/mm/secretmem.c\n+++ b/mm/secretmem.c\n@@ -195,18 +195,11 @@ static struct file *secretmem_file_create(unsigned long flags)\n \tstruct file *file;\n \tstruct inode *inode;\n \tconst char *anon_name = \"[secretmem]\";\n-\tint err;\n \n-\tinode = alloc_anon_inode(secretmem_mnt->mnt_sb);\n+\tinode = anon_inode_make_secure_inode(secretmem_mnt->mnt_sb, anon_name, NULL);\n \tif (IS_ERR(inode))\n \t\treturn ERR_CAST(inode);\n \n-\terr = security_inode_init_security_anon(inode, &QSTR(anon_name), NULL);\n-\tif (err) {\n-\t\tfile = ERR_PTR(err);\n-\t\tgoto err_free_inode;\n-\t}\n-\n \tfile = alloc_file_pseudo(inode, secretmem_mnt, \"secretmem\",\n \t\t\t\t O_RDWR, &secretmem_fops);\n \tif (IS_ERR(file))\ndiff --git a/tools/testing/selftests/coredump/stackdump_test.c b/tools/testing/selftests/coredump/stackdump_test.c\nindex 9984413be9f0..68f8e479ac36 100644\n--- a/tools/testing/selftests/coredump/stackdump_test.c\n+++ b/tools/testing/selftests/coredump/stackdump_test.c\n@@ -461,10 +461,15 @@ TEST_F(coredump, socket_detect_userspace_client)\n \t\t\t_exit(EXIT_FAILURE);\n \t\t}\n \n+\t\tret = read(fd_coredump, &c, 1);\n+\n \t\tclose(fd_coredump);\n \t\tclose(fd_server);\n \t\tclose(fd_peer_pidfd);\n \t\tclose(fd_core_file);\n+\n+\t\tif (ret < 1)\n+\t\t\t_exit(EXIT_FAILURE);\n \t\t_exit(EXIT_SUCCESS);\n \t}\n \tself->pid_coredump_server = pid_coredump_server;",
    "stats": {
      "insertions": 351,
      "deletions": 438,
      "files": 21
    }
  },
  {
    "sha": "d38376b3ee48d073c64e75e150510d7e6b4b04f7",
    "message": "drm/imagination: Fix kernel crash when hard resetting the GPU\n\nThe GPU hard reset sequence calls pm_runtime_force_suspend() and\npm_runtime_force_resume(), which according to their documentation should\nonly be used during system-wide PM transitions to sleep states.\n\nThe main issue though is that depending on some internal runtime PM\nstate as seen by pm_runtime_force_suspend() (whether the usage count is\n<= 1), pm_runtime_force_resume() might not resume the device unless\nneeded. If that happens, the runtime PM resume callback\npvr_power_device_resume() is not called, the GPU clocks are not\nre-enabled, and the kernel crashes on the next attempt to access GPU\nregisters as part of the power-on sequence.\n\nReplace calls to pm_runtime_force_suspend() and\npm_runtime_force_resume() with direct calls to the driver's runtime PM\ncallbacks, pvr_power_device_suspend() and pvr_power_device_resume(),\nto ensure clocks are re-enabled and avoid the kernel crash.\n\nFixes: cc1aeedb98ad (\"drm/imagination: Implement firmware infrastructure and META FW support\")\nSigned-off-by: Alessio Belle <alessio.belle@imgtec.com>\nReviewed-by: Matt Coster <matt.coster@imgtec.com>\nLink: https://lore.kernel.org/r/20250624-fix-kernel-crash-gpu-hard-reset-v1-1-6d24810d72a6@imgtec.com\nCc: stable@vger.kernel.org\nSigned-off-by: Matt Coster <matt.coster@imgtec.com>",
    "author": "Alessio Belle",
    "date": "2025-07-04T16:32:10+01:00",
    "files_changed": [
      "drivers/gpu/drm/imagination/pvr_power.c"
    ],
    "diff": "diff --git a/drivers/gpu/drm/imagination/pvr_power.c b/drivers/gpu/drm/imagination/pvr_power.c\nindex 41f5d89e78b8..3e349d039fc0 100644\n--- a/drivers/gpu/drm/imagination/pvr_power.c\n+++ b/drivers/gpu/drm/imagination/pvr_power.c\n@@ -386,13 +386,13 @@ pvr_power_reset(struct pvr_device *pvr_dev, bool hard_reset)\n \t\tif (!err) {\n \t\t\tif (hard_reset) {\n \t\t\t\tpvr_dev->fw_dev.booted = false;\n-\t\t\t\tWARN_ON(pm_runtime_force_suspend(from_pvr_device(pvr_dev)->dev));\n+\t\t\t\tWARN_ON(pvr_power_device_suspend(from_pvr_device(pvr_dev)->dev));\n \n \t\t\t\terr = pvr_fw_hard_reset(pvr_dev);\n \t\t\t\tif (err)\n \t\t\t\t\tgoto err_device_lost;\n \n-\t\t\t\terr = pm_runtime_force_resume(from_pvr_device(pvr_dev)->dev);\n+\t\t\t\terr = pvr_power_device_resume(from_pvr_device(pvr_dev)->dev);\n \t\t\t\tpvr_dev->fw_dev.booted = true;\n \t\t\t\tif (err)\n \t\t\t\t\tgoto err_device_lost;",
    "stats": {
      "insertions": 2,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "ef8923e6c051a98164c2889db943df9695a39888",
    "message": "arm64: efi: Fix KASAN false positive for EFI runtime stack\n\nKASAN reports invalid accesses during arch_stack_walk() for EFI runtime\nservices due to vmalloc tagging[1]. The EFI runtime stack must be allocated\nwith KASAN tags reset to avoid false positives.\n\nThis patch uses arch_alloc_vmap_stack() instead of __vmalloc_node() for\nEFI stack allocation, which internally calls kasan_reset_tag()\n\nThe changes ensure EFI runtime stacks are properly sanitized for KASAN\nwhile maintaining functional consistency.\n\nLink: https://lore.kernel.org/all/aFVVEgD0236LdrL6@gmail.com/ [1]\nSuggested-by: Andrey Konovalov <andreyknvl@gmail.com>\nSuggested-by: Catalin Marinas <catalin.marinas@arm.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20250704-arm_kasan-v2-1-32ebb4fd7607@debian.org\nSigned-off-by: Will Deacon <will@kernel.org>",
    "author": "Breno Leitao",
    "date": "2025-07-04T14:47:06+01:00",
    "files_changed": [
      "arch/arm64/kernel/efi.c"
    ],
    "diff": "diff --git a/arch/arm64/kernel/efi.c b/arch/arm64/kernel/efi.c\nindex 3857fd7ee8d4..62230d6dd919 100644\n--- a/arch/arm64/kernel/efi.c\n+++ b/arch/arm64/kernel/efi.c\n@@ -15,6 +15,7 @@\n \n #include <asm/efi.h>\n #include <asm/stacktrace.h>\n+#include <asm/vmap_stack.h>\n \n static bool region_is_misaligned(const efi_memory_desc_t *md)\n {\n@@ -214,9 +215,13 @@ static int __init arm64_efi_rt_init(void)\n \tif (!efi_enabled(EFI_RUNTIME_SERVICES))\n \t\treturn 0;\n \n-\tp = __vmalloc_node(THREAD_SIZE, THREAD_ALIGN, GFP_KERNEL,\n-\t\t\t   NUMA_NO_NODE, &&l);\n-l:\tif (!p) {\n+\tif (!IS_ENABLED(CONFIG_VMAP_STACK)) {\n+\t\tclear_bit(EFI_RUNTIME_SERVICES, &efi.flags);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tp = arch_alloc_vmap_stack(THREAD_SIZE, NUMA_NO_NODE);\n+\tif (!p) {\n \t\tpr_warn(\"Failed to allocate EFI runtime stack\\n\");\n \t\tclear_bit(EFI_RUNTIME_SERVICES, &efi.flags);\n \t\treturn -ENOMEM;",
    "stats": {
      "insertions": 8,
      "deletions": 3,
      "files": 1
    }
  },
  {
    "sha": "44306a684cd1699b8562a54945ddc43e2abc9eab",
    "message": "drm/tegra: nvdec: Fix dma_alloc_coherent error check\n\nCheck for NULL return value with dma_alloc_coherent, in line with\nRobin's fix for vic.c in 'drm/tegra: vic: Fix DMA API misuse'.\n\nFixes: 46f226c93d35 (\"drm/tegra: Add NVDEC driver\")\nSigned-off-by: Mikko Perttunen <mperttunen@nvidia.com>\nSigned-off-by: Thierry Reding <treding@nvidia.com>\nLink: https://lore.kernel.org/r/20250702-nvdec-dma-error-check-v1-1-c388b402c53a@nvidia.com",
    "author": "Mikko Perttunen",
    "date": "2025-07-04T11:15:07+02:00",
    "files_changed": [
      "drivers/gpu/drm/tegra/nvdec.c"
    ],
    "diff": "diff --git a/drivers/gpu/drm/tegra/nvdec.c b/drivers/gpu/drm/tegra/nvdec.c\nindex 2d9a0a3f6c38..7a38664e890e 100644\n--- a/drivers/gpu/drm/tegra/nvdec.c\n+++ b/drivers/gpu/drm/tegra/nvdec.c\n@@ -261,10 +261,8 @@ static int nvdec_load_falcon_firmware(struct nvdec *nvdec)\n \n \tif (!client->group) {\n \t\tvirt = dma_alloc_coherent(nvdec->dev, size, &iova, GFP_KERNEL);\n-\n-\t\terr = dma_mapping_error(nvdec->dev, iova);\n-\t\tif (err < 0)\n-\t\t\treturn err;\n+\t\tif (!virt)\n+\t\t\treturn -ENOMEM;\n \t} else {\n \t\tvirt = tegra_drm_alloc(tegra, size, &iova);\n \t\tif (IS_ERR(virt))",
    "stats": {
      "insertions": 2,
      "deletions": 4,
      "files": 1
    }
  }
]
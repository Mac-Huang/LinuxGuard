[
  {
    "sha": "25b1b75bbaf96331750fb01302825069657b2ff8",
    "message": "iommu/vt-d: Assign devtlb cache tag on ATS enablement\n\nCommit <4f1492efb495> (\"iommu/vt-d: Revert ATS timing change to fix boot\nfailure\") placed the enabling of ATS in the probe_finalize callback. This\noccurs after the default domain attachment, which is when the ATS cache\ntag is assigned. Consequently, the device TLB cache tag is missed when the\ndomain is attached, leading to the device TLB not being invalidated in the\niommu_unmap paths.\n\nFix this by assigning the CACHE_TAG_DEVTLB cache tag when ATS is enabled.\n\nFixes: 4f1492efb495 (\"iommu/vt-d: Revert ATS timing change to fix boot failure\")\nCc: stable@vger.kernel.org\nSuggested-by: Kevin Tian <kevin.tian@intel.com>\nSigned-off-by: Lu Baolu <baolu.lu@linux.intel.com>\nTested-by: Shuicheng Lin <shuicheng.lin@intel.com>\nReviewed-by: Kevin Tian <kevin.tian@intel.com>\nLink: https://lore.kernel.org/r/20250625050135.3129955-1-baolu.lu@linux.intel.com\nLink: https://lore.kernel.org/r/20250628100351.3198955-2-baolu.lu@linux.intel.com\nSigned-off-by: Joerg Roedel <joerg.roedel@amd.com>",
    "author": "Lu Baolu",
    "date": "2025-07-04T10:33:56+02:00",
    "files_changed": [
      "drivers/iommu/intel/cache.c",
      "drivers/iommu/intel/iommu.c",
      "drivers/iommu/intel/iommu.h"
    ],
    "diff": "diff --git a/drivers/iommu/intel/cache.c b/drivers/iommu/intel/cache.c\nindex fc35cba59145..47692cbfaabd 100644\n--- a/drivers/iommu/intel/cache.c\n+++ b/drivers/iommu/intel/cache.c\n@@ -40,9 +40,8 @@ static bool cache_tage_match(struct cache_tag *tag, u16 domain_id,\n }\n \n /* Assign a cache tag with specified type to domain. */\n-static int cache_tag_assign(struct dmar_domain *domain, u16 did,\n-\t\t\t    struct device *dev, ioasid_t pasid,\n-\t\t\t    enum cache_tag_type type)\n+int cache_tag_assign(struct dmar_domain *domain, u16 did, struct device *dev,\n+\t\t     ioasid_t pasid, enum cache_tag_type type)\n {\n \tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n \tstruct intel_iommu *iommu = info->iommu;\ndiff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c\nindex 7aa3932251b2..148b944143b8 100644\n--- a/drivers/iommu/intel/iommu.c\n+++ b/drivers/iommu/intel/iommu.c\n@@ -3780,8 +3780,17 @@ static void intel_iommu_probe_finalize(struct device *dev)\n \t    !pci_enable_pasid(to_pci_dev(dev), info->pasid_supported & ~1))\n \t\tinfo->pasid_enabled = 1;\n \n-\tif (sm_supported(iommu) && !dev_is_real_dma_subdevice(dev))\n+\tif (sm_supported(iommu) && !dev_is_real_dma_subdevice(dev)) {\n \t\tiommu_enable_pci_ats(info);\n+\t\t/* Assign a DEVTLB cache tag to the default domain. */\n+\t\tif (info->ats_enabled && info->domain) {\n+\t\t\tu16 did = domain_id_iommu(info->domain, iommu);\n+\n+\t\t\tif (cache_tag_assign(info->domain, did, dev,\n+\t\t\t\t\t     IOMMU_NO_PASID, CACHE_TAG_DEVTLB))\n+\t\t\t\tiommu_disable_pci_ats(info);\n+\t\t}\n+\t}\n \tiommu_enable_pci_pri(info);\n }\n \ndiff --git a/drivers/iommu/intel/iommu.h b/drivers/iommu/intel/iommu.h\nindex 3ddbcc603de2..2d1afab5eedc 100644\n--- a/drivers/iommu/intel/iommu.h\n+++ b/drivers/iommu/intel/iommu.h\n@@ -1289,6 +1289,8 @@ struct cache_tag {\n \tunsigned int users;\n };\n \n+int cache_tag_assign(struct dmar_domain *domain, u16 did, struct device *dev,\n+\t\t     ioasid_t pasid, enum cache_tag_type type);\n int cache_tag_assign_domain(struct dmar_domain *domain,\n \t\t\t    struct device *dev, ioasid_t pasid);\n void cache_tag_unassign_domain(struct dmar_domain *domain,",
    "stats": {
      "insertions": 14,
      "deletions": 4,
      "files": 3
    }
  },
  {
    "sha": "30e0fd3c0273dc106320081793793a424f1f1950",
    "message": "gpiolib: fix performance regression when using gpio_chip_get_multiple()\n\ncommit 74abd086d2ee (\"gpiolib: sanitize the return value of\ngpio_chip::get_multiple()\") altered the value returned by\ngc->get_multiple() in case it is positive (> 0), but failed to return\nfor other cases (<= 0).\n\nThis may result in the \"if (gc->get)\" block being executed and thus\nnegates the performance gain that is normally obtained by using\ngc->get_multiple().\n\nFix by returning the result of gc->get_multiple() if it is <= 0.\n\nAlso move the \"ret\" variable to the scope where it is used, which as an\nadded bonus fixes an indentation error introduced by the aforementioned\ncommit.\n\nFixes: 74abd086d2ee (\"gpiolib: sanitize the return value of gpio_chip::get_multiple()\")\nCc: stable@vger.kernel.org\nSigned-off-by: Hugo Villeneuve <hvilleneuve@dimonoff.com>\nLink: https://lore.kernel.org/r/20250703191829.2952986-1-hugo@hugovil.com\nSigned-off-by: Bartosz Golaszewski <bartosz.golaszewski@linaro.org>",
    "author": "Hugo Villeneuve",
    "date": "2025-07-04T10:24:03+02:00",
    "files_changed": [
      "drivers/gpio/gpiolib.c"
    ],
    "diff": "diff --git a/drivers/gpio/gpiolib.c b/drivers/gpio/gpiolib.c\nindex fdafa0df1b43..3a3eca5b4c40 100644\n--- a/drivers/gpio/gpiolib.c\n+++ b/drivers/gpio/gpiolib.c\n@@ -3297,14 +3297,15 @@ static int gpiod_get_raw_value_commit(const struct gpio_desc *desc)\n static int gpio_chip_get_multiple(struct gpio_chip *gc,\n \t\t\t\t  unsigned long *mask, unsigned long *bits)\n {\n-\tint ret;\n-\t\n \tlockdep_assert_held(&gc->gpiodev->srcu);\n \n \tif (gc->get_multiple) {\n+\t\tint ret;\n+\n \t\tret = gc->get_multiple(gc, mask, bits);\n \t\tif (ret > 0)\n \t\t\treturn -EBADE;\n+\t\treturn ret;\n \t}\n \n \tif (gc->get) {",
    "stats": {
      "insertions": 3,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "043faef334a1f3d96ae88e1b7618bfa2b4946388",
    "message": "ALSA: ad1816a: Fix potential NULL pointer deref in snd_card_ad1816a_pnp()\n\nUse pr_warn() instead of dev_warn() when 'pdev' is NULL to avoid a\npotential NULL pointer dereference.\n\nCc: stable@vger.kernel.org\nFixes: 20869176d7a7 (\"ALSA: ad1816a: Use standard print API\")\nSigned-off-by: Thorsten Blum <thorsten.blum@linux.dev>\nLink: https://patch.msgid.link/20250703200616.304309-2-thorsten.blum@linux.dev\nSigned-off-by: Takashi Iwai <tiwai@suse.de>",
    "author": "Thorsten Blum",
    "date": "2025-07-04T09:04:12+02:00",
    "files_changed": [
      "sound/isa/ad1816a/ad1816a.c"
    ],
    "diff": "diff --git a/sound/isa/ad1816a/ad1816a.c b/sound/isa/ad1816a/ad1816a.c\nindex 99006dc4777e..5c9e2d41d900 100644\n--- a/sound/isa/ad1816a/ad1816a.c\n+++ b/sound/isa/ad1816a/ad1816a.c\n@@ -98,7 +98,7 @@ static int snd_card_ad1816a_pnp(int dev, struct pnp_card_link *card,\n \tpdev = pnp_request_card_device(card, id->devs[1].id, NULL);\n \tif (pdev == NULL) {\n \t\tmpu_port[dev] = -1;\n-\t\tdev_warn(&pdev->dev, \"MPU401 device busy, skipping.\\n\");\n+\t\tpr_warn(\"MPU401 device busy, skipping.\\n\");\n \t\treturn 0;\n \t}\n ",
    "stats": {
      "insertions": 1,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "4cf65845fdd09d711fc7546d60c9abe010956922",
    "message": "Input: cs40l50-vibra - fix potential NULL dereference in cs40l50_upload_owt()\n\nThe cs40l50_upload_owt() function allocates memory via kmalloc()\nwithout checking for allocation failure, which could lead to a\nNULL pointer dereference.\n\nReturn -ENOMEM in case allocation fails.\n\nSigned-off-by: Yunshui Jiang <jiangyunshui@kylinos.cn>\nFixes: c38fe1bb5d21 (\"Input: cs40l50 - Add support for the CS40L50 haptic driver\")\nLink: https://lore.kernel.org/r/20250704024010.2353841-1-jiangyunshui@kylinos.cn\nCc: stable@vger.kernel.org\nSigned-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>",
    "author": "Yunshui Jiang",
    "date": "2025-07-03T22:20:46-07:00",
    "files_changed": [
      "drivers/input/misc/cs40l50-vibra.c"
    ],
    "diff": "diff --git a/drivers/input/misc/cs40l50-vibra.c b/drivers/input/misc/cs40l50-vibra.c\nindex dce3b0ec8cf3..330f09123631 100644\n--- a/drivers/input/misc/cs40l50-vibra.c\n+++ b/drivers/input/misc/cs40l50-vibra.c\n@@ -238,6 +238,8 @@ static int cs40l50_upload_owt(struct cs40l50_work *work_data)\n \theader.data_words = len / sizeof(u32);\n \n \tnew_owt_effect_data = kmalloc(sizeof(header) + len, GFP_KERNEL);\n+\tif (!new_owt_effect_data)\n+\t\treturn -ENOMEM;\n \n \tmemcpy(new_owt_effect_data, &header, sizeof(header));\n \tmemcpy(new_owt_effect_data + sizeof(header), work_data->custom_data, len);",
    "stats": {
      "insertions": 2,
      "deletions": 0,
      "files": 1
    }
  },
  {
    "sha": "da8d8e9001c6a3741e9bec26a6cdcfd75ecabc88",
    "message": "Merge tag 'drm-xe-fixes-2025-07-03' of https://gitlab.freedesktop.org/drm/xe/kernel into drm-fixes\n\nDriver Changes:\n- Fix chunking the PTE updates and overflowing the maximum number of\n  dwords with with MI_STORE_DATA_IMM (Jia Yao)\n- Move WA BB to the LRC BO to mitigate hangs on context switch (Matthew\n  Brost)\n- Fix frequency/flush WAs for BMG (Vinay / Lucas)\n- Fix kconfig prompt title and description (Lucas)\n- Do not require kunit (Harry Austen / Lucas)\n- Extend 14018094691 WA to BMG (Daniele)\n- Fix wedging the device on signal (Matthew Brost)\n\nSigned-off-by: Dave Airlie <airlied@redhat.com>\n\nFrom: Lucas De Marchi <lucas.demarchi@intel.com>\nLink: https://lore.kernel.org/r/o5662wz6nrlf6xt5sjgxq5oe6qoujefzywuwblm3m626hreifv@foqayqydd6ig",
    "author": "Dave Airlie",
    "date": "2025-07-04T10:01:53+10:00",
    "files_changed": [
      "drivers/gpu/drm/xe/xe_device.c",
      "drivers/gpu/drm/xe/xe_drv.h",
      "drivers/gpu/drm/xe/xe_guc_pc.c",
      "drivers/gpu/drm/xe/xe_guc_pc.h",
      "drivers/gpu/drm/xe/xe_guc_pc_types.h",
      "drivers/gpu/drm/xe/xe_guc_submit.c",
      "drivers/gpu/drm/xe/xe_lrc.c",
      "drivers/gpu/drm/xe/xe_lrc_types.h",
      "drivers/gpu/drm/xe/xe_migrate.c"
    ],
    "diff": "diff --git a/drivers/gpu/drm/xe/Kconfig b/drivers/gpu/drm/xe/Kconfig\nindex fcc2677a4229..99a91355842e 100644\n--- a/drivers/gpu/drm/xe/Kconfig\n+++ b/drivers/gpu/drm/xe/Kconfig\n@@ -1,7 +1,8 @@\n # SPDX-License-Identifier: GPL-2.0-only\n config DRM_XE\n-\ttristate \"Intel Xe Graphics\"\n-\tdepends on DRM && PCI && (m || (y && KUNIT=y))\n+\ttristate \"Intel Xe2 Graphics\"\n+\tdepends on DRM && PCI\n+\tdepends on KUNIT || !KUNIT\n \tdepends on INTEL_VSEC || !INTEL_VSEC\n \tdepends on X86_PLATFORM_DEVICES || !(X86 && ACPI)\n \tselect INTERVAL_TREE\n@@ -46,7 +47,8 @@ config DRM_XE\n \tselect AUXILIARY_BUS\n \tselect HMM_MIRROR\n \thelp\n-\t  Experimental driver for Intel Xe series GPUs\n+\t  Driver for Intel Xe2 series GPUs and later. Experimental support\n+\t  for Xe series is also available.\n \n \t  If \"M\" is selected, the module will be called xe.\n \ndiff --git a/drivers/gpu/drm/xe/xe_device.c b/drivers/gpu/drm/xe/xe_device.c\nindex c02c4c4e9412..e9f3c1a53db2 100644\n--- a/drivers/gpu/drm/xe/xe_device.c\n+++ b/drivers/gpu/drm/xe/xe_device.c\n@@ -40,6 +40,7 @@\n #include \"xe_gt_printk.h\"\n #include \"xe_gt_sriov_vf.h\"\n #include \"xe_guc.h\"\n+#include \"xe_guc_pc.h\"\n #include \"xe_hw_engine_group.h\"\n #include \"xe_hwmon.h\"\n #include \"xe_irq.h\"\n@@ -986,38 +987,15 @@ void xe_device_wmb(struct xe_device *xe)\n \t\txe_mmio_write32(xe_root_tile_mmio(xe), VF_CAP_REG, 0);\n }\n \n-/**\n- * xe_device_td_flush() - Flush transient L3 cache entries\n- * @xe: The device\n- *\n- * Display engine has direct access to memory and is never coherent with L3/L4\n- * caches (or CPU caches), however KMD is responsible for specifically flushing\n- * transient L3 GPU cache entries prior to the flip sequence to ensure scanout\n- * can happen from such a surface without seeing corruption.\n- *\n- * Display surfaces can be tagged as transient by mapping it using one of the\n- * various L3:XD PAT index modes on Xe2.\n- *\n- * Note: On non-discrete xe2 platforms, like LNL, the entire L3 cache is flushed\n- * at the end of each submission via PIPE_CONTROL for compute/render, since SA\n- * Media is not coherent with L3 and we want to support render-vs-media\n- * usescases. For other engines like copy/blt the HW internally forces uncached\n- * behaviour, hence why we can skip the TDF on such platforms.\n+/*\n+ * Issue a TRANSIENT_FLUSH_REQUEST and wait for completion on each gt.\n  */\n-void xe_device_td_flush(struct xe_device *xe)\n+static void tdf_request_sync(struct xe_device *xe)\n {\n-\tstruct xe_gt *gt;\n \tunsigned int fw_ref;\n+\tstruct xe_gt *gt;\n \tu8 id;\n \n-\tif (!IS_DGFX(xe) || GRAPHICS_VER(xe) < 20)\n-\t\treturn;\n-\n-\tif (XE_WA(xe_root_mmio_gt(xe), 16023588340)) {\n-\t\txe_device_l2_flush(xe);\n-\t\treturn;\n-\t}\n-\n \tfor_each_gt(gt, xe, id) {\n \t\tif (xe_gt_is_media_type(gt))\n \t\t\tcontinue;\n@@ -1027,6 +1005,7 @@ void xe_device_td_flush(struct xe_device *xe)\n \t\t\treturn;\n \n \t\txe_mmio_write32(&gt->mmio, XE2_TDF_CTRL, TRANSIENT_FLUSH_REQUEST);\n+\n \t\t/*\n \t\t * FIXME: We can likely do better here with our choice of\n \t\t * timeout. Currently we just assume the worst case, i.e. 150us,\n@@ -1057,15 +1036,52 @@ void xe_device_l2_flush(struct xe_device *xe)\n \t\treturn;\n \n \tspin_lock(&gt->global_invl_lock);\n-\txe_mmio_write32(&gt->mmio, XE2_GLOBAL_INVAL, 0x1);\n \n+\txe_mmio_write32(&gt->mmio, XE2_GLOBAL_INVAL, 0x1);\n \tif (xe_mmio_wait32(&gt->mmio, XE2_GLOBAL_INVAL, 0x1, 0x0, 500, NULL, true))\n \t\txe_gt_err_once(gt, \"Global invalidation timeout\\n\");\n+\n \tspin_unlock(&gt->global_invl_lock);\n \n \txe_force_wake_put(gt_to_fw(gt), fw_ref);\n }\n \n+/**\n+ * xe_device_td_flush() - Flush transient L3 cache entries\n+ * @xe: The device\n+ *\n+ * Display engine has direct access to memory and is never coherent with L3/L4\n+ * caches (or CPU caches), however KMD is responsible for specifically flushing\n+ * transient L3 GPU cache entries prior to the flip sequence to ensure scanout\n+ * can happen from such a surface without seeing corruption.\n+ *\n+ * Display surfaces can be tagged as transient by mapping it using one of the\n+ * various L3:XD PAT index modes on Xe2.\n+ *\n+ * Note: On non-discrete xe2 platforms, like LNL, the entire L3 cache is flushed\n+ * at the end of each submission via PIPE_CONTROL for compute/render, since SA\n+ * Media is not coherent with L3 and we want to support render-vs-media\n+ * usescases. For other engines like copy/blt the HW internally forces uncached\n+ * behaviour, hence why we can skip the TDF on such platforms.\n+ */\n+void xe_device_td_flush(struct xe_device *xe)\n+{\n+\tstruct xe_gt *root_gt;\n+\n+\tif (!IS_DGFX(xe) || GRAPHICS_VER(xe) < 20)\n+\t\treturn;\n+\n+\troot_gt = xe_root_mmio_gt(xe);\n+\tif (XE_WA(root_gt, 16023588340)) {\n+\t\t/* A transient flush is not sufficient: flush the L2 */\n+\t\txe_device_l2_flush(xe);\n+\t} else {\n+\t\txe_guc_pc_apply_flush_freq_limit(&root_gt->uc.guc.pc);\n+\t\ttdf_request_sync(xe);\n+\t\txe_guc_pc_remove_flush_freq_limit(&root_gt->uc.guc.pc);\n+\t}\n+}\n+\n u32 xe_device_ccs_bytes(struct xe_device *xe, u64 size)\n {\n \treturn xe_device_has_flat_ccs(xe) ?\ndiff --git a/drivers/gpu/drm/xe/xe_drv.h b/drivers/gpu/drm/xe/xe_drv.h\nindex d61650d4aa0b..95242a375e54 100644\n--- a/drivers/gpu/drm/xe/xe_drv.h\n+++ b/drivers/gpu/drm/xe/xe_drv.h\n@@ -9,7 +9,7 @@\n #include <drm/drm_drv.h>\n \n #define DRIVER_NAME\t\t\"xe\"\n-#define DRIVER_DESC\t\t\"Intel Xe Graphics\"\n+#define DRIVER_DESC\t\t\"Intel Xe2 Graphics\"\n \n /* Interface history:\n  *\ndiff --git a/drivers/gpu/drm/xe/xe_guc_pc.c b/drivers/gpu/drm/xe/xe_guc_pc.c\nindex 3beaaa7b25c1..c0ca61695d76 100644\n--- a/drivers/gpu/drm/xe/xe_guc_pc.c\n+++ b/drivers/gpu/drm/xe/xe_guc_pc.c\n@@ -5,8 +5,11 @@\n \n #include \"xe_guc_pc.h\"\n \n+#include <linux/cleanup.h>\n #include <linux/delay.h>\n+#include <linux/jiffies.h>\n #include <linux/ktime.h>\n+#include <linux/wait_bit.h>\n \n #include <drm/drm_managed.h>\n #include <drm/drm_print.h>\n@@ -51,9 +54,12 @@\n \n #define LNL_MERT_FREQ_CAP\t800\n #define BMG_MERT_FREQ_CAP\t2133\n+#define BMG_MIN_FREQ\t\t1200\n+#define BMG_MERT_FLUSH_FREQ_CAP\t2600\n \n #define SLPC_RESET_TIMEOUT_MS 5 /* roughly 5ms, but no need for precision */\n #define SLPC_RESET_EXTENDED_TIMEOUT_MS 1000 /* To be used only at pc_start */\n+#define SLPC_ACT_FREQ_TIMEOUT_MS 100\n \n /**\n  * DOC: GuC Power Conservation (PC)\n@@ -141,6 +147,36 @@ static int wait_for_pc_state(struct xe_guc_pc *pc,\n \treturn -ETIMEDOUT;\n }\n \n+static int wait_for_flush_complete(struct xe_guc_pc *pc)\n+{\n+\tconst unsigned long timeout = msecs_to_jiffies(30);\n+\n+\tif (!wait_var_event_timeout(&pc->flush_freq_limit,\n+\t\t\t\t    !atomic_read(&pc->flush_freq_limit),\n+\t\t\t\t    timeout))\n+\t\treturn -ETIMEDOUT;\n+\n+\treturn 0;\n+}\n+\n+static int wait_for_act_freq_limit(struct xe_guc_pc *pc, u32 freq)\n+{\n+\tint timeout_us = SLPC_ACT_FREQ_TIMEOUT_MS * USEC_PER_MSEC;\n+\tint slept, wait = 10;\n+\n+\tfor (slept = 0; slept < timeout_us;) {\n+\t\tif (xe_guc_pc_get_act_freq(pc) <= freq)\n+\t\t\treturn 0;\n+\n+\t\tusleep_range(wait, wait << 1);\n+\t\tslept += wait;\n+\t\twait <<= 1;\n+\t\tif (slept + wait > timeout_us)\n+\t\t\twait = timeout_us - slept;\n+\t}\n+\n+\treturn -ETIMEDOUT;\n+}\n static int pc_action_reset(struct xe_guc_pc *pc)\n {\n \tstruct xe_guc_ct *ct = pc_to_ct(pc);\n@@ -553,6 +589,25 @@ u32 xe_guc_pc_get_rpn_freq(struct xe_guc_pc *pc)\n \treturn pc->rpn_freq;\n }\n \n+static int xe_guc_pc_get_min_freq_locked(struct xe_guc_pc *pc, u32 *freq)\n+{\n+\tint ret;\n+\n+\tlockdep_assert_held(&pc->freq_lock);\n+\n+\t/* Might be in the middle of a gt reset */\n+\tif (!pc->freq_ready)\n+\t\treturn -EAGAIN;\n+\n+\tret = pc_action_query_task_state(pc);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\t*freq = pc_get_min_freq(pc);\n+\n+\treturn 0;\n+}\n+\n /**\n  * xe_guc_pc_get_min_freq - Get the min operational frequency\n  * @pc: The GuC PC\n@@ -562,27 +617,29 @@ u32 xe_guc_pc_get_rpn_freq(struct xe_guc_pc *pc)\n  *         -EAGAIN if GuC PC not ready (likely in middle of a reset).\n  */\n int xe_guc_pc_get_min_freq(struct xe_guc_pc *pc, u32 *freq)\n+{\n+\tguard(mutex)(&pc->freq_lock);\n+\n+\treturn xe_guc_pc_get_min_freq_locked(pc, freq);\n+}\n+\n+static int xe_guc_pc_set_min_freq_locked(struct xe_guc_pc *pc, u32 freq)\n {\n \tint ret;\n \n-\txe_device_assert_mem_access(pc_to_xe(pc));\n+\tlockdep_assert_held(&pc->freq_lock);\n \n-\tmutex_lock(&pc->freq_lock);\n-\tif (!pc->freq_ready) {\n-\t\t/* Might be in the middle of a gt reset */\n-\t\tret = -EAGAIN;\n-\t\tgoto out;\n-\t}\n+\t/* Might be in the middle of a gt reset */\n+\tif (!pc->freq_ready)\n+\t\treturn -EAGAIN;\n \n-\tret = pc_action_query_task_state(pc);\n+\tret = pc_set_min_freq(pc, freq);\n \tif (ret)\n-\t\tgoto out;\n+\t\treturn ret;\n \n-\t*freq = pc_get_min_freq(pc);\n+\tpc->user_requested_min = freq;\n \n-out:\n-\tmutex_unlock(&pc->freq_lock);\n-\treturn ret;\n+\treturn 0;\n }\n \n /**\n@@ -595,25 +652,29 @@ int xe_guc_pc_get_min_freq(struct xe_guc_pc *pc, u32 *freq)\n  *         -EINVAL if value out of bounds.\n  */\n int xe_guc_pc_set_min_freq(struct xe_guc_pc *pc, u32 freq)\n+{\n+\tguard(mutex)(&pc->freq_lock);\n+\n+\treturn xe_guc_pc_set_min_freq_locked(pc, freq);\n+}\n+\n+static int xe_guc_pc_get_max_freq_locked(struct xe_guc_pc *pc, u32 *freq)\n {\n \tint ret;\n \n-\tmutex_lock(&pc->freq_lock);\n-\tif (!pc->freq_ready) {\n-\t\t/* Might be in the middle of a gt reset */\n-\t\tret = -EAGAIN;\n-\t\tgoto out;\n-\t}\n+\tlockdep_assert_held(&pc->freq_lock);\n \n-\tret = pc_set_min_freq(pc, freq);\n+\t/* Might be in the middle of a gt reset */\n+\tif (!pc->freq_ready)\n+\t\treturn -EAGAIN;\n+\n+\tret = pc_action_query_task_state(pc);\n \tif (ret)\n-\t\tgoto out;\n+\t\treturn ret;\n \n-\tpc->user_requested_min = freq;\n+\t*freq = pc_get_max_freq(pc);\n \n-out:\n-\tmutex_unlock(&pc->freq_lock);\n-\treturn ret;\n+\treturn 0;\n }\n \n /**\n@@ -625,25 +686,29 @@ int xe_guc_pc_set_min_freq(struct xe_guc_pc *pc, u32 freq)\n  *         -EAGAIN if GuC PC not ready (likely in middle of a reset).\n  */\n int xe_guc_pc_get_max_freq(struct xe_guc_pc *pc, u32 *freq)\n+{\n+\tguard(mutex)(&pc->freq_lock);\n+\n+\treturn xe_guc_pc_get_max_freq_locked(pc, freq);\n+}\n+\n+static int xe_guc_pc_set_max_freq_locked(struct xe_guc_pc *pc, u32 freq)\n {\n \tint ret;\n \n-\tmutex_lock(&pc->freq_lock);\n-\tif (!pc->freq_ready) {\n-\t\t/* Might be in the middle of a gt reset */\n-\t\tret = -EAGAIN;\n-\t\tgoto out;\n-\t}\n+\tlockdep_assert_held(&pc->freq_lock);\n \n-\tret = pc_action_query_task_state(pc);\n+\t/* Might be in the middle of a gt reset */\n+\tif (!pc->freq_ready)\n+\t\treturn -EAGAIN;\n+\n+\tret = pc_set_max_freq(pc, freq);\n \tif (ret)\n-\t\tgoto out;\n+\t\treturn ret;\n \n-\t*freq = pc_get_max_freq(pc);\n+\tpc->user_requested_max = freq;\n \n-out:\n-\tmutex_unlock(&pc->freq_lock);\n-\treturn ret;\n+\treturn 0;\n }\n \n /**\n@@ -657,24 +722,14 @@ int xe_guc_pc_get_max_freq(struct xe_guc_pc *pc, u32 *freq)\n  */\n int xe_guc_pc_set_max_freq(struct xe_guc_pc *pc, u32 freq)\n {\n-\tint ret;\n-\n-\tmutex_lock(&pc->freq_lock);\n-\tif (!pc->freq_ready) {\n-\t\t/* Might be in the middle of a gt reset */\n-\t\tret = -EAGAIN;\n-\t\tgoto out;\n+\tif (XE_WA(pc_to_gt(pc), 22019338487)) {\n+\t\tif (wait_for_flush_complete(pc) != 0)\n+\t\t\treturn -EAGAIN;\n \t}\n \n-\tret = pc_set_max_freq(pc, freq);\n-\tif (ret)\n-\t\tgoto out;\n-\n-\tpc->user_requested_max = freq;\n+\tguard(mutex)(&pc->freq_lock);\n \n-out:\n-\tmutex_unlock(&pc->freq_lock);\n-\treturn ret;\n+\treturn xe_guc_pc_set_max_freq_locked(pc, freq);\n }\n \n /**\n@@ -817,6 +872,7 @@ void xe_guc_pc_init_early(struct xe_guc_pc *pc)\n \n static int pc_adjust_freq_bounds(struct xe_guc_pc *pc)\n {\n+\tstruct xe_tile *tile = gt_to_tile(pc_to_gt(pc));\n \tint ret;\n \n \tlockdep_assert_held(&pc->freq_lock);\n@@ -843,6 +899,9 @@ static int pc_adjust_freq_bounds(struct xe_guc_pc *pc)\n \tif (pc_get_min_freq(pc) > pc->rp0_freq)\n \t\tret = pc_set_min_freq(pc, pc->rp0_freq);\n \n+\tif (XE_WA(tile->primary_gt, 14022085890))\n+\t\tret = pc_set_min_freq(pc, max(BMG_MIN_FREQ, pc_get_min_freq(pc)));\n+\n out:\n \treturn ret;\n }\n@@ -868,30 +927,117 @@ static int pc_adjust_requested_freq(struct xe_guc_pc *pc)\n \treturn ret;\n }\n \n-static int pc_set_mert_freq_cap(struct xe_guc_pc *pc)\n+static bool needs_flush_freq_limit(struct xe_guc_pc *pc)\n {\n-\tint ret = 0;\n+\tstruct xe_gt *gt = pc_to_gt(pc);\n \n-\tif (XE_WA(pc_to_gt(pc), 22019338487)) {\n-\t\t/*\n-\t\t * Get updated min/max and stash them.\n-\t\t */\n-\t\tret = xe_guc_pc_get_min_freq(pc, &pc->stashed_min_freq);\n-\t\tif (!ret)\n-\t\t\tret = xe_guc_pc_get_max_freq(pc, &pc->stashed_max_freq);\n-\t\tif (ret)\n-\t\t\treturn ret;\n+\treturn  XE_WA(gt, 22019338487) &&\n+\t\tpc->rp0_freq > BMG_MERT_FLUSH_FREQ_CAP;\n+}\n+\n+/**\n+ * xe_guc_pc_apply_flush_freq_limit() - Limit max GT freq during L2 flush\n+ * @pc: the xe_guc_pc object\n+ *\n+ * As per the WA, reduce max GT frequency during L2 cache flush\n+ */\n+void xe_guc_pc_apply_flush_freq_limit(struct xe_guc_pc *pc)\n+{\n+\tstruct xe_gt *gt = pc_to_gt(pc);\n+\tu32 max_freq;\n+\tint ret;\n+\n+\tif (!needs_flush_freq_limit(pc))\n+\t\treturn;\n+\n+\tguard(mutex)(&pc->freq_lock);\n+\n+\tret = xe_guc_pc_get_max_freq_locked(pc, &max_freq);\n+\tif (!ret && max_freq > BMG_MERT_FLUSH_FREQ_CAP) {\n+\t\tret = pc_set_max_freq(pc, BMG_MERT_FLUSH_FREQ_CAP);\n+\t\tif (ret) {\n+\t\t\txe_gt_err_once(gt, \"Failed to cap max freq on flush to %u, %pe\\n\",\n+\t\t\t\t       BMG_MERT_FLUSH_FREQ_CAP, ERR_PTR(ret));\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tatomic_set(&pc->flush_freq_limit, 1);\n \n \t\t/*\n-\t\t * Ensure min and max are bound by MERT_FREQ_CAP until driver loads.\n+\t\t * If user has previously changed max freq, stash that value to\n+\t\t * restore later, otherwise use the current max. New user\n+\t\t * requests wait on flush.\n \t\t */\n-\t\tmutex_lock(&pc->freq_lock);\n-\t\tret = pc_set_min_freq(pc, min(pc->rpe_freq, pc_max_freq_cap(pc)));\n-\t\tif (!ret)\n-\t\t\tret = pc_set_max_freq(pc, min(pc->rp0_freq, pc_max_freq_cap(pc)));\n-\t\tmutex_unlock(&pc->freq_lock);\n+\t\tif (pc->user_requested_max != 0)\n+\t\t\tpc->stashed_max_freq = pc->user_requested_max;\n+\t\telse\n+\t\t\tpc->stashed_max_freq = max_freq;\n \t}\n \n+\t/*\n+\t * Wait for actual freq to go below the flush cap: even if the previous\n+\t * max was below cap, the current one might still be above it\n+\t */\n+\tret = wait_for_act_freq_limit(pc, BMG_MERT_FLUSH_FREQ_CAP);\n+\tif (ret)\n+\t\txe_gt_err_once(gt, \"Actual freq did not reduce to %u, %pe\\n\",\n+\t\t\t       BMG_MERT_FLUSH_FREQ_CAP, ERR_PTR(ret));\n+}\n+\n+/**\n+ * xe_guc_pc_remove_flush_freq_limit() - Remove max GT freq limit after L2 flush completes.\n+ * @pc: the xe_guc_pc object\n+ *\n+ * Retrieve the previous GT max frequency value.\n+ */\n+void xe_guc_pc_remove_flush_freq_limit(struct xe_guc_pc *pc)\n+{\n+\tstruct xe_gt *gt = pc_to_gt(pc);\n+\tint ret = 0;\n+\n+\tif (!needs_flush_freq_limit(pc))\n+\t\treturn;\n+\n+\tif (!atomic_read(&pc->flush_freq_limit))\n+\t\treturn;\n+\n+\tmutex_lock(&pc->freq_lock);\n+\n+\tret = pc_set_max_freq(&gt->uc.guc.pc, pc->stashed_max_freq);\n+\tif (ret)\n+\t\txe_gt_err_once(gt, \"Failed to restore max freq %u:%d\",\n+\t\t\t       pc->stashed_max_freq, ret);\n+\n+\tatomic_set(&pc->flush_freq_limit, 0);\n+\tmutex_unlock(&pc->freq_lock);\n+\twake_up_var(&pc->flush_freq_limit);\n+}\n+\n+static int pc_set_mert_freq_cap(struct xe_guc_pc *pc)\n+{\n+\tint ret;\n+\n+\tif (!XE_WA(pc_to_gt(pc), 22019338487))\n+\t\treturn 0;\n+\n+\tguard(mutex)(&pc->freq_lock);\n+\n+\t/*\n+\t * Get updated min/max and stash them.\n+\t */\n+\tret = xe_guc_pc_get_min_freq_locked(pc, &pc->stashed_min_freq);\n+\tif (!ret)\n+\t\tret = xe_guc_pc_get_max_freq_locked(pc, &pc->stashed_max_freq);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\t/*\n+\t * Ensure min and max are bound by MERT_FREQ_CAP until driver loads.\n+\t */\n+\tret = pc_set_min_freq(pc, min(pc->rpe_freq, pc_max_freq_cap(pc)));\n+\tif (!ret)\n+\t\tret = pc_set_max_freq(pc, min(pc->rp0_freq, pc_max_freq_cap(pc)));\n+\n \treturn ret;\n }\n \ndiff --git a/drivers/gpu/drm/xe/xe_guc_pc.h b/drivers/gpu/drm/xe/xe_guc_pc.h\nindex 0a2664d5c811..52ecdd5ddbff 100644\n--- a/drivers/gpu/drm/xe/xe_guc_pc.h\n+++ b/drivers/gpu/drm/xe/xe_guc_pc.h\n@@ -38,5 +38,7 @@ u64 xe_guc_pc_mc6_residency(struct xe_guc_pc *pc);\n void xe_guc_pc_init_early(struct xe_guc_pc *pc);\n int xe_guc_pc_restore_stashed_freq(struct xe_guc_pc *pc);\n void xe_guc_pc_raise_unslice(struct xe_guc_pc *pc);\n+void xe_guc_pc_apply_flush_freq_limit(struct xe_guc_pc *pc);\n+void xe_guc_pc_remove_flush_freq_limit(struct xe_guc_pc *pc);\n \n #endif /* _XE_GUC_PC_H_ */\ndiff --git a/drivers/gpu/drm/xe/xe_guc_pc_types.h b/drivers/gpu/drm/xe/xe_guc_pc_types.h\nindex 2978ac9a249b..c02053948a57 100644\n--- a/drivers/gpu/drm/xe/xe_guc_pc_types.h\n+++ b/drivers/gpu/drm/xe/xe_guc_pc_types.h\n@@ -15,6 +15,8 @@\n struct xe_guc_pc {\n \t/** @bo: GGTT buffer object that is shared with GuC PC */\n \tstruct xe_bo *bo;\n+\t/** @flush_freq_limit: 1 when max freq changes are limited by driver */\n+\tatomic_t flush_freq_limit;\n \t/** @rp0_freq: HW RP0 frequency - The Maximum one */\n \tu32 rp0_freq;\n \t/** @rpa_freq: HW RPa frequency - The Achievable one */\ndiff --git a/drivers/gpu/drm/xe/xe_guc_submit.c b/drivers/gpu/drm/xe/xe_guc_submit.c\nindex 9567f6700cf2..2ac87ff4a057 100644\n--- a/drivers/gpu/drm/xe/xe_guc_submit.c\n+++ b/drivers/gpu/drm/xe/xe_guc_submit.c\n@@ -891,12 +891,13 @@ static void xe_guc_exec_queue_lr_cleanup(struct work_struct *w)\n \tstruct xe_exec_queue *q = ge->q;\n \tstruct xe_guc *guc = exec_queue_to_guc(q);\n \tstruct xe_gpu_scheduler *sched = &ge->sched;\n-\tbool wedged;\n+\tbool wedged = false;\n \n \txe_gt_assert(guc_to_gt(guc), xe_exec_queue_is_lr(q));\n \ttrace_xe_exec_queue_lr_cleanup(q);\n \n-\twedged = guc_submit_hint_wedged(exec_queue_to_guc(q));\n+\tif (!exec_queue_killed(q))\n+\t\twedged = guc_submit_hint_wedged(exec_queue_to_guc(q));\n \n \t/* Kill the run_job / process_msg entry points */\n \txe_sched_submission_stop(sched);\n@@ -1070,7 +1071,7 @@ guc_exec_queue_timedout_job(struct drm_sched_job *drm_job)\n \tint err = -ETIME;\n \tpid_t pid = -1;\n \tint i = 0;\n-\tbool wedged, skip_timeout_check;\n+\tbool wedged = false, skip_timeout_check;\n \n \t/*\n \t * TDR has fired before free job worker. Common if exec queue\n@@ -1116,7 +1117,8 @@ guc_exec_queue_timedout_job(struct drm_sched_job *drm_job)\n \t * doesn't work for SRIOV. For now assuming timeouts in wedged mode are\n \t * genuine timeouts.\n \t */\n-\twedged = guc_submit_hint_wedged(exec_queue_to_guc(q));\n+\tif (!exec_queue_killed(q))\n+\t\twedged = guc_submit_hint_wedged(exec_queue_to_guc(q));\n \n \t/* Engine state now stable, disable scheduling to check timestamp */\n \tif (!wedged && exec_queue_registered(q)) {\ndiff --git a/drivers/gpu/drm/xe/xe_lrc.c b/drivers/gpu/drm/xe/xe_lrc.c\nindex bf7c3981897d..6e7b70532d11 100644\n--- a/drivers/gpu/drm/xe/xe_lrc.c\n+++ b/drivers/gpu/drm/xe/xe_lrc.c\n@@ -40,6 +40,7 @@\n \n #define LRC_PPHWSP_SIZE\t\t\t\tSZ_4K\n #define LRC_INDIRECT_RING_STATE_SIZE\t\tSZ_4K\n+#define LRC_WA_BB_SIZE\t\t\t\tSZ_4K\n \n static struct xe_device *\n lrc_to_xe(struct xe_lrc *lrc)\n@@ -910,7 +911,11 @@ static void xe_lrc_finish(struct xe_lrc *lrc)\n {\n \txe_hw_fence_ctx_finish(&lrc->fence_ctx);\n \txe_bo_unpin_map_no_vm(lrc->bo);\n-\txe_bo_unpin_map_no_vm(lrc->bb_per_ctx_bo);\n+}\n+\n+static size_t wa_bb_offset(struct xe_lrc *lrc)\n+{\n+\treturn lrc->bo->size - LRC_WA_BB_SIZE;\n }\n \n /*\n@@ -943,15 +948,16 @@ static void xe_lrc_finish(struct xe_lrc *lrc)\n #define CONTEXT_ACTIVE 1ULL\n static int xe_lrc_setup_utilization(struct xe_lrc *lrc)\n {\n+\tconst size_t max_size = LRC_WA_BB_SIZE;\n \tu32 *cmd, *buf = NULL;\n \n-\tif (lrc->bb_per_ctx_bo->vmap.is_iomem) {\n-\t\tbuf = kmalloc(lrc->bb_per_ctx_bo->size, GFP_KERNEL);\n+\tif (lrc->bo->vmap.is_iomem) {\n+\t\tbuf = kmalloc(max_size, GFP_KERNEL);\n \t\tif (!buf)\n \t\t\treturn -ENOMEM;\n \t\tcmd = buf;\n \t} else {\n-\t\tcmd = lrc->bb_per_ctx_bo->vmap.vaddr;\n+\t\tcmd = lrc->bo->vmap.vaddr + wa_bb_offset(lrc);\n \t}\n \n \t*cmd++ = MI_STORE_REGISTER_MEM | MI_SRM_USE_GGTT | MI_SRM_ADD_CS_OFFSET;\n@@ -974,13 +980,14 @@ static int xe_lrc_setup_utilization(struct xe_lrc *lrc)\n \t*cmd++ = MI_BATCH_BUFFER_END;\n \n \tif (buf) {\n-\t\txe_map_memcpy_to(gt_to_xe(lrc->gt), &lrc->bb_per_ctx_bo->vmap, 0,\n-\t\t\t\t buf, (cmd - buf) * sizeof(*cmd));\n+\t\txe_map_memcpy_to(gt_to_xe(lrc->gt), &lrc->bo->vmap,\n+\t\t\t\t wa_bb_offset(lrc), buf,\n+\t\t\t\t (cmd - buf) * sizeof(*cmd));\n \t\tkfree(buf);\n \t}\n \n-\txe_lrc_write_ctx_reg(lrc, CTX_BB_PER_CTX_PTR,\n-\t\t\t     xe_bo_ggtt_addr(lrc->bb_per_ctx_bo) | 1);\n+\txe_lrc_write_ctx_reg(lrc, CTX_BB_PER_CTX_PTR, xe_bo_ggtt_addr(lrc->bo) +\n+\t\t\t     wa_bb_offset(lrc) + 1);\n \n \treturn 0;\n }\n@@ -1018,20 +1025,13 @@ static int xe_lrc_init(struct xe_lrc *lrc, struct xe_hw_engine *hwe,\n \t * FIXME: Perma-pinning LRC as we don't yet support moving GGTT address\n \t * via VM bind calls.\n \t */\n-\tlrc->bo = xe_bo_create_pin_map(xe, tile, NULL, lrc_size,\n+\tlrc->bo = xe_bo_create_pin_map(xe, tile, NULL,\n+\t\t\t\t       lrc_size + LRC_WA_BB_SIZE,\n \t\t\t\t       ttm_bo_type_kernel,\n \t\t\t\t       bo_flags);\n \tif (IS_ERR(lrc->bo))\n \t\treturn PTR_ERR(lrc->bo);\n \n-\tlrc->bb_per_ctx_bo = xe_bo_create_pin_map(xe, tile, NULL, SZ_4K,\n-\t\t\t\t\t\t  ttm_bo_type_kernel,\n-\t\t\t\t\t\t  bo_flags);\n-\tif (IS_ERR(lrc->bb_per_ctx_bo)) {\n-\t\terr = PTR_ERR(lrc->bb_per_ctx_bo);\n-\t\tgoto err_lrc_finish;\n-\t}\n-\n \tlrc->size = lrc_size;\n \tlrc->ring.size = ring_size;\n \tlrc->ring.tail = 0;\n@@ -1819,7 +1819,8 @@ struct xe_lrc_snapshot *xe_lrc_snapshot_capture(struct xe_lrc *lrc)\n \tsnapshot->seqno = xe_lrc_seqno(lrc);\n \tsnapshot->lrc_bo = xe_bo_get(lrc->bo);\n \tsnapshot->lrc_offset = xe_lrc_pphwsp_offset(lrc);\n-\tsnapshot->lrc_size = lrc->bo->size - snapshot->lrc_offset;\n+\tsnapshot->lrc_size = lrc->bo->size - snapshot->lrc_offset -\n+\t\tLRC_WA_BB_SIZE;\n \tsnapshot->lrc_snapshot = NULL;\n \tsnapshot->ctx_timestamp = lower_32_bits(xe_lrc_ctx_timestamp(lrc));\n \tsnapshot->ctx_job_timestamp = xe_lrc_ctx_job_timestamp(lrc);\ndiff --git a/drivers/gpu/drm/xe/xe_lrc_types.h b/drivers/gpu/drm/xe/xe_lrc_types.h\nindex ae24cf6f8dd9..883e550a9423 100644\n--- a/drivers/gpu/drm/xe/xe_lrc_types.h\n+++ b/drivers/gpu/drm/xe/xe_lrc_types.h\n@@ -53,9 +53,6 @@ struct xe_lrc {\n \n \t/** @ctx_timestamp: readout value of CTX_TIMESTAMP on last update */\n \tu64 ctx_timestamp;\n-\n-\t/** @bb_per_ctx_bo: buffer object for per context batch wa buffer */\n-\tstruct xe_bo *bb_per_ctx_bo;\n };\n \n struct xe_lrc_snapshot;\ndiff --git a/drivers/gpu/drm/xe/xe_migrate.c b/drivers/gpu/drm/xe/xe_migrate.c\nindex 8f8e9fdfb2a8..7acdc4c78866 100644\n--- a/drivers/gpu/drm/xe/xe_migrate.c\n+++ b/drivers/gpu/drm/xe/xe_migrate.c\n@@ -82,7 +82,7 @@ struct xe_migrate {\n  * of the instruction.  Subtracting the instruction header (1 dword) and\n  * address (2 dwords), that leaves 0x3FD dwords (0x1FE qwords) for PTE values.\n  */\n-#define MAX_PTE_PER_SDI 0x1FE\n+#define MAX_PTE_PER_SDI 0x1FEU\n \n /**\n  * xe_tile_migrate_exec_queue() - Get this tile's migrate exec queue.\n@@ -1553,15 +1553,17 @@ static u32 pte_update_cmd_size(u64 size)\n \tu64 entries = DIV_U64_ROUND_UP(size, XE_PAGE_SIZE);\n \n \tXE_WARN_ON(size > MAX_PREEMPTDISABLE_TRANSFER);\n+\n \t/*\n \t * MI_STORE_DATA_IMM command is used to update page table. Each\n-\t * instruction can update maximumly 0x1ff pte entries. To update\n-\t * n (n <= 0x1ff) pte entries, we need:\n-\t * 1 dword for the MI_STORE_DATA_IMM command header (opcode etc)\n-\t * 2 dword for the page table's physical location\n-\t * 2*n dword for value of pte to fill (each pte entry is 2 dwords)\n+\t * instruction can update maximumly MAX_PTE_PER_SDI pte entries. To\n+\t * update n (n <= MAX_PTE_PER_SDI) pte entries, we need:\n+\t *\n+\t * - 1 dword for the MI_STORE_DATA_IMM command header (opcode etc)\n+\t * - 2 dword for the page table's physical location\n+\t * - 2*n dword for value of pte to fill (each pte entry is 2 dwords)\n \t */\n-\tnum_dword = (1 + 2) * DIV_U64_ROUND_UP(entries, 0x1ff);\n+\tnum_dword = (1 + 2) * DIV_U64_ROUND_UP(entries, MAX_PTE_PER_SDI);\n \tnum_dword += entries * 2;\n \n \treturn num_dword;\n@@ -1577,7 +1579,7 @@ static void build_pt_update_batch_sram(struct xe_migrate *m,\n \n \tptes = DIV_ROUND_UP(size, XE_PAGE_SIZE);\n \twhile (ptes) {\n-\t\tu32 chunk = min(0x1ffU, ptes);\n+\t\tu32 chunk = min(MAX_PTE_PER_SDI, ptes);\n \n \t\tbb->cs[bb->len++] = MI_STORE_DATA_IMM | MI_SDI_NUM_QW(chunk);\n \t\tbb->cs[bb->len++] = pt_offset;\ndiff --git a/drivers/gpu/drm/xe/xe_wa_oob.rules b/drivers/gpu/drm/xe/xe_wa_oob.rules\nindex 9efc5accd43d..69c1d7fc695e 100644\n--- a/drivers/gpu/drm/xe/xe_wa_oob.rules\n+++ b/drivers/gpu/drm/xe/xe_wa_oob.rules\n@@ -21,7 +21,8 @@\n \t\tGRAPHICS_VERSION_RANGE(1270, 1274)\n \t\tMEDIA_VERSION(1300)\n \t\tPLATFORM(DG2)\n-14018094691\tGRAPHICS_VERSION(2004)\n+14018094691\tGRAPHICS_VERSION_RANGE(2001, 2002)\n+\t\tGRAPHICS_VERSION(2004)\n 14019882105\tGRAPHICS_VERSION(2004), GRAPHICS_STEP(A0, B0)\n 18024947630\tGRAPHICS_VERSION(2001)\n \t\tGRAPHICS_VERSION(2004)\n@@ -59,3 +60,7 @@ no_media_l3\tMEDIA_VERSION(3000)\n \t\tMEDIA_VERSION_RANGE(1301, 3000)\n 16026508708\tGRAPHICS_VERSION_RANGE(1200, 3001)\n \t\tMEDIA_VERSION_RANGE(1300, 3000)\n+\n+# SoC workaround - currently applies to all platforms with the following\n+# primary GT GMDID\n+14022085890\tGRAPHICS_VERSION(2001)",
    "stats": {
      "insertions": 310,
      "deletions": 135,
      "files": 11
    }
  }
]
[
  {
    "sha": "89a33de314945c866b155d369f224fa552af1722",
    "message": "Bluetooth: btintel_pcie: Fix potential race condition in firmware download\n\nDuring firmware download, if an error occurs, interrupts must be\ndisabled, synchronized, and re-enabled before retrying the download.\nThis change ensures proper interrupt handling to prevent race\nconditions.\n\nSigned-off-by: Chandrashekar Devegowda <chandrashekar.devegowda@intel.com>\nSigned-off-by: Kiran K <kiran.k@intel.com>\nSigned-off-by: Luiz Augusto von Dentz <luiz.von.dentz@intel.com>",
    "author": "Kiran K",
    "date": "2025-06-20T11:50:55-04:00",
    "files_changed": [
      "drivers/bluetooth/btintel_pcie.c"
    ],
    "diff": "diff --git a/drivers/bluetooth/btintel_pcie.c b/drivers/bluetooth/btintel_pcie.c\nindex 563165c5efae..e1c688dd2d45 100644\n--- a/drivers/bluetooth/btintel_pcie.c\n+++ b/drivers/bluetooth/btintel_pcie.c\n@@ -2033,6 +2033,28 @@ static void btintel_pcie_release_hdev(struct btintel_pcie_data *data)\n \tdata->hdev = NULL;\n }\n \n+static void btintel_pcie_disable_interrupts(struct btintel_pcie_data *data)\n+{\n+\tspin_lock(&data->irq_lock);\n+\tbtintel_pcie_wr_reg32(data, BTINTEL_PCIE_CSR_MSIX_FH_INT_MASK, data->fh_init_mask);\n+\tbtintel_pcie_wr_reg32(data, BTINTEL_PCIE_CSR_MSIX_HW_INT_MASK, data->hw_init_mask);\n+\tspin_unlock(&data->irq_lock);\n+}\n+\n+static void btintel_pcie_enable_interrupts(struct btintel_pcie_data *data)\n+{\n+\tspin_lock(&data->irq_lock);\n+\tbtintel_pcie_wr_reg32(data, BTINTEL_PCIE_CSR_MSIX_FH_INT_MASK, ~data->fh_init_mask);\n+\tbtintel_pcie_wr_reg32(data, BTINTEL_PCIE_CSR_MSIX_HW_INT_MASK, ~data->hw_init_mask);\n+\tspin_unlock(&data->irq_lock);\n+}\n+\n+static void btintel_pcie_synchronize_irqs(struct btintel_pcie_data *data)\n+{\n+\tfor (int i = 0; i < data->alloc_vecs; i++)\n+\t\tsynchronize_irq(data->msix_entries[i].vector);\n+}\n+\n static int btintel_pcie_setup_internal(struct hci_dev *hdev)\n {\n \tstruct btintel_pcie_data *data = hci_get_drvdata(hdev);\n@@ -2152,6 +2174,8 @@ static int btintel_pcie_setup(struct hci_dev *hdev)\n \t\tbt_dev_err(hdev, \"Firmware download retry count: %d\",\n \t\t\t   fw_dl_retry);\n \t\tbtintel_pcie_dump_debug_registers(hdev);\n+\t\tbtintel_pcie_disable_interrupts(data);\n+\t\tbtintel_pcie_synchronize_irqs(data);\n \t\terr = btintel_pcie_reset_bt(data);\n \t\tif (err) {\n \t\t\tbt_dev_err(hdev, \"Failed to do shr reset: %d\", err);\n@@ -2159,6 +2183,7 @@ static int btintel_pcie_setup(struct hci_dev *hdev)\n \t\t}\n \t\tusleep_range(10000, 12000);\n \t\tbtintel_pcie_reset_ia(data);\n+\t\tbtintel_pcie_enable_interrupts(data);\n \t\tbtintel_pcie_config_msix(data);\n \t\terr = btintel_pcie_enable_bt(data);\n \t\tif (err) {\n@@ -2291,6 +2316,12 @@ static void btintel_pcie_remove(struct pci_dev *pdev)\n \n \tdata = pci_get_drvdata(pdev);\n \n+\tbtintel_pcie_disable_interrupts(data);\n+\n+\tbtintel_pcie_synchronize_irqs(data);\n+\n+\tflush_work(&data->rx_work);\n+\n \tbtintel_pcie_reset_bt(data);\n \tfor (int i = 0; i < data->alloc_vecs; i++) {\n \t\tstruct msix_entry *msix_entry;\n@@ -2303,8 +2334,6 @@ static void btintel_pcie_remove(struct pci_dev *pdev)\n \n \tbtintel_pcie_release_hdev(data);\n \n-\tflush_work(&data->rx_work);\n-\n \tdestroy_workqueue(data->workqueue);\n \n \tbtintel_pcie_free(data);",
    "stats": {
      "insertions": 31,
      "deletions": 2,
      "files": 1
    }
  },
  {
    "sha": "a8905238c3bbe13db90065ed74682418f23830c3",
    "message": "HID: lenovo: Add support for ThinkPad X1 Tablet Thin Keyboard Gen2\n\nAdd \"Thinkpad X1 Tablet Gen 2 Keyboard\" PID to hid-lenovo driver to fix trackpoint not working issue.\n\nSigned-off-by: Akira Inoue <niyarium@gmail.com>\nSigned-off-by: Jiri Kosina <jkosina@suse.com>",
    "author": "Akira Inoue",
    "date": "2025-06-20T09:10:14+02:00",
    "files_changed": [
      "drivers/hid/hid-ids.h",
      "drivers/hid/hid-lenovo.c",
      "drivers/hid/hid-multitouch.c"
    ],
    "diff": "diff --git a/drivers/hid/hid-ids.h b/drivers/hid/hid-ids.h\nindex 2d3769405ec3..c6468568aea1 100644\n--- a/drivers/hid/hid-ids.h\n+++ b/drivers/hid/hid-ids.h\n@@ -821,6 +821,7 @@\n #define USB_DEVICE_ID_LENOVO_TPPRODOCK\t0x6067\n #define USB_DEVICE_ID_LENOVO_X1_COVER\t0x6085\n #define USB_DEVICE_ID_LENOVO_X1_TAB\t0x60a3\n+#define USB_DEVICE_ID_LENOVO_X1_TAB2\t0x60a4\n #define USB_DEVICE_ID_LENOVO_X1_TAB3\t0x60b5\n #define USB_DEVICE_ID_LENOVO_X12_TAB\t0x60fe\n #define USB_DEVICE_ID_LENOVO_X12_TAB2\t0x61ae\ndiff --git a/drivers/hid/hid-lenovo.c b/drivers/hid/hid-lenovo.c\nindex a3c23a72316a..b3121fa7a72d 100644\n--- a/drivers/hid/hid-lenovo.c\n+++ b/drivers/hid/hid-lenovo.c\n@@ -492,6 +492,7 @@ static int lenovo_input_mapping(struct hid_device *hdev,\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB:\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\treturn lenovo_input_mapping_x1_tab_kbd(hdev, hi, field, usage, bit, max);\n \tdefault:\n@@ -608,6 +609,7 @@ static ssize_t attr_fn_lock_store(struct device *dev,\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_TP10UBKBD:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\tret = lenovo_led_set_tp10ubkbd(hdev, TP10UBKBD_FN_LOCK_LED, value);\n \t\tif (ret)\n@@ -864,6 +866,7 @@ static int lenovo_event(struct hid_device *hdev, struct hid_field *field,\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_TP10UBKBD:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\treturn lenovo_event_tp10ubkbd(hdev, field, usage, value);\n \tdefault:\n@@ -1147,6 +1150,7 @@ static int lenovo_led_brightness_set(struct led_classdev *led_cdev,\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_TP10UBKBD:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\tret = lenovo_led_set_tp10ubkbd(hdev, tp10ubkbd_led[led_nr], value);\n \t\tbreak;\n@@ -1387,6 +1391,7 @@ static int lenovo_probe(struct hid_device *hdev,\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_TP10UBKBD:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\tret = lenovo_probe_tp10ubkbd(hdev);\n \t\tbreak;\n@@ -1476,6 +1481,7 @@ static void lenovo_remove(struct hid_device *hdev)\n \tcase USB_DEVICE_ID_LENOVO_X12_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_TP10UBKBD:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB:\n+\tcase USB_DEVICE_ID_LENOVO_X1_TAB2:\n \tcase USB_DEVICE_ID_LENOVO_X1_TAB3:\n \t\tlenovo_remove_tp10ubkbd(hdev);\n \t\tbreak;\n@@ -1526,6 +1532,8 @@ static const struct hid_device_id lenovo_devices[] = {\n \t */\n \t{ HID_DEVICE(BUS_USB, HID_GROUP_GENERIC,\n \t\t     USB_VENDOR_ID_LENOVO, USB_DEVICE_ID_LENOVO_X1_TAB) },\n+\t{ HID_DEVICE(BUS_USB, HID_GROUP_GENERIC,\n+\t\t     USB_VENDOR_ID_LENOVO, USB_DEVICE_ID_LENOVO_X1_TAB2) },\n \t{ HID_DEVICE(BUS_USB, HID_GROUP_GENERIC,\n \t\t     USB_VENDOR_ID_LENOVO, USB_DEVICE_ID_LENOVO_X1_TAB3) },\n \t{ HID_DEVICE(BUS_USB, HID_GROUP_GENERIC,\ndiff --git a/drivers/hid/hid-multitouch.c b/drivers/hid/hid-multitouch.c\nindex ded0fef7d8c7..24aa6e7e6fdd 100644\n--- a/drivers/hid/hid-multitouch.c\n+++ b/drivers/hid/hid-multitouch.c\n@@ -2132,12 +2132,18 @@ static const struct hid_device_id mt_devices[] = {\n \t\tHID_DEVICE(BUS_I2C, HID_GROUP_GENERIC,\n \t\t\tUSB_VENDOR_ID_LG, I2C_DEVICE_ID_LG_7010) },\n \n-\t/* Lenovo X1 TAB Gen 2 */\n+\t/* Lenovo X1 TAB Gen 1 */\n \t{ .driver_data = MT_CLS_WIN_8_FORCE_MULTI_INPUT,\n \t\tHID_DEVICE(BUS_USB, HID_GROUP_MULTITOUCH_WIN_8,\n \t\t\t   USB_VENDOR_ID_LENOVO,\n \t\t\t   USB_DEVICE_ID_LENOVO_X1_TAB) },\n \n+\t/* Lenovo X1 TAB Gen 2 */\n+\t{ .driver_data = MT_CLS_WIN_8_FORCE_MULTI_INPUT,\n+\t\tHID_DEVICE(BUS_USB, HID_GROUP_MULTITOUCH_WIN_8,\n+\t\t\t   USB_VENDOR_ID_LENOVO,\n+\t\t\t   USB_DEVICE_ID_LENOVO_X1_TAB2) },\n+\n \t/* Lenovo X1 TAB Gen 3 */\n \t{ .driver_data = MT_CLS_WIN_8_FORCE_MULTI_INPUT,\n \t\tHID_DEVICE(BUS_USB, HID_GROUP_MULTITOUCH_WIN_8,",
    "stats": {
      "insertions": 16,
      "deletions": 1,
      "files": 3
    }
  },
  {
    "sha": "255da9b8d761c20dbdca3ff2c96635d50a9f1fb8",
    "message": "Merge tag 'io_uring-6.16-20250619' of git://git.kernel.dk/linux\n\nPull io_uring fixes from Jens Axboe:\n\n - Two fixes for error injection failures. One fixes a task leak issue\n   introduced in this merge window, the other an older issue with\n   handling allocation of a mapped buffer.\n\n - Fix for a syzbot issue that triggers a kmalloc warning on attempting\n   an allocation that's too large\n\n - Fix for an error injection failure causing a double put of a task,\n   introduced in this merge window\n\n* tag 'io_uring-6.16-20250619' of git://git.kernel.dk/linux:\n  io_uring: fix potential page leak in io_sqe_buffer_register()\n  io_uring/sqpoll: don't put task_struct on tctx setup failure\n  io_uring: remove duplicate io_uring_alloc_task_context() definition\n  io_uring: fix task leak issue in io_wq_create()\n  io_uring/rsrc: validate buffer count with offset for cloning",
    "author": "Linus Torvalds",
    "date": "2025-06-19T23:25:28-07:00",
    "files_changed": [
      "io_uring/io-wq.c",
      "io_uring/io_uring.h",
      "io_uring/rsrc.c",
      "io_uring/sqpoll.c"
    ],
    "diff": "diff --git a/io_uring/io-wq.c b/io_uring/io-wq.c\nindex cd1fcb115739..be91edf34f01 100644\n--- a/io_uring/io-wq.c\n+++ b/io_uring/io-wq.c\n@@ -1259,8 +1259,10 @@ struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data)\n \tatomic_set(&wq->worker_refs, 1);\n \tinit_completion(&wq->worker_done);\n \tret = cpuhp_state_add_instance_nocalls(io_wq_online, &wq->cpuhp_node);\n-\tif (ret)\n+\tif (ret) {\n+\t\tput_task_struct(wq->task);\n \t\tgoto err;\n+\t}\n \n \treturn wq;\n err:\ndiff --git a/io_uring/io_uring.h b/io_uring/io_uring.h\nindex d59c12277d58..66c1ca73f55e 100644\n--- a/io_uring/io_uring.h\n+++ b/io_uring/io_uring.h\n@@ -98,8 +98,6 @@ struct llist_node *io_handle_tw_list(struct llist_node *node, unsigned int *coun\n struct llist_node *tctx_task_work_run(struct io_uring_task *tctx, unsigned int max_entries, unsigned int *count);\n void tctx_task_work(struct callback_head *cb);\n __cold void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd);\n-int io_uring_alloc_task_context(struct task_struct *task,\n-\t\t\t\tstruct io_ring_ctx *ctx);\n \n int io_ring_add_registered_file(struct io_uring_task *tctx, struct file *file,\n \t\t\t\t     int start, int end);\ndiff --git a/io_uring/rsrc.c b/io_uring/rsrc.c\nindex c592ceace97d..d724602697e7 100644\n--- a/io_uring/rsrc.c\n+++ b/io_uring/rsrc.c\n@@ -809,10 +809,8 @@ static struct io_rsrc_node *io_sqe_buffer_register(struct io_ring_ctx *ctx,\n \n \timu->nr_bvecs = nr_pages;\n \tret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\n-\tif (ret) {\n-\t\tunpin_user_pages(pages, nr_pages);\n+\tif (ret)\n \t\tgoto done;\n-\t}\n \n \tsize = iov->iov_len;\n \t/* store original address for later verification */\n@@ -842,6 +840,8 @@ static struct io_rsrc_node *io_sqe_buffer_register(struct io_ring_ctx *ctx,\n \tif (ret) {\n \t\tif (imu)\n \t\t\tio_free_imu(ctx, imu);\n+\t\tif (pages)\n+\t\t\tunpin_user_pages(pages, nr_pages);\n \t\tio_cache_free(&ctx->node_cache, node);\n \t\tnode = ERR_PTR(ret);\n \t}\n@@ -1177,6 +1177,8 @@ static int io_clone_buffers(struct io_ring_ctx *ctx, struct io_ring_ctx *src_ctx\n \t\treturn -EINVAL;\n \tif (check_add_overflow(arg->nr, arg->dst_off, &nbufs))\n \t\treturn -EOVERFLOW;\n+\tif (nbufs > IORING_MAX_REG_BUFFERS)\n+\t\treturn -EINVAL;\n \n \tret = io_rsrc_data_alloc(&data, max(nbufs, ctx->buf_table.nr));\n \tif (ret)\ndiff --git a/io_uring/sqpoll.c b/io_uring/sqpoll.c\nindex 268d2fbe6160..a3f11349ce06 100644\n--- a/io_uring/sqpoll.c\n+++ b/io_uring/sqpoll.c\n@@ -16,6 +16,7 @@\n #include <uapi/linux/io_uring.h>\n \n #include \"io_uring.h\"\n+#include \"tctx.h\"\n #include \"napi.h\"\n #include \"sqpoll.h\"\n \n@@ -419,7 +420,6 @@ void io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n __cold int io_sq_offload_create(struct io_ring_ctx *ctx,\n \t\t\t\tstruct io_uring_params *p)\n {\n-\tstruct task_struct *task_to_put = NULL;\n \tint ret;\n \n \t/* Retain compatibility with failing for an invalid attach attempt */\n@@ -498,7 +498,7 @@ __cold int io_sq_offload_create(struct io_ring_ctx *ctx,\n \t\trcu_assign_pointer(sqd->thread, tsk);\n \t\tmutex_unlock(&sqd->lock);\n \n-\t\ttask_to_put = get_task_struct(tsk);\n+\t\tget_task_struct(tsk);\n \t\tret = io_uring_alloc_task_context(tsk, ctx);\n \t\twake_up_new_task(tsk);\n \t\tif (ret)\n@@ -513,8 +513,6 @@ __cold int io_sq_offload_create(struct io_ring_ctx *ctx,\n \tcomplete(&ctx->sq_data->exited);\n err:\n \tio_sq_thread_finish(ctx);\n-\tif (task_to_put)\n-\t\tput_task_struct(task_to_put);\n \treturn ret;\n }\n ",
    "stats": {
      "insertions": 10,
      "deletions": 10,
      "files": 4
    }
  },
  {
    "sha": "5f2b6c5f6b692c696a232d12c43b8e41c0d393b9",
    "message": "Merge tag 'drm-fixes-2025-06-20' of https://gitlab.freedesktop.org/drm/kernel\n\nPull drm fixes from Dave Airlie:\n \"Bit of an uptick in fixes for rc3, msm and amdgpu leading the way,\n  with i915/xe/nouveau with a few each and then some scattered misc\n  bits, nothing looks too crazy:\n\n  msm:\n   - Display:\n      - Fixed DP output on SDM845\n      - Fixed 10nm DSI PLL init\n   - GPU:\n      - SUBMIT ioctl error path leak fixes\n      - drm half of stall-on-fault fixes\n      - a7xx: Missing CP_RESET_CONTEXT_STATE\n      - Skip GPU component bind if GPU is not in the device table\n\n  i915:\n   - Fix MIPI vtotal programming off by one on Broxton\n   - Fix PMU code for GCOV and AutoFDO enabled build\n\n  xe:\n   - A workaround update\n   - Fix memset on iomem\n   - Fix early wedge on GuC Load failure\n\n  amdgpu:\n   - DP tunneling fix\n   - LTTPR fix\n   - DSC fix\n   - DML2.x ABGR16161616 fix\n   - RMCM fix\n   - Backlight fixes\n   - GFX11 kicker support\n   - SDMA reset fixes\n   - VCN 5.0.1 fix\n   - Reset fix\n   - Misc small fixes\n\n  amdkfd:\n   - SDMA reset fix\n   - Fix race in GWS scheduling\n\n  nouveau:\n   - update docs reference\n   - fix backlight name buffer size\n   - fix UAF in r535 gsp rpc msg\n   - fix undefined shift\n\n  mgag200:\n   - drop export header\n\n  ast:\n   - drop export header\n\n  malidp:\n   - drop informational error\n\n  ssd130x:\n   - fix clear columns\n\n  etnaviv:\n   - scheduler locking fix\n\n  v3d:\n   - null pointer crash fix\"\n\n* tag 'drm-fixes-2025-06-20' of https://gitlab.freedesktop.org/drm/kernel: (50 commits)\n  drm/xe: Fix early wedge on GuC load failure\n  drm/xe: Fix memset on iomem\n  drm/xe/bmg: Update Wa_16023588340\n  drm/amdgpu/sdma5.2: init engine reset mutex\n  drm/amdkfd: Fix race in GWS queue scheduling\n  drm/amdgpu/sdma5: init engine reset mutex\n  drm/amdgpu: switch job hw_fence to amdgpu_fence\n  drm/amdgpu: Fix SDMA UTC_L1 handling during start/stop sequences\n  drm/amdgpu: Release reset locks during failures\n  drm/amd/display: Check dce_hwseq before dereferencing it\n  drm/amdgpu: VCN v5_0_1 to prevent FW checking RB during DPG pause\n  drm/amdgpu: Use logical instance ID for SDMA v4_4_2 queue operations\n  drm/amdgpu: Fix SDMA engine reset with logical instance ID\n  drm/amdgpu: add kicker fws loading for gfx11/smu13/psp13\n  drm/amdgpu: Add kicker device detection\n  drm/amd/display: Export full brightness range to userspace\n  drm/amd/display: Only read ACPI backlight caps once\n  drm/amd/display: Fix RMCM programming seq errors\n  drm/amd/display: Fix mpv playback corruption on weston\n  drm/amd/display: Add more checks for DSC / HUBP ONO guarantees\n  ...",
    "author": "Linus Torvalds",
    "date": "2025-06-19T23:18:59-07:00",
    "files_changed": [
      "drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_device.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_job.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_job.h",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_psp.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.c",
      "drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.h",
      "drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c",
      "drivers/gpu/drm/amd/amdgpu/imu_v11_0.c",
      "drivers/gpu/drm/amd/amdgpu/psp_v13_0.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c",
      "drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c",
      "drivers/gpu/drm/amd/amdgpu/vcn_v5_0_1.c",
      "drivers/gpu/drm/amd/amdkfd/kfd_packet_manager_v9.c",
      "drivers/gpu/drm/amd/amdkfd/kfd_topology.c",
      "drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c",
      "drivers/gpu/drm/amd/display/dc/core/dc.c",
      "drivers/gpu/drm/amd/display/dc/dc.h",
      "drivers/gpu/drm/amd/display/dc/dc_dp_types.h",
      "drivers/gpu/drm/amd/display/dc/dml2/dml21/dml21_translation_helper.c",
      "drivers/gpu/drm/amd/display/dc/dml2/dml21/src/dml2_core/dml2_core_dcn4_calcs.c",
      "drivers/gpu/drm/amd/display/dc/dml2/dml2_translation_helper.c",
      "drivers/gpu/drm/amd/display/dc/hwss/dce110/dce110_hwseq.c",
      "drivers/gpu/drm/amd/display/dc/hwss/dcn35/dcn35_hwseq.c",
      "drivers/gpu/drm/amd/display/dc/link/protocols/link_dp_capability.c",
      "drivers/gpu/drm/amd/display/dc/resource/dcn31/dcn31_resource.c",
      "drivers/gpu/drm/amd/display/dc/resource/dcn314/dcn314_resource.c",
      "drivers/gpu/drm/amd/display/dc/resource/dcn35/dcn35_resource.c",
      "drivers/gpu/drm/amd/display/dc/resource/dcn351/dcn351_resource.c",
      "drivers/gpu/drm/amd/display/dc/resource/dcn36/dcn36_resource.c",
      "drivers/gpu/drm/amd/pm/swsmu/smu13/smu_v13_0.c",
      "drivers/gpu/drm/arm/malidp_planes.c",
      "drivers/gpu/drm/ast/ast_mode.c",
      "drivers/gpu/drm/etnaviv/etnaviv_sched.c",
      "drivers/gpu/drm/i915/display/vlv_dsi.c",
      "drivers/gpu/drm/i915/i915_pmu.c",
      "drivers/gpu/drm/mgag200/mgag200_ddc.c",
      "drivers/gpu/drm/msm/adreno/a2xx_gpummu.c",
      "drivers/gpu/drm/msm/adreno/a5xx_gpu.c",
      "drivers/gpu/drm/msm/adreno/a6xx_gpu.c",
      "drivers/gpu/drm/msm/adreno/adreno_device.c",
      "drivers/gpu/drm/msm/adreno/adreno_gpu.c",
      "drivers/gpu/drm/msm/adreno/adreno_gpu.h",
      "drivers/gpu/drm/msm/disp/dpu1/dpu_encoder_phys_vid.c",
      "drivers/gpu/drm/msm/dp/dp_display.c",
      "drivers/gpu/drm/msm/dsi/phy/dsi_phy_10nm.c",
      "drivers/gpu/drm/msm/msm_debugfs.c",
      "drivers/gpu/drm/msm/msm_drv.c",
      "drivers/gpu/drm/msm/msm_drv.h",
      "drivers/gpu/drm/msm/msm_gem_submit.c",
      "drivers/gpu/drm/msm/msm_gpu.c",
      "drivers/gpu/drm/msm/msm_gpu.h",
      "drivers/gpu/drm/msm/msm_iommu.c",
      "drivers/gpu/drm/msm/msm_mmu.h",
      "drivers/gpu/drm/nouveau/nouveau_backlight.c",
      "drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c",
      "drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/vmm.c",
      "drivers/gpu/drm/solomon/ssd130x.c",
      "drivers/gpu/drm/v3d/v3d_sched.c",
      "drivers/gpu/drm/xe/xe_gt.c",
      "drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c",
      "drivers/gpu/drm/xe/xe_guc_ct.c",
      "drivers/gpu/drm/xe/xe_guc_ct.h",
      "drivers/gpu/drm/xe/xe_guc_pc.c",
      "drivers/gpu/drm/xe/xe_guc_submit.c"
    ],
    "diff": "diff --git a/Documentation/gpu/nouveau.rst b/Documentation/gpu/nouveau.rst\nindex b8c801e0068c..cab2e81013bc 100644\n--- a/Documentation/gpu/nouveau.rst\n+++ b/Documentation/gpu/nouveau.rst\n@@ -25,7 +25,7 @@ providing a consistent API to upper layers of the driver stack.\n GSP Support\n ------------------------\n \n-.. kernel-doc:: drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c\n+.. kernel-doc:: drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c\n    :doc: GSP message queue element\n \n .. kernel-doc:: drivers/gpu/drm/nouveau/include/nvkm/subdev/gsp.h\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c\nindex 8e626f50b362..f81608330a3d 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c\n@@ -1902,7 +1902,7 @@ static void amdgpu_ib_preempt_mark_partial_job(struct amdgpu_ring *ring)\n \t\t\tcontinue;\n \t\t}\n \t\tjob = to_amdgpu_job(s_job);\n-\t\tif (preempted && (&job->hw_fence) == fence)\n+\t\tif (preempted && (&job->hw_fence.base) == fence)\n \t\t\t/* mark the job as preempted */\n \t\t\tjob->preemption_status |= AMDGPU_IB_PREEMPTED;\n \t}\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c\nindex e1bab6a96cb6..78f8755996f0 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c\n@@ -6019,16 +6019,12 @@ static int amdgpu_device_health_check(struct list_head *device_list_handle)\n \treturn ret;\n }\n \n-static int amdgpu_device_halt_activities(struct amdgpu_device *adev,\n-\t\t\t      struct amdgpu_job *job,\n-\t\t\t      struct amdgpu_reset_context *reset_context,\n-\t\t\t      struct list_head *device_list,\n-\t\t\t      struct amdgpu_hive_info *hive,\n-\t\t\t      bool need_emergency_restart)\n+static int amdgpu_device_recovery_prepare(struct amdgpu_device *adev,\n+\t\t\t\t\t  struct list_head *device_list,\n+\t\t\t\t\t  struct amdgpu_hive_info *hive)\n {\n-\tstruct list_head *device_list_handle =  NULL;\n \tstruct amdgpu_device *tmp_adev = NULL;\n-\tint i, r = 0;\n+\tint r;\n \n \t/*\n \t * Build list of devices to reset.\n@@ -6045,26 +6041,54 @@ static int amdgpu_device_halt_activities(struct amdgpu_device *adev,\n \t\t}\n \t\tif (!list_is_first(&adev->reset_list, device_list))\n \t\t\tlist_rotate_to_front(&adev->reset_list, device_list);\n-\t\tdevice_list_handle = device_list;\n \t} else {\n \t\tlist_add_tail(&adev->reset_list, device_list);\n-\t\tdevice_list_handle = device_list;\n \t}\n \n \tif (!amdgpu_sriov_vf(adev) && (!adev->pcie_reset_ctx.occurs_dpc)) {\n-\t\tr = amdgpu_device_health_check(device_list_handle);\n+\t\tr = amdgpu_device_health_check(device_list);\n \t\tif (r)\n \t\t\treturn r;\n \t}\n \n-\t/* We need to lock reset domain only once both for XGMI and single device */\n-\ttmp_adev = list_first_entry(device_list_handle, struct amdgpu_device,\n-\t\t\t\t    reset_list);\n+\treturn 0;\n+}\n+\n+static void amdgpu_device_recovery_get_reset_lock(struct amdgpu_device *adev,\n+\t\t\t\t\t\t  struct list_head *device_list)\n+{\n+\tstruct amdgpu_device *tmp_adev = NULL;\n+\n+\tif (list_empty(device_list))\n+\t\treturn;\n+\ttmp_adev =\n+\t\tlist_first_entry(device_list, struct amdgpu_device, reset_list);\n \tamdgpu_device_lock_reset_domain(tmp_adev->reset_domain);\n+}\n \n-\t/* block all schedulers and reset given job's ring */\n-\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n+static void amdgpu_device_recovery_put_reset_lock(struct amdgpu_device *adev,\n+\t\t\t\t\t\t  struct list_head *device_list)\n+{\n+\tstruct amdgpu_device *tmp_adev = NULL;\n \n+\tif (list_empty(device_list))\n+\t\treturn;\n+\ttmp_adev =\n+\t\tlist_first_entry(device_list, struct amdgpu_device, reset_list);\n+\tamdgpu_device_unlock_reset_domain(tmp_adev->reset_domain);\n+}\n+\n+static int amdgpu_device_halt_activities(\n+\tstruct amdgpu_device *adev, struct amdgpu_job *job,\n+\tstruct amdgpu_reset_context *reset_context,\n+\tstruct list_head *device_list, struct amdgpu_hive_info *hive,\n+\tbool need_emergency_restart)\n+{\n+\tstruct amdgpu_device *tmp_adev = NULL;\n+\tint i, r = 0;\n+\n+\t/* block all schedulers and reset given job's ring */\n+\tlist_for_each_entry(tmp_adev, device_list, reset_list) {\n \t\tamdgpu_device_set_mp1_state(tmp_adev);\n \n \t\t/*\n@@ -6252,11 +6276,6 @@ static void amdgpu_device_gpu_resume(struct amdgpu_device *adev,\n \t\tamdgpu_ras_set_error_query_ready(tmp_adev, true);\n \n \t}\n-\n-\ttmp_adev = list_first_entry(device_list, struct amdgpu_device,\n-\t\t\t\t\t    reset_list);\n-\tamdgpu_device_unlock_reset_domain(tmp_adev->reset_domain);\n-\n }\n \n \n@@ -6324,10 +6343,16 @@ int amdgpu_device_gpu_recover(struct amdgpu_device *adev,\n \treset_context->hive = hive;\n \tINIT_LIST_HEAD(&device_list);\n \n+\tif (amdgpu_device_recovery_prepare(adev, &device_list, hive))\n+\t\tgoto end_reset;\n+\n+\t/* We need to lock reset domain only once both for XGMI and single device */\n+\tamdgpu_device_recovery_get_reset_lock(adev, &device_list);\n+\n \tr = amdgpu_device_halt_activities(adev, job, reset_context, &device_list,\n \t\t\t\t\t hive, need_emergency_restart);\n \tif (r)\n-\t\tgoto end_reset;\n+\t\tgoto reset_unlock;\n \n \tif (need_emergency_restart)\n \t\tgoto skip_sched_resume;\n@@ -6337,7 +6362,7 @@ int amdgpu_device_gpu_recover(struct amdgpu_device *adev,\n \t *\n \t * job->base holds a reference to parent fence\n \t */\n-\tif (job && dma_fence_is_signaled(&job->hw_fence)) {\n+\tif (job && dma_fence_is_signaled(&job->hw_fence.base)) {\n \t\tjob_signaled = true;\n \t\tdev_info(adev->dev, \"Guilty job already signaled, skipping HW reset\");\n \t\tgoto skip_hw_reset;\n@@ -6345,13 +6370,15 @@ int amdgpu_device_gpu_recover(struct amdgpu_device *adev,\n \n \tr = amdgpu_device_asic_reset(adev, &device_list, reset_context);\n \tif (r)\n-\t\tgoto end_reset;\n+\t\tgoto reset_unlock;\n skip_hw_reset:\n \tr = amdgpu_device_sched_resume(&device_list, reset_context, job_signaled);\n \tif (r)\n-\t\tgoto end_reset;\n+\t\tgoto reset_unlock;\n skip_sched_resume:\n \tamdgpu_device_gpu_resume(adev, &device_list, need_emergency_restart);\n+reset_unlock:\n+\tamdgpu_device_recovery_put_reset_lock(adev, &device_list);\n end_reset:\n \tif (hive) {\n \t\tmutex_unlock(&hive->hive_lock);\n@@ -6763,6 +6790,8 @@ pci_ers_result_t amdgpu_pci_error_detected(struct pci_dev *pdev, pci_channel_sta\n \t\tmemset(&reset_context, 0, sizeof(reset_context));\n \t\tINIT_LIST_HEAD(&device_list);\n \n+\t\tamdgpu_device_recovery_prepare(adev, &device_list, hive);\n+\t\tamdgpu_device_recovery_get_reset_lock(adev, &device_list);\n \t\tr = amdgpu_device_halt_activities(adev, NULL, &reset_context, &device_list,\n \t\t\t\t\t hive, false);\n \t\tif (hive) {\n@@ -6880,8 +6909,8 @@ pci_ers_result_t amdgpu_pci_slot_reset(struct pci_dev *pdev)\n \t\tif (hive) {\n \t\t\tlist_for_each_entry(tmp_adev, &device_list, reset_list)\n \t\t\t\tamdgpu_device_unset_mp1_state(tmp_adev);\n-\t\t\tamdgpu_device_unlock_reset_domain(adev->reset_domain);\n \t\t}\n+\t\tamdgpu_device_recovery_put_reset_lock(adev, &device_list);\n \t}\n \n \tif (hive) {\n@@ -6927,6 +6956,7 @@ void amdgpu_pci_resume(struct pci_dev *pdev)\n \n \tamdgpu_device_sched_resume(&device_list, NULL, NULL);\n \tamdgpu_device_gpu_resume(adev, &device_list, false);\n+\tamdgpu_device_recovery_put_reset_lock(adev, &device_list);\n \tadev->pcie_reset_ctx.occurs_dpc = false;\n \n \tif (hive) {\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c\nindex 8cecf25996ed..5fec808d7f54 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c\n@@ -41,22 +41,6 @@\n #include \"amdgpu_trace.h\"\n #include \"amdgpu_reset.h\"\n \n-/*\n- * Fences mark an event in the GPUs pipeline and are used\n- * for GPU/CPU synchronization.  When the fence is written,\n- * it is expected that all buffers associated with that fence\n- * are no longer in use by the associated ring on the GPU and\n- * that the relevant GPU caches have been flushed.\n- */\n-\n-struct amdgpu_fence {\n-\tstruct dma_fence base;\n-\n-\t/* RB, DMA, etc. */\n-\tstruct amdgpu_ring\t\t*ring;\n-\tktime_t\t\t\t\tstart_timestamp;\n-};\n-\n static struct kmem_cache *amdgpu_fence_slab;\n \n int amdgpu_fence_slab_init(void)\n@@ -151,12 +135,12 @@ int amdgpu_fence_emit(struct amdgpu_ring *ring, struct dma_fence **f, struct amd\n \t\tam_fence = kmem_cache_alloc(amdgpu_fence_slab, GFP_ATOMIC);\n \t\tif (am_fence == NULL)\n \t\t\treturn -ENOMEM;\n-\t\tfence = &am_fence->base;\n-\t\tam_fence->ring = ring;\n \t} else {\n \t\t/* take use of job-embedded fence */\n-\t\tfence = &job->hw_fence;\n+\t\tam_fence = &job->hw_fence;\n \t}\n+\tfence = &am_fence->base;\n+\tam_fence->ring = ring;\n \n \tseq = ++ring->fence_drv.sync_seq;\n \tif (job && job->job_run_counter) {\n@@ -718,7 +702,7 @@ void amdgpu_fence_driver_clear_job_fences(struct amdgpu_ring *ring)\n \t\t\t * it right here or we won't be able to track them in fence_drv\n \t\t\t * and they will remain unsignaled during sa_bo free.\n \t\t\t */\n-\t\t\tjob = container_of(old, struct amdgpu_job, hw_fence);\n+\t\t\tjob = container_of(old, struct amdgpu_job, hw_fence.base);\n \t\t\tif (!job->base.s_fence && !dma_fence_is_signaled(old))\n \t\t\t\tdma_fence_signal(old);\n \t\t\tRCU_INIT_POINTER(*ptr, NULL);\n@@ -780,7 +764,7 @@ static const char *amdgpu_fence_get_timeline_name(struct dma_fence *f)\n \n static const char *amdgpu_job_fence_get_timeline_name(struct dma_fence *f)\n {\n-\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence);\n+\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence.base);\n \n \treturn (const char *)to_amdgpu_ring(job->base.sched)->name;\n }\n@@ -810,7 +794,7 @@ static bool amdgpu_fence_enable_signaling(struct dma_fence *f)\n  */\n static bool amdgpu_job_fence_enable_signaling(struct dma_fence *f)\n {\n-\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence);\n+\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence.base);\n \n \tif (!timer_pending(&to_amdgpu_ring(job->base.sched)->fence_drv.fallback_timer))\n \t\tamdgpu_fence_schedule_fallback(to_amdgpu_ring(job->base.sched));\n@@ -845,7 +829,7 @@ static void amdgpu_job_fence_free(struct rcu_head *rcu)\n \tstruct dma_fence *f = container_of(rcu, struct dma_fence, rcu);\n \n \t/* free job if fence has a parent job */\n-\tkfree(container_of(f, struct amdgpu_job, hw_fence));\n+\tkfree(container_of(f, struct amdgpu_job, hw_fence.base));\n }\n \n /**\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c\nindex acb21fc8b3ce..ddb9d3269357 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c\n@@ -272,8 +272,8 @@ void amdgpu_job_free_resources(struct amdgpu_job *job)\n \t/* Check if any fences where initialized */\n \tif (job->base.s_fence && job->base.s_fence->finished.ops)\n \t\tf = &job->base.s_fence->finished;\n-\telse if (job->hw_fence.ops)\n-\t\tf = &job->hw_fence;\n+\telse if (job->hw_fence.base.ops)\n+\t\tf = &job->hw_fence.base;\n \telse\n \t\tf = NULL;\n \n@@ -290,10 +290,10 @@ static void amdgpu_job_free_cb(struct drm_sched_job *s_job)\n \tamdgpu_sync_free(&job->explicit_sync);\n \n \t/* only put the hw fence if has embedded fence */\n-\tif (!job->hw_fence.ops)\n+\tif (!job->hw_fence.base.ops)\n \t\tkfree(job);\n \telse\n-\t\tdma_fence_put(&job->hw_fence);\n+\t\tdma_fence_put(&job->hw_fence.base);\n }\n \n void amdgpu_job_set_gang_leader(struct amdgpu_job *job,\n@@ -322,10 +322,10 @@ void amdgpu_job_free(struct amdgpu_job *job)\n \tif (job->gang_submit != &job->base.s_fence->scheduled)\n \t\tdma_fence_put(job->gang_submit);\n \n-\tif (!job->hw_fence.ops)\n+\tif (!job->hw_fence.base.ops)\n \t\tkfree(job);\n \telse\n-\t\tdma_fence_put(&job->hw_fence);\n+\t\tdma_fence_put(&job->hw_fence.base);\n }\n \n struct dma_fence *amdgpu_job_submit(struct amdgpu_job *job)\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.h\nindex f2c049129661..931fed8892cc 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_job.h\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_job.h\n@@ -48,7 +48,7 @@ struct amdgpu_job {\n \tstruct drm_sched_job    base;\n \tstruct amdgpu_vm\t*vm;\n \tstruct amdgpu_sync\texplicit_sync;\n-\tstruct dma_fence\thw_fence;\n+\tstruct amdgpu_fence\thw_fence;\n \tstruct dma_fence\t*gang_submit;\n \tuint32_t\t\tpreamble_status;\n \tuint32_t                preemption_status;\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_psp.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_psp.c\nindex e6f0b035e20b..c14f63cefe67 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_psp.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_psp.c\n@@ -3522,8 +3522,12 @@ int psp_init_sos_microcode(struct psp_context *psp, const char *chip_name)\n \tuint8_t *ucode_array_start_addr;\n \tint err = 0;\n \n-\terr = amdgpu_ucode_request(adev, &adev->psp.sos_fw, AMDGPU_UCODE_REQUIRED,\n-\t\t\t\t   \"amdgpu/%s_sos.bin\", chip_name);\n+\tif (amdgpu_is_kicker_fw(adev))\n+\t\terr = amdgpu_ucode_request(adev, &adev->psp.sos_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_sos_kicker.bin\", chip_name);\n+\telse\n+\t\terr = amdgpu_ucode_request(adev, &adev->psp.sos_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_sos.bin\", chip_name);\n \tif (err)\n \t\tgoto out;\n \n@@ -3799,8 +3803,12 @@ int psp_init_ta_microcode(struct psp_context *psp, const char *chip_name)\n \tstruct amdgpu_device *adev = psp->adev;\n \tint err;\n \n-\terr = amdgpu_ucode_request(adev, &adev->psp.ta_fw, AMDGPU_UCODE_REQUIRED,\n-\t\t\t\t   \"amdgpu/%s_ta.bin\", chip_name);\n+\tif (amdgpu_is_kicker_fw(adev))\n+\t\terr = amdgpu_ucode_request(adev, &adev->psp.ta_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_ta_kicker.bin\", chip_name);\n+\telse\n+\t\terr = amdgpu_ucode_request(adev, &adev->psp.ta_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_ta.bin\", chip_name);\n \tif (err)\n \t\treturn err;\n \ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\nindex b95b47110769..e1f25218943a 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\n@@ -127,6 +127,22 @@ struct amdgpu_fence_driver {\n \tstruct dma_fence\t\t**fences;\n };\n \n+/*\n+ * Fences mark an event in the GPUs pipeline and are used\n+ * for GPU/CPU synchronization.  When the fence is written,\n+ * it is expected that all buffers associated with that fence\n+ * are no longer in use by the associated ring on the GPU and\n+ * that the relevant GPU caches have been flushed.\n+ */\n+\n+struct amdgpu_fence {\n+\tstruct dma_fence base;\n+\n+\t/* RB, DMA, etc. */\n+\tstruct amdgpu_ring\t\t*ring;\n+\tktime_t\t\t\t\tstart_timestamp;\n+};\n+\n extern const struct drm_sched_backend_ops amdgpu_sched_ops;\n \n void amdgpu_fence_driver_clear_job_fences(struct amdgpu_ring *ring);\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c\nindex 6716ac281c49..9b54a1ece447 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c\n@@ -540,8 +540,10 @@ static int amdgpu_sdma_soft_reset(struct amdgpu_device *adev, u32 instance_id)\n \tcase IP_VERSION(4, 4, 2):\n \tcase IP_VERSION(4, 4, 4):\n \tcase IP_VERSION(4, 4, 5):\n-\t\t/* For SDMA 4.x, use the existing DPM interface for backward compatibility */\n-\t\tr = amdgpu_dpm_reset_sdma(adev, 1 << instance_id);\n+\t\t/* For SDMA 4.x, use the existing DPM interface for backward compatibility,\n+\t\t * we need to convert the logical instance ID to physical instance ID before reset.\n+\t\t */\n+\t\tr = amdgpu_dpm_reset_sdma(adev, 1 << GET_INST(SDMA0, instance_id));\n \t\tbreak;\n \tcase IP_VERSION(5, 0, 0):\n \tcase IP_VERSION(5, 0, 1):\n@@ -568,7 +570,7 @@ static int amdgpu_sdma_soft_reset(struct amdgpu_device *adev, u32 instance_id)\n /**\n  * amdgpu_sdma_reset_engine - Reset a specific SDMA engine\n  * @adev: Pointer to the AMDGPU device\n- * @instance_id: ID of the SDMA engine instance to reset\n+ * @instance_id: Logical ID of the SDMA engine instance to reset\n  *\n  * Returns: 0 on success, or a negative error code on failure.\n  */\n@@ -601,7 +603,7 @@ int amdgpu_sdma_reset_engine(struct amdgpu_device *adev, uint32_t instance_id)\n \t/* Perform the SDMA reset for the specified instance */\n \tret = amdgpu_sdma_soft_reset(adev, instance_id);\n \tif (ret) {\n-\t\tdev_err(adev->dev, \"Failed to reset SDMA instance %u\\n\", instance_id);\n+\t\tdev_err(adev->dev, \"Failed to reset SDMA logical instance %u\\n\", instance_id);\n \t\tgoto exit;\n \t}\n \ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.c\nindex 2505c46a9c3d..eaddc441c51a 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.c\n@@ -30,6 +30,10 @@\n \n #define AMDGPU_UCODE_NAME_MAX\t\t(128)\n \n+static const struct kicker_device kicker_device_list[] = {\n+\t{0x744B, 0x00},\n+};\n+\n static void amdgpu_ucode_print_common_hdr(const struct common_firmware_header *hdr)\n {\n \tDRM_DEBUG(\"size_bytes: %u\\n\", le32_to_cpu(hdr->size_bytes));\n@@ -1387,6 +1391,19 @@ static const char *amdgpu_ucode_legacy_naming(struct amdgpu_device *adev, int bl\n \treturn NULL;\n }\n \n+bool amdgpu_is_kicker_fw(struct amdgpu_device *adev)\n+{\n+\tint i;\n+\n+\tfor (i = 0; i < ARRAY_SIZE(kicker_device_list); i++) {\n+\t\tif (adev->pdev->device == kicker_device_list[i].device &&\n+\t\t\tadev->pdev->revision == kicker_device_list[i].revision)\n+\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n void amdgpu_ucode_ip_version_decode(struct amdgpu_device *adev, int block_type, char *ucode_prefix, int len)\n {\n \tint maj, min, rev;\ndiff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.h\nindex 9e89c3487be5..6349aad6da35 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.h\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ucode.h\n@@ -605,6 +605,11 @@ struct amdgpu_firmware {\n \tuint32_t pldm_version;\n };\n \n+struct kicker_device{\n+\tunsigned short device;\n+\tu8 revision;\n+};\n+\n void amdgpu_ucode_print_mc_hdr(const struct common_firmware_header *hdr);\n void amdgpu_ucode_print_smc_hdr(const struct common_firmware_header *hdr);\n void amdgpu_ucode_print_imu_hdr(const struct common_firmware_header *hdr);\n@@ -632,5 +637,6 @@ amdgpu_ucode_get_load_type(struct amdgpu_device *adev, int load_type);\n const char *amdgpu_ucode_name(enum AMDGPU_UCODE_ID ucode_id);\n \n void amdgpu_ucode_ip_version_decode(struct amdgpu_device *adev, int block_type, char *ucode_prefix, int len);\n+bool amdgpu_is_kicker_fw(struct amdgpu_device *adev);\n \n #endif\ndiff --git a/drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c b/drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\nindex afd6d59164bf..ec9b84f92d46 100644\n--- a/drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\n@@ -85,6 +85,7 @@ MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_pfp.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_me.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_mec.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_rlc.bin\");\n+MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_rlc_kicker.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_rlc_1.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_toc.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_1_pfp.bin\");\n@@ -759,6 +760,10 @@ static int gfx_v11_0_init_microcode(struct amdgpu_device *adev)\n \t\t\terr = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,\n \t\t\t\t\t\t   AMDGPU_UCODE_REQUIRED,\n \t\t\t\t\t\t   \"amdgpu/gc_11_0_0_rlc_1.bin\");\n+\t\telse if (amdgpu_is_kicker_fw(adev))\n+\t\t\terr = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,\n+\t\t\t\t\t\t   AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t\t   \"amdgpu/%s_rlc_kicker.bin\", ucode_prefix);\n \t\telse\n \t\t\terr = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw,\n \t\t\t\t\t\t   AMDGPU_UCODE_REQUIRED,\ndiff --git a/drivers/gpu/drm/amd/amdgpu/imu_v11_0.c b/drivers/gpu/drm/amd/amdgpu/imu_v11_0.c\nindex cfa91d709d49..cc626036ed9c 100644\n--- a/drivers/gpu/drm/amd/amdgpu/imu_v11_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/imu_v11_0.c\n@@ -32,6 +32,7 @@\n #include \"gc/gc_11_0_0_sh_mask.h\"\n \n MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_imu.bin\");\n+MODULE_FIRMWARE(\"amdgpu/gc_11_0_0_imu_kicker.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_1_imu.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_2_imu.bin\");\n MODULE_FIRMWARE(\"amdgpu/gc_11_0_3_imu.bin\");\n@@ -51,8 +52,12 @@ static int imu_v11_0_init_microcode(struct amdgpu_device *adev)\n \tDRM_DEBUG(\"\\n\");\n \n \tamdgpu_ucode_ip_version_decode(adev, GC_HWIP, ucode_prefix, sizeof(ucode_prefix));\n-\terr = amdgpu_ucode_request(adev, &adev->gfx.imu_fw, AMDGPU_UCODE_REQUIRED,\n-\t\t\t\t   \"amdgpu/%s_imu.bin\", ucode_prefix);\n+\tif (amdgpu_is_kicker_fw(adev))\n+\t\terr = amdgpu_ucode_request(adev, &adev->gfx.imu_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_imu_kicker.bin\", ucode_prefix);\n+\telse\n+\t\terr = amdgpu_ucode_request(adev, &adev->gfx.imu_fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_imu.bin\", ucode_prefix);\n \tif (err)\n \t\tgoto out;\n \ndiff --git a/drivers/gpu/drm/amd/amdgpu/psp_v13_0.c b/drivers/gpu/drm/amd/amdgpu/psp_v13_0.c\nindex df612fd9cc50..ead616c11705 100644\n--- a/drivers/gpu/drm/amd/amdgpu/psp_v13_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/psp_v13_0.c\n@@ -42,7 +42,9 @@ MODULE_FIRMWARE(\"amdgpu/psp_13_0_5_ta.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_8_toc.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_8_ta.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_0_sos.bin\");\n+MODULE_FIRMWARE(\"amdgpu/psp_13_0_0_sos_kicker.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_0_ta.bin\");\n+MODULE_FIRMWARE(\"amdgpu/psp_13_0_0_ta_kicker.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_7_sos.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_7_ta.bin\");\n MODULE_FIRMWARE(\"amdgpu/psp_13_0_10_sos.bin\");\ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c b/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\nindex 9c169112a5e7..cef68df4c663 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c\n@@ -490,7 +490,7 @@ static void sdma_v4_4_2_inst_gfx_stop(struct amdgpu_device *adev,\n {\n \tstruct amdgpu_ring *sdma[AMDGPU_MAX_SDMA_INSTANCES];\n \tu32 doorbell_offset, doorbell;\n-\tu32 rb_cntl, ib_cntl;\n+\tu32 rb_cntl, ib_cntl, sdma_cntl;\n \tint i;\n \n \tfor_each_inst(i, inst_mask) {\n@@ -502,6 +502,9 @@ static void sdma_v4_4_2_inst_gfx_stop(struct amdgpu_device *adev,\n \t\tib_cntl = RREG32_SDMA(i, regSDMA_GFX_IB_CNTL);\n \t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_GFX_IB_CNTL, IB_ENABLE, 0);\n \t\tWREG32_SDMA(i, regSDMA_GFX_IB_CNTL, ib_cntl);\n+\t\tsdma_cntl = RREG32_SDMA(i, regSDMA_CNTL);\n+\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA_CNTL, UTC_L1_ENABLE, 0);\n+\t\tWREG32_SDMA(i, regSDMA_CNTL, sdma_cntl);\n \n \t\tif (sdma[i]->use_doorbell) {\n \t\t\tdoorbell = RREG32_SDMA(i, regSDMA_GFX_DOORBELL);\n@@ -995,6 +998,7 @@ static int sdma_v4_4_2_inst_start(struct amdgpu_device *adev,\n \t\t/* set utc l1 enable flag always to 1 */\n \t\ttemp = RREG32_SDMA(i, regSDMA_CNTL);\n \t\ttemp = REG_SET_FIELD(temp, SDMA_CNTL, UTC_L1_ENABLE, 1);\n+\t\tWREG32_SDMA(i, regSDMA_CNTL, temp);\n \n \t\tif (amdgpu_ip_version(adev, SDMA0_HWIP, 0) < IP_VERSION(4, 4, 5)) {\n \t\t\t/* enable context empty interrupt during initialization */\n@@ -1670,7 +1674,7 @@ static bool sdma_v4_4_2_page_ring_is_guilty(struct amdgpu_ring *ring)\n static int sdma_v4_4_2_reset_queue(struct amdgpu_ring *ring, unsigned int vmid)\n {\n \tstruct amdgpu_device *adev = ring->adev;\n-\tu32 id = GET_INST(SDMA0, ring->me);\n+\tu32 id = ring->me;\n \tint r;\n \n \tif (!(adev->sdma.supported_reset & AMDGPU_RESET_TYPE_PER_QUEUE))\n@@ -1686,7 +1690,7 @@ static int sdma_v4_4_2_reset_queue(struct amdgpu_ring *ring, unsigned int vmid)\n static int sdma_v4_4_2_stop_queue(struct amdgpu_ring *ring)\n {\n \tstruct amdgpu_device *adev = ring->adev;\n-\tu32 instance_id = GET_INST(SDMA0, ring->me);\n+\tu32 instance_id = ring->me;\n \tu32 inst_mask;\n \tuint64_t rptr;\n \ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c b/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\nindex 9505ae96fbec..1813c3ed0aa6 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c\n@@ -1399,6 +1399,7 @@ static int sdma_v5_0_sw_init(struct amdgpu_ip_block *ip_block)\n \t\treturn r;\n \n \tfor (i = 0; i < adev->sdma.num_instances; i++) {\n+\t\tmutex_init(&adev->sdma.instance[i].engine_reset_mutex);\n \t\tadev->sdma.instance[i].funcs = &sdma_v5_0_sdma_funcs;\n \t\tring = &adev->sdma.instance[i].ring;\n \t\tring->ring_obj = NULL;\ndiff --git a/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c b/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\nindex a6e612b4a892..23f97da62808 100644\n--- a/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\n+++ b/drivers/gpu/drm/amd/amdgpu/sdma_v5_2.c\n@@ -1318,6 +1318,7 @@ static int sdma_v5_2_sw_init(struct amdgpu_ip_block *ip_block)\n \t}\n \n \tfor (i = 0; i < adev->sdma.num_instances; i++) {\n+\t\tmutex_init(&adev->sdma.instance[i].engine_reset_mutex);\n \t\tadev->sdma.instance[i].funcs = &sdma_v5_2_sdma_funcs;\n \t\tring = &adev->sdma.instance[i].ring;\n \t\tring->ring_obj = NULL;\ndiff --git a/drivers/gpu/drm/amd/amdgpu/vcn_v5_0_1.c b/drivers/gpu/drm/amd/amdgpu/vcn_v5_0_1.c\nindex 338cf43c45fe..cdefd7fcb0da 100644\n--- a/drivers/gpu/drm/amd/amdgpu/vcn_v5_0_1.c\n+++ b/drivers/gpu/drm/amd/amdgpu/vcn_v5_0_1.c\n@@ -669,6 +669,9 @@ static int vcn_v5_0_1_start_dpg_mode(struct amdgpu_vcn_inst *vinst,\n \tif (indirect)\n \t\tamdgpu_vcn_psp_update_sram(adev, inst_idx, AMDGPU_UCODE_ID_VCN0_RAM);\n \n+\t/* resetting ring, fw should not check RB ring */\n+\tfw_shared->sq.queue_mode |= FW_QUEUE_RING_RESET;\n+\n \t/* Pause dpg */\n \tvcn_v5_0_1_pause_dpg_mode(vinst, &state);\n \n@@ -681,7 +684,7 @@ static int vcn_v5_0_1_start_dpg_mode(struct amdgpu_vcn_inst *vinst,\n \ttmp = RREG32_SOC15(VCN, vcn_inst, regVCN_RB_ENABLE);\n \ttmp &= ~(VCN_RB_ENABLE__RB1_EN_MASK);\n \tWREG32_SOC15(VCN, vcn_inst, regVCN_RB_ENABLE, tmp);\n-\tfw_shared->sq.queue_mode |= FW_QUEUE_RING_RESET;\n+\n \tWREG32_SOC15(VCN, vcn_inst, regUVD_RB_RPTR, 0);\n \tWREG32_SOC15(VCN, vcn_inst, regUVD_RB_WPTR, 0);\n \n@@ -692,6 +695,7 @@ static int vcn_v5_0_1_start_dpg_mode(struct amdgpu_vcn_inst *vinst,\n \ttmp = RREG32_SOC15(VCN, vcn_inst, regVCN_RB_ENABLE);\n \ttmp |= VCN_RB_ENABLE__RB1_EN_MASK;\n \tWREG32_SOC15(VCN, vcn_inst, regVCN_RB_ENABLE, tmp);\n+\t/* resetting done, fw can check RB ring */\n \tfw_shared->sq.queue_mode &= ~(FW_QUEUE_RING_RESET | FW_QUEUE_DPG_HOLD_OFF);\n \n \tWREG32_SOC15(VCN, vcn_inst, regVCN_RB1_DB_CTRL,\ndiff --git a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager_v9.c\nindex 8fa6489b6f5d..505036968a77 100644\n--- a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager_v9.c\n+++ b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager_v9.c\n@@ -240,7 +240,7 @@ static int pm_map_queues_v9(struct packet_manager *pm, uint32_t *buffer,\n \n \tpacket->bitfields2.engine_sel =\n \t\tengine_sel__mes_map_queues__compute_vi;\n-\tpacket->bitfields2.gws_control_queue = q->gws ? 1 : 0;\n+\tpacket->bitfields2.gws_control_queue = q->properties.is_gws ? 1 : 0;\n \tpacket->bitfields2.extended_engine_sel =\n \t\textended_engine_sel__mes_map_queues__legacy_engine_sel;\n \tpacket->bitfields2.queue_type =\ndiff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c\nindex baa2374acdeb..4ec73f33535e 100644\n--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c\n+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c\n@@ -510,6 +510,10 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,\n \t\t\tdev->node_props.capability |=\n \t\t\t\t\tHSA_CAP_AQL_QUEUE_DOUBLE_MAP;\n \n+\t\tif (KFD_GC_VERSION(dev->gpu) < IP_VERSION(10, 0, 0) &&\n+\t\t\t(dev->gpu->adev->sdma.supported_reset & AMDGPU_RESET_TYPE_PER_QUEUE))\n+\t\t\t\tdev->node_props.capability2 |= HSA_CAP2_PER_SDMA_QUEUE_RESET_SUPPORTED;\n+\n \t\tsysfs_show_32bit_prop(buffer, offs, \"max_engine_clk_fcompute\",\n \t\t\tdev->node_props.max_engine_clk_fcompute);\n \n@@ -2008,8 +2012,6 @@ static void kfd_topology_set_capabilities(struct kfd_topology_device *dev)\n \t\tif (!amdgpu_sriov_vf(dev->gpu->adev))\n \t\t\tdev->node_props.capability |= HSA_CAP_PER_QUEUE_RESET_SUPPORTED;\n \n-\t\tif (dev->gpu->adev->sdma.supported_reset & AMDGPU_RESET_TYPE_PER_QUEUE)\n-\t\t\tdev->node_props.capability2 |= HSA_CAP2_PER_SDMA_QUEUE_RESET_SUPPORTED;\n \t} else {\n \t\tdev->node_props.debug_prop |= HSA_DBG_WATCH_ADDR_MASK_LO_BIT_GFX10 |\n \t\t\t\t\tHSA_DBG_WATCH_ADDR_MASK_HI_BIT;\ndiff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\nindex d3100f641ac6..bc4cd11bfc79 100644\n--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\n+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c\n@@ -4718,9 +4718,23 @@ static int get_brightness_range(const struct amdgpu_dm_backlight_caps *caps,\n \treturn 1;\n }\n \n+/* Rescale from [min..max] to [0..AMDGPU_MAX_BL_LEVEL] */\n+static inline u32 scale_input_to_fw(int min, int max, u64 input)\n+{\n+\treturn DIV_ROUND_CLOSEST_ULL(input * AMDGPU_MAX_BL_LEVEL, max - min);\n+}\n+\n+/* Rescale from [0..AMDGPU_MAX_BL_LEVEL] to [min..max] */\n+static inline u32 scale_fw_to_input(int min, int max, u64 input)\n+{\n+\treturn min + DIV_ROUND_CLOSEST_ULL(input * (max - min), AMDGPU_MAX_BL_LEVEL);\n+}\n+\n static void convert_custom_brightness(const struct amdgpu_dm_backlight_caps *caps,\n-\t\t\t\t      uint32_t *brightness)\n+\t\t\t\t      unsigned int min, unsigned int max,\n+\t\t\t\t      uint32_t *user_brightness)\n {\n+\tu32 brightness = scale_input_to_fw(min, max, *user_brightness);\n \tu8 prev_signal = 0, prev_lum = 0;\n \tint i = 0;\n \n@@ -4731,7 +4745,7 @@ static void convert_custom_brightness(const struct amdgpu_dm_backlight_caps *cap\n \t\treturn;\n \n \t/* choose start to run less interpolation steps */\n-\tif (caps->luminance_data[caps->data_points/2].input_signal > *brightness)\n+\tif (caps->luminance_data[caps->data_points/2].input_signal > brightness)\n \t\ti = caps->data_points/2;\n \tdo {\n \t\tu8 signal = caps->luminance_data[i].input_signal;\n@@ -4742,17 +4756,18 @@ static void convert_custom_brightness(const struct amdgpu_dm_backlight_caps *cap\n \t\t * brightness < signal: interpolate between previous and current luminance numerator\n \t\t * brightness > signal: find next data point\n \t\t */\n-\t\tif (*brightness > signal) {\n+\t\tif (brightness > signal) {\n \t\t\tprev_signal = signal;\n \t\t\tprev_lum = lum;\n \t\t\ti++;\n \t\t\tcontinue;\n \t\t}\n-\t\tif (*brightness < signal)\n+\t\tif (brightness < signal)\n \t\t\tlum = prev_lum + DIV_ROUND_CLOSEST((lum - prev_lum) *\n-\t\t\t\t\t\t\t   (*brightness - prev_signal),\n+\t\t\t\t\t\t\t   (brightness - prev_signal),\n \t\t\t\t\t\t\t   signal - prev_signal);\n-\t\t*brightness = DIV_ROUND_CLOSEST(lum * *brightness, 101);\n+\t\t*user_brightness = scale_fw_to_input(min, max,\n+\t\t\t\t\t\t     DIV_ROUND_CLOSEST(lum * brightness, 101));\n \t\treturn;\n \t} while (i < caps->data_points);\n }\n@@ -4765,11 +4780,10 @@ static u32 convert_brightness_from_user(const struct amdgpu_dm_backlight_caps *c\n \tif (!get_brightness_range(caps, &min, &max))\n \t\treturn brightness;\n \n-\tconvert_custom_brightness(caps, &brightness);\n+\tconvert_custom_brightness(caps, min, max, &brightness);\n \n-\t// Rescale 0..255 to min..max\n-\treturn min + DIV_ROUND_CLOSEST((max - min) * brightness,\n-\t\t\t\t       AMDGPU_MAX_BL_LEVEL);\n+\t// Rescale 0..max to min..max\n+\treturn min + DIV_ROUND_CLOSEST_ULL((u64)(max - min) * brightness, max);\n }\n \n static u32 convert_brightness_to_user(const struct amdgpu_dm_backlight_caps *caps,\n@@ -4782,8 +4796,8 @@ static u32 convert_brightness_to_user(const struct amdgpu_dm_backlight_caps *cap\n \n \tif (brightness < min)\n \t\treturn 0;\n-\t// Rescale min..max to 0..255\n-\treturn DIV_ROUND_CLOSEST(AMDGPU_MAX_BL_LEVEL * (brightness - min),\n+\t// Rescale min..max to 0..max\n+\treturn DIV_ROUND_CLOSEST_ULL((u64)max * (brightness - min),\n \t\t\t\t max - min);\n }\n \n@@ -4908,7 +4922,7 @@ amdgpu_dm_register_backlight_device(struct amdgpu_dm_connector *aconnector)\n \tstruct drm_device *drm = aconnector->base.dev;\n \tstruct amdgpu_display_manager *dm = &drm_to_adev(drm)->dm;\n \tstruct backlight_properties props = { 0 };\n-\tstruct amdgpu_dm_backlight_caps caps = { 0 };\n+\tstruct amdgpu_dm_backlight_caps *caps;\n \tchar bl_name[16];\n \tint min, max;\n \n@@ -4922,22 +4936,21 @@ amdgpu_dm_register_backlight_device(struct amdgpu_dm_connector *aconnector)\n \t\treturn;\n \t}\n \n-\tamdgpu_acpi_get_backlight_caps(&caps);\n-\tif (caps.caps_valid && get_brightness_range(&caps, &min, &max)) {\n+\tcaps = &dm->backlight_caps[aconnector->bl_idx];\n+\tif (get_brightness_range(caps, &min, &max)) {\n \t\tif (power_supply_is_system_supplied() > 0)\n-\t\t\tprops.brightness = (max - min) * DIV_ROUND_CLOSEST(caps.ac_level, 100);\n+\t\t\tprops.brightness = (max - min) * DIV_ROUND_CLOSEST(caps->ac_level, 100);\n \t\telse\n-\t\t\tprops.brightness = (max - min) * DIV_ROUND_CLOSEST(caps.dc_level, 100);\n+\t\t\tprops.brightness = (max - min) * DIV_ROUND_CLOSEST(caps->dc_level, 100);\n \t\t/* min is zero, so max needs to be adjusted */\n \t\tprops.max_brightness = max - min;\n \t\tdrm_dbg(drm, \"Backlight caps: min: %d, max: %d, ac %d, dc %d\\n\", min, max,\n-\t\t\tcaps.ac_level, caps.dc_level);\n+\t\t\tcaps->ac_level, caps->dc_level);\n \t} else\n-\t\tprops.brightness = AMDGPU_MAX_BL_LEVEL;\n+\t\tprops.brightness = props.max_brightness = AMDGPU_MAX_BL_LEVEL;\n \n-\tif (caps.data_points && !(amdgpu_dc_debug_mask & DC_DISABLE_CUSTOM_BRIGHTNESS_CURVE))\n+\tif (caps->data_points && !(amdgpu_dc_debug_mask & DC_DISABLE_CUSTOM_BRIGHTNESS_CURVE))\n \t\tdrm_info(drm, \"Using custom brightness curve\\n\");\n-\tprops.max_brightness = AMDGPU_MAX_BL_LEVEL;\n \tprops.type = BACKLIGHT_RAW;\n \n \tsnprintf(bl_name, sizeof(bl_name), \"amdgpu_bl%d\",\ndiff --git a/drivers/gpu/drm/amd/display/dc/core/dc.c b/drivers/gpu/drm/amd/display/dc/core/dc.c\nindex 56d011a1323c..b34b5b52236d 100644\n--- a/drivers/gpu/drm/amd/display/dc/core/dc.c\n+++ b/drivers/gpu/drm/amd/display/dc/core/dc.c\n@@ -241,6 +241,7 @@ static bool create_links(\n \tDC_LOG_DC(\"BIOS object table - end\");\n \n \t/* Create a link for each usb4 dpia port */\n+\tdc->lowest_dpia_link_index = MAX_LINKS;\n \tfor (i = 0; i < dc->res_pool->usb4_dpia_count; i++) {\n \t\tstruct link_init_data link_init_params = {0};\n \t\tstruct dc_link *link;\n@@ -253,6 +254,9 @@ static bool create_links(\n \n \t\tlink = dc->link_srv->create_link(&link_init_params);\n \t\tif (link) {\n+\t\t\tif (dc->lowest_dpia_link_index > dc->link_count)\n+\t\t\t\tdc->lowest_dpia_link_index = dc->link_count;\n+\n \t\t\tdc->links[dc->link_count] = link;\n \t\t\tlink->dc = dc;\n \t\t\t++dc->link_count;\n@@ -6376,6 +6380,35 @@ unsigned int dc_get_det_buffer_size_from_state(const struct dc_state *context)\n \telse\n \t\treturn 0;\n }\n+/**\n+ ***********************************************************************************************\n+ * dc_get_host_router_index: Get index of host router from a dpia link\n+ *\n+ * This function return a host router index of the target link. If the target link is dpia link.\n+ *\n+ * @param [in] link: target link\n+ * @param [out] host_router_index: host router index of the target link\n+ *\n+ * @return: true if the host router index is found and valid.\n+ *\n+ ***********************************************************************************************\n+ */\n+bool dc_get_host_router_index(const struct dc_link *link, unsigned int *host_router_index)\n+{\n+\tstruct dc *dc = link->ctx->dc;\n+\n+\tif (link->ep_type != DISPLAY_ENDPOINT_USB4_DPIA)\n+\t\treturn false;\n+\n+\tif (link->link_index < dc->lowest_dpia_link_index)\n+\t\treturn false;\n+\n+\t*host_router_index = (link->link_index - dc->lowest_dpia_link_index) / dc->caps.num_of_dpias_per_host_router;\n+\tif (*host_router_index < dc->caps.num_of_host_routers)\n+\t\treturn true;\n+\telse\n+\t\treturn false;\n+}\n \n bool dc_is_cursor_limit_pending(struct dc *dc)\n {\ndiff --git a/drivers/gpu/drm/amd/display/dc/dc.h b/drivers/gpu/drm/amd/display/dc/dc.h\nindex 1d917be36fc4..f41073c0147e 100644\n--- a/drivers/gpu/drm/amd/display/dc/dc.h\n+++ b/drivers/gpu/drm/amd/display/dc/dc.h\n@@ -66,7 +66,8 @@ struct dmub_notification;\n #define MAX_STREAMS 6\n #define MIN_VIEWPORT_SIZE 12\n #define MAX_NUM_EDP 2\n-#define MAX_HOST_ROUTERS_NUM 2\n+#define MAX_HOST_ROUTERS_NUM 3\n+#define MAX_DPIA_PER_HOST_ROUTER 2\n \n /* Display Core Interfaces */\n struct dc_versions {\n@@ -305,6 +306,8 @@ struct dc_caps {\n \t/* Conservative limit for DCC cases which require ODM4:1 to support*/\n \tuint32_t dcc_plane_width_limit;\n \tstruct dc_scl_caps scl_caps;\n+\tuint8_t num_of_host_routers;\n+\tuint8_t num_of_dpias_per_host_router;\n };\n \n struct dc_bug_wa {\n@@ -1603,6 +1606,7 @@ struct dc {\n \n \tuint8_t link_count;\n \tstruct dc_link *links[MAX_LINKS];\n+\tuint8_t lowest_dpia_link_index;\n \tstruct link_service *link_srv;\n \n \tstruct dc_state *current_state;\n@@ -2595,6 +2599,8 @@ struct dc_power_profile dc_get_power_profile_for_dc_state(const struct dc_state\n \n unsigned int dc_get_det_buffer_size_from_state(const struct dc_state *context);\n \n+bool dc_get_host_router_index(const struct dc_link *link, unsigned int *host_router_index);\n+\n /* DSC Interfaces */\n #include \"dc_dsc.h\"\n \ndiff --git a/drivers/gpu/drm/amd/display/dc/dc_dp_types.h b/drivers/gpu/drm/amd/display/dc/dc_dp_types.h\nindex 0bad8304ccf6..d346f8ae1634 100644\n--- a/drivers/gpu/drm/amd/display/dc/dc_dp_types.h\n+++ b/drivers/gpu/drm/amd/display/dc/dc_dp_types.h\n@@ -1172,8 +1172,8 @@ struct dc_lttpr_caps {\n \tunion dp_128b_132b_supported_lttpr_link_rates supported_128b_132b_rates;\n \tunion dp_alpm_lttpr_cap alpm;\n \tuint8_t aux_rd_interval[MAX_REPEATER_CNT - 1];\n-\tuint8_t lttpr_ieee_oui[3];\n-\tuint8_t lttpr_device_id[6];\n+\tuint8_t lttpr_ieee_oui[3]; // Always read from closest LTTPR to host\n+\tuint8_t lttpr_device_id[6]; // Always read from closest LTTPR to host\n };\n \n struct dc_dongle_dfp_cap_ext {\ndiff --git a/drivers/gpu/drm/amd/display/dc/dml2/dml21/dml21_translation_helper.c b/drivers/gpu/drm/amd/display/dc/dml2/dml21/dml21_translation_helper.c\nindex d47cacfdb695..2aa6d44bb359 100644\n--- a/drivers/gpu/drm/amd/display/dc/dml2/dml21/dml21_translation_helper.c\n+++ b/drivers/gpu/drm/amd/display/dc/dml2/dml21/dml21_translation_helper.c\n@@ -788,6 +788,7 @@ static void populate_dml21_plane_config_from_plane_state(struct dml2_context *dm\n \t\tplane->pixel_format = dml2_420_10;\n \t\tbreak;\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:\n+\tcase SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616:\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616F:\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:\n \t\tplane->pixel_format = dml2_444_64;\ndiff --git a/drivers/gpu/drm/amd/display/dc/dml2/dml21/src/dml2_core/dml2_core_dcn4_calcs.c b/drivers/gpu/drm/amd/display/dc/dml2/dml21/src/dml2_core/dml2_core_dcn4_calcs.c\nindex c4dad7164d31..5b62cd19d979 100644\n--- a/drivers/gpu/drm/amd/display/dc/dml2/dml21/src/dml2_core/dml2_core_dcn4_calcs.c\n+++ b/drivers/gpu/drm/amd/display/dc/dml2/dml21/src/dml2_core/dml2_core_dcn4_calcs.c\n@@ -4685,7 +4685,10 @@ static void calculate_tdlut_setting(\n \t//the tdlut is fetched during the 2 row times of prefetch.\n \tif (p->setup_for_tdlut) {\n \t\t*p->tdlut_groups_per_2row_ub = (unsigned int)math_ceil2((double) *p->tdlut_bytes_per_frame / *p->tdlut_bytes_per_group, 1);\n-\t\t*p->tdlut_opt_time = (*p->tdlut_bytes_per_frame - p->cursor_buffer_size * 1024) / tdlut_drain_rate;\n+\t\tif (*p->tdlut_bytes_per_frame > p->cursor_buffer_size * 1024)\n+\t\t\t*p->tdlut_opt_time = (*p->tdlut_bytes_per_frame - p->cursor_buffer_size * 1024) / tdlut_drain_rate;\n+\t\telse\n+\t\t\t*p->tdlut_opt_time = 0;\n \t\t*p->tdlut_drain_time = p->cursor_buffer_size * 1024 / tdlut_drain_rate;\n \t\t*p->tdlut_bytes_to_deliver = (unsigned int) (p->cursor_buffer_size * 1024.0);\n \t}\ndiff --git a/drivers/gpu/drm/amd/display/dc/dml2/dml2_translation_helper.c b/drivers/gpu/drm/amd/display/dc/dml2/dml2_translation_helper.c\nindex 5de775fd8fce..208630754c8a 100644\n--- a/drivers/gpu/drm/amd/display/dc/dml2/dml2_translation_helper.c\n+++ b/drivers/gpu/drm/amd/display/dc/dml2/dml2_translation_helper.c\n@@ -953,6 +953,7 @@ static void populate_dml_surface_cfg_from_plane_state(enum dml_project_id dml2_p\n \t\tout->SourcePixelFormat[location] = dml_420_10;\n \t\tbreak;\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:\n+\tcase SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616:\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616F:\n \tcase SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:\n \t\tout->SourcePixelFormat[location] = dml_444_64;\ndiff --git a/drivers/gpu/drm/amd/display/dc/hwss/dce110/dce110_hwseq.c b/drivers/gpu/drm/amd/display/dc/hwss/dce110/dce110_hwseq.c\nindex e8730cc40edb..38e17b1796e1 100644\n--- a/drivers/gpu/drm/amd/display/dc/hwss/dce110/dce110_hwseq.c\n+++ b/drivers/gpu/drm/amd/display/dc/hwss/dce110/dce110_hwseq.c\n@@ -1225,7 +1225,7 @@ void dce110_blank_stream(struct pipe_ctx *pipe_ctx)\n \t\treturn;\n \n \tif (link->local_sink && link->local_sink->sink_signal == SIGNAL_TYPE_EDP) {\n-\t\tif (!link->skip_implict_edp_power_control)\n+\t\tif (!link->skip_implict_edp_power_control && hws)\n \t\t\thws->funcs.edp_backlight_control(link, false);\n \t\tlink->dc->hwss.set_abm_immediate_disable(pipe_ctx);\n \t}\ndiff --git a/drivers/gpu/drm/amd/display/dc/hwss/dcn35/dcn35_hwseq.c b/drivers/gpu/drm/amd/display/dc/hwss/dcn35/dcn35_hwseq.c\nindex c814d957305a..a267f574b619 100644\n--- a/drivers/gpu/drm/amd/display/dc/hwss/dcn35/dcn35_hwseq.c\n+++ b/drivers/gpu/drm/amd/display/dc/hwss/dcn35/dcn35_hwseq.c\n@@ -1047,6 +1047,15 @@ void dcn35_calc_blocks_to_gate(struct dc *dc, struct dc_state *context,\n \t\t\tif (dc->caps.sequential_ono) {\n \t\t\t\tupdate_state->pg_pipe_res_update[PG_HUBP][pipe_ctx->stream_res.dsc->inst] = false;\n \t\t\t\tupdate_state->pg_pipe_res_update[PG_DPP][pipe_ctx->stream_res.dsc->inst] = false;\n+\n+\t\t\t\t/* All HUBP/DPP instances must be powered if the DSC inst != HUBP inst */\n+\t\t\t\tif (!pipe_ctx->top_pipe && pipe_ctx->plane_res.hubp &&\n+\t\t\t\t    pipe_ctx->plane_res.hubp->inst != pipe_ctx->stream_res.dsc->inst) {\n+\t\t\t\t\tfor (j = 0; j < dc->res_pool->pipe_count; ++j) {\n+\t\t\t\t\t\tupdate_state->pg_pipe_res_update[PG_HUBP][j] = false;\n+\t\t\t\t\t\tupdate_state->pg_pipe_res_update[PG_DPP][j] = false;\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n@@ -1193,6 +1202,25 @@ void dcn35_calc_blocks_to_ungate(struct dc *dc, struct dc_state *context,\n \t\tupdate_state->pg_pipe_res_update[PG_HDMISTREAM][0] = true;\n \n \tif (dc->caps.sequential_ono) {\n+\t\tfor (i = 0; i < dc->res_pool->pipe_count; i++) {\n+\t\t\tstruct pipe_ctx *new_pipe = &context->res_ctx.pipe_ctx[i];\n+\n+\t\t\tif (new_pipe->stream_res.dsc && !new_pipe->top_pipe &&\n+\t\t\t    update_state->pg_pipe_res_update[PG_DSC][new_pipe->stream_res.dsc->inst]) {\n+\t\t\t\tupdate_state->pg_pipe_res_update[PG_HUBP][new_pipe->stream_res.dsc->inst] = true;\n+\t\t\t\tupdate_state->pg_pipe_res_update[PG_DPP][new_pipe->stream_res.dsc->inst] = true;\n+\n+\t\t\t\t/* All HUBP/DPP instances must be powered if the DSC inst != HUBP inst */\n+\t\t\t\tif (new_pipe->plane_res.hubp &&\n+\t\t\t\t    new_pipe->plane_res.hubp->inst != new_pipe->stream_res.dsc->inst) {\n+\t\t\t\t\tfor (j = 0; j < dc->res_pool->pipe_count; ++j) {\n+\t\t\t\t\t\tupdate_state->pg_pipe_res_update[PG_HUBP][j] = true;\n+\t\t\t\t\t\tupdate_state->pg_pipe_res_update[PG_DPP][j] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n \t\tfor (i = dc->res_pool->pipe_count - 1; i >= 0; i--) {\n \t\t\tif (update_state->pg_pipe_res_update[PG_HUBP][i] &&\n \t\t\t    update_state->pg_pipe_res_update[PG_DPP][i]) {\ndiff --git a/drivers/gpu/drm/amd/display/dc/link/protocols/link_dp_capability.c b/drivers/gpu/drm/amd/display/dc/link/protocols/link_dp_capability.c\nindex a5127c2d47ef..0f965380a9b4 100644\n--- a/drivers/gpu/drm/amd/display/dc/link/protocols/link_dp_capability.c\n+++ b/drivers/gpu/drm/amd/display/dc/link/protocols/link_dp_capability.c\n@@ -385,9 +385,15 @@ bool dp_is_128b_132b_signal(struct pipe_ctx *pipe_ctx)\n bool dp_is_lttpr_present(struct dc_link *link)\n {\n \t/* Some sink devices report invalid LTTPR revision, so don't validate against that cap */\n-\treturn (dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt) != 0 &&\n+\tuint32_t lttpr_count = dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt);\n+\tbool is_lttpr_present = (lttpr_count > 0 &&\n \t\t\tlink->dpcd_caps.lttpr_caps.max_lane_count > 0 &&\n \t\t\tlink->dpcd_caps.lttpr_caps.max_lane_count <= 4);\n+\n+\tif (lttpr_count > 0 && !is_lttpr_present)\n+\t\tDC_LOG_ERROR(\"LTTPR count is nonzero but invalid lane count reported. Assuming no LTTPR present.\\n\");\n+\n+\treturn is_lttpr_present;\n }\n \n /* in DP compliance test, DPR-120 may have\n@@ -1551,6 +1557,8 @@ enum dc_status dp_retrieve_lttpr_cap(struct dc_link *link)\n \tuint8_t lttpr_dpcd_data[10] = {0};\n \tenum dc_status status;\n \tbool is_lttpr_present;\n+\tuint32_t lttpr_count;\n+\tuint32_t closest_lttpr_offset;\n \n \t/* Logic to determine LTTPR support*/\n \tbool vbios_lttpr_interop = link->dc->caps.vbios_lttpr_aware;\n@@ -1602,20 +1610,22 @@ enum dc_status dp_retrieve_lttpr_cap(struct dc_link *link)\n \t\t\tlttpr_dpcd_data[DP_LTTPR_ALPM_CAPABILITIES -\n \t\t\t\t\t\t\tDP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];\n \n+\tlttpr_count = dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt);\n+\n \t/* If this chip cap is set, at least one retimer must exist in the chain\n \t * Override count to 1 if we receive a known bad count (0 or an invalid value) */\n \tif (((link->chip_caps & AMD_EXT_DISPLAY_PATH_CAPS__EXT_CHIP_MASK) == AMD_EXT_DISPLAY_PATH_CAPS__DP_FIXED_VS_EN) &&\n-\t\t\t(dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt) == 0)) {\n+\t\t\tlttpr_count == 0) {\n \t\t/* If you see this message consistently, either the host platform has FIXED_VS flag\n \t\t * incorrectly configured or the sink device is returning an invalid count.\n \t\t */\n \t\tDC_LOG_ERROR(\"lttpr_caps phy_repeater_cnt is 0x%x, forcing it to 0x80.\",\n \t\t\t     link->dpcd_caps.lttpr_caps.phy_repeater_cnt);\n \t\tlink->dpcd_caps.lttpr_caps.phy_repeater_cnt = 0x80;\n+\t\tlttpr_count = 1;\n \t\tDC_LOG_DC(\"lttpr_caps forced phy_repeater_cnt = %d\\n\", link->dpcd_caps.lttpr_caps.phy_repeater_cnt);\n \t}\n \n-\t/* Attempt to train in LTTPR transparent mode if repeater count exceeds 8. */\n \tis_lttpr_present = dp_is_lttpr_present(link);\n \n \tDC_LOG_DC(\"is_lttpr_present = %d\\n\", is_lttpr_present);\n@@ -1623,11 +1633,25 @@ enum dc_status dp_retrieve_lttpr_cap(struct dc_link *link)\n \tif (is_lttpr_present) {\n \t\tCONN_DATA_DETECT(link, lttpr_dpcd_data, sizeof(lttpr_dpcd_data), \"LTTPR Caps: \");\n \n-\t\tcore_link_read_dpcd(link, DP_LTTPR_IEEE_OUI, link->dpcd_caps.lttpr_caps.lttpr_ieee_oui, sizeof(link->dpcd_caps.lttpr_caps.lttpr_ieee_oui));\n-\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_ieee_oui, sizeof(link->dpcd_caps.lttpr_caps.lttpr_ieee_oui), \"LTTPR IEEE OUI: \");\n+\t\t// Identify closest LTTPR to determine if workarounds required for known embedded LTTPR\n+\t\tclosest_lttpr_offset = dp_get_closest_lttpr_offset(lttpr_count);\n \n-\t\tcore_link_read_dpcd(link, DP_LTTPR_DEVICE_ID, link->dpcd_caps.lttpr_caps.lttpr_device_id, sizeof(link->dpcd_caps.lttpr_caps.lttpr_device_id));\n-\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_device_id, sizeof(link->dpcd_caps.lttpr_caps.lttpr_device_id), \"LTTPR Device ID: \");\n+\t\tcore_link_read_dpcd(link, (DP_LTTPR_IEEE_OUI + closest_lttpr_offset),\n+\t\t\t\tlink->dpcd_caps.lttpr_caps.lttpr_ieee_oui, sizeof(link->dpcd_caps.lttpr_caps.lttpr_ieee_oui));\n+\t\tcore_link_read_dpcd(link, (DP_LTTPR_DEVICE_ID + closest_lttpr_offset),\n+\t\t\t\tlink->dpcd_caps.lttpr_caps.lttpr_device_id, sizeof(link->dpcd_caps.lttpr_caps.lttpr_device_id));\n+\n+\t\tif (lttpr_count > 1) {\n+\t\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_ieee_oui, sizeof(link->dpcd_caps.lttpr_caps.lttpr_ieee_oui),\n+\t\t\t\t\t\"Closest LTTPR To Host's IEEE OUI: \");\n+\t\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_device_id, sizeof(link->dpcd_caps.lttpr_caps.lttpr_device_id),\n+\t\t\t\t\t\"Closest LTTPR To Host's LTTPR Device ID: \");\n+\t\t} else {\n+\t\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_ieee_oui, sizeof(link->dpcd_caps.lttpr_caps.lttpr_ieee_oui),\n+\t\t\t\t\t\"LTTPR IEEE OUI: \");\n+\t\t\tCONN_DATA_DETECT(link, link->dpcd_caps.lttpr_caps.lttpr_device_id, sizeof(link->dpcd_caps.lttpr_caps.lttpr_device_id),\n+\t\t\t\t\t\"LTTPR Device ID: \");\n+\t\t}\n \t}\n \n \treturn status;\ndiff --git a/drivers/gpu/drm/amd/display/dc/resource/dcn31/dcn31_resource.c b/drivers/gpu/drm/amd/display/dc/resource/dcn31/dcn31_resource.c\nindex 7e0af5297dc4..51ca0b2959fc 100644\n--- a/drivers/gpu/drm/amd/display/dc/resource/dcn31/dcn31_resource.c\n+++ b/drivers/gpu/drm/amd/display/dc/resource/dcn31/dcn31_resource.c\n@@ -1954,6 +1954,9 @@ static bool dcn31_resource_construct(\n \tdc->caps.color.mpc.ogam_rom_caps.hlg = 0;\n \tdc->caps.color.mpc.ocsc = 1;\n \n+\tdc->caps.num_of_host_routers = 2;\n+\tdc->caps.num_of_dpias_per_host_router = 2;\n+\n \t/* Use pipe context based otg sync logic */\n \tdc->config.use_pipe_ctx_sync_logic = true;\n \tdc->config.disable_hbr_audio_dp2 = true;\ndiff --git a/drivers/gpu/drm/amd/display/dc/resource/dcn314/dcn314_resource.c b/drivers/gpu/drm/amd/display/dc/resource/dcn314/dcn314_resource.c\nindex d96bc6cb73ad..8383e2e59be5 100644\n--- a/drivers/gpu/drm/amd/display/dc/resource/dcn314/dcn314_resource.c\n+++ b/drivers/gpu/drm/amd/display/dc/resource/dcn314/dcn314_resource.c\n@@ -1885,6 +1885,9 @@ static bool dcn314_resource_construct(\n \n \tdc->caps.max_disp_clock_khz_at_vmin = 650000;\n \n+\tdc->caps.num_of_host_routers = 2;\n+\tdc->caps.num_of_dpias_per_host_router = 2;\n+\n \t/* Use pipe context based otg sync logic */\n \tdc->config.use_pipe_ctx_sync_logic = true;\n \ndiff --git a/drivers/gpu/drm/amd/display/dc/resource/dcn35/dcn35_resource.c b/drivers/gpu/drm/amd/display/dc/resource/dcn35/dcn35_resource.c\nindex 72c6cf047db0..e01aa2f2e13e 100644\n--- a/drivers/gpu/drm/amd/display/dc/resource/dcn35/dcn35_resource.c\n+++ b/drivers/gpu/drm/amd/display/dc/resource/dcn35/dcn35_resource.c\n@@ -1894,6 +1894,9 @@ static bool dcn35_resource_construct(\n \tdc->caps.color.mpc.ogam_rom_caps.hlg = 0;\n \tdc->caps.color.mpc.ocsc = 1;\n \n+\tdc->caps.num_of_host_routers = 2;\n+\tdc->caps.num_of_dpias_per_host_router = 2;\n+\n \t/* max_disp_clock_khz_at_vmin is slightly lower than the STA value in order\n \t * to provide some margin.\n \t * It's expected for furture ASIC to have equal or higher value, in order to\ndiff --git a/drivers/gpu/drm/amd/display/dc/resource/dcn351/dcn351_resource.c b/drivers/gpu/drm/amd/display/dc/resource/dcn351/dcn351_resource.c\nindex 989a270f7dea..4ebe4e00a4f8 100644\n--- a/drivers/gpu/drm/amd/display/dc/resource/dcn351/dcn351_resource.c\n+++ b/drivers/gpu/drm/amd/display/dc/resource/dcn351/dcn351_resource.c\n@@ -1866,6 +1866,9 @@ static bool dcn351_resource_construct(\n \tdc->caps.color.mpc.ogam_rom_caps.hlg = 0;\n \tdc->caps.color.mpc.ocsc = 1;\n \n+\tdc->caps.num_of_host_routers = 2;\n+\tdc->caps.num_of_dpias_per_host_router = 2;\n+\n \t/* max_disp_clock_khz_at_vmin is slightly lower than the STA value in order\n \t * to provide some margin.\n \t * It's expected for furture ASIC to have equal or higher value, in order to\ndiff --git a/drivers/gpu/drm/amd/display/dc/resource/dcn36/dcn36_resource.c b/drivers/gpu/drm/amd/display/dc/resource/dcn36/dcn36_resource.c\nindex 48e1f234185f..db36b8f9ce65 100644\n--- a/drivers/gpu/drm/amd/display/dc/resource/dcn36/dcn36_resource.c\n+++ b/drivers/gpu/drm/amd/display/dc/resource/dcn36/dcn36_resource.c\n@@ -1867,6 +1867,9 @@ static bool dcn36_resource_construct(\n \tdc->caps.color.mpc.ogam_rom_caps.hlg = 0;\n \tdc->caps.color.mpc.ocsc = 1;\n \n+\tdc->caps.num_of_host_routers = 2;\n+\tdc->caps.num_of_dpias_per_host_router = 2;\n+\n \t/* max_disp_clock_khz_at_vmin is slightly lower than the STA value in order\n \t * to provide some margin.\n \t * It's expected for furture ASIC to have equal or higher value, in order to\ndiff --git a/drivers/gpu/drm/amd/pm/swsmu/smu13/smu_v13_0.c b/drivers/gpu/drm/amd/pm/swsmu/smu13/smu_v13_0.c\nindex a7167668d189..1c7235935d14 100644\n--- a/drivers/gpu/drm/amd/pm/swsmu/smu13/smu_v13_0.c\n+++ b/drivers/gpu/drm/amd/pm/swsmu/smu13/smu_v13_0.c\n@@ -58,6 +58,7 @@\n \n MODULE_FIRMWARE(\"amdgpu/aldebaran_smc.bin\");\n MODULE_FIRMWARE(\"amdgpu/smu_13_0_0.bin\");\n+MODULE_FIRMWARE(\"amdgpu/smu_13_0_0_kicker.bin\");\n MODULE_FIRMWARE(\"amdgpu/smu_13_0_7.bin\");\n MODULE_FIRMWARE(\"amdgpu/smu_13_0_10.bin\");\n \n@@ -92,7 +93,7 @@ const int pmfw_decoded_link_width[7] = {0, 1, 2, 4, 8, 12, 16};\n int smu_v13_0_init_microcode(struct smu_context *smu)\n {\n \tstruct amdgpu_device *adev = smu->adev;\n-\tchar ucode_prefix[15];\n+\tchar ucode_prefix[30];\n \tint err = 0;\n \tconst struct smc_firmware_header_v1_0 *hdr;\n \tconst struct common_firmware_header *header;\n@@ -103,8 +104,13 @@ int smu_v13_0_init_microcode(struct smu_context *smu)\n \t\treturn 0;\n \n \tamdgpu_ucode_ip_version_decode(adev, MP1_HWIP, ucode_prefix, sizeof(ucode_prefix));\n-\terr = amdgpu_ucode_request(adev, &adev->pm.fw, AMDGPU_UCODE_REQUIRED,\n-\t\t\t\t   \"amdgpu/%s.bin\", ucode_prefix);\n+\n+\tif (amdgpu_is_kicker_fw(adev))\n+\t\terr = amdgpu_ucode_request(adev, &adev->pm.fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s_kicker.bin\", ucode_prefix);\n+\telse\n+\t\terr = amdgpu_ucode_request(adev, &adev->pm.fw, AMDGPU_UCODE_REQUIRED,\n+\t\t\t\t\t   \"amdgpu/%s.bin\", ucode_prefix);\n \tif (err)\n \t\tgoto out;\n \ndiff --git a/drivers/gpu/drm/arm/malidp_planes.c b/drivers/gpu/drm/arm/malidp_planes.c\nindex 34547edf1ee3..87f2e5ee8790 100644\n--- a/drivers/gpu/drm/arm/malidp_planes.c\n+++ b/drivers/gpu/drm/arm/malidp_planes.c\n@@ -159,7 +159,7 @@ bool malidp_format_mod_supported(struct drm_device *drm,\n \t}\n \n \tif (!fourcc_mod_is_vendor(modifier, ARM)) {\n-\t\tDRM_ERROR(\"Unknown modifier (not Arm)\\n\");\n+\t\tDRM_DEBUG_KMS(\"Unknown modifier (not Arm)\\n\");\n \t\treturn false;\n \t}\n \ndiff --git a/drivers/gpu/drm/ast/ast_mode.c b/drivers/gpu/drm/ast/ast_mode.c\nindex 1de832964e92..031980d8f3ab 100644\n--- a/drivers/gpu/drm/ast/ast_mode.c\n+++ b/drivers/gpu/drm/ast/ast_mode.c\n@@ -29,7 +29,6 @@\n  */\n \n #include <linux/delay.h>\n-#include <linux/export.h>\n #include <linux/pci.h>\n \n #include <drm/drm_atomic.h>\ndiff --git a/drivers/gpu/drm/etnaviv/etnaviv_sched.c b/drivers/gpu/drm/etnaviv/etnaviv_sched.c\nindex 76a3a3e517d8..71e2e6b9d713 100644\n--- a/drivers/gpu/drm/etnaviv/etnaviv_sched.c\n+++ b/drivers/gpu/drm/etnaviv/etnaviv_sched.c\n@@ -35,6 +35,7 @@ static enum drm_gpu_sched_stat etnaviv_sched_timedout_job(struct drm_sched_job\n \t\t\t\t\t\t\t  *sched_job)\n {\n \tstruct etnaviv_gem_submit *submit = to_etnaviv_submit(sched_job);\n+\tstruct drm_gpu_scheduler *sched = sched_job->sched;\n \tstruct etnaviv_gpu *gpu = submit->gpu;\n \tu32 dma_addr, primid = 0;\n \tint change;\n@@ -89,7 +90,9 @@ static enum drm_gpu_sched_stat etnaviv_sched_timedout_job(struct drm_sched_job\n \treturn DRM_GPU_SCHED_STAT_NOMINAL;\n \n out_no_timeout:\n-\tlist_add(&sched_job->list, &sched_job->sched->pending_list);\n+\tspin_lock(&sched->job_list_lock);\n+\tlist_add(&sched_job->list, &sched->pending_list);\n+\tspin_unlock(&sched->job_list_lock);\n \treturn DRM_GPU_SCHED_STAT_NOMINAL;\n }\n \ndiff --git a/drivers/gpu/drm/i915/display/vlv_dsi.c b/drivers/gpu/drm/i915/display/vlv_dsi.c\nindex 346737f15fa9..21c1e10caf68 100644\n--- a/drivers/gpu/drm/i915/display/vlv_dsi.c\n+++ b/drivers/gpu/drm/i915/display/vlv_dsi.c\n@@ -1056,7 +1056,7 @@ static void bxt_dsi_get_pipe_config(struct intel_encoder *encoder,\n \t\t\t\t              BXT_MIPI_TRANS_VACTIVE(port));\n \tadjusted_mode->crtc_vtotal =\n \t\t\t\tintel_de_read(display,\n-\t\t\t\t              BXT_MIPI_TRANS_VTOTAL(port));\n+\t\t\t\t              BXT_MIPI_TRANS_VTOTAL(port)) + 1;\n \n \thactive = adjusted_mode->crtc_hdisplay;\n \thfp = intel_de_read(display, MIPI_HFP_COUNT(display, port));\n@@ -1260,7 +1260,7 @@ static void set_dsi_timings(struct intel_encoder *encoder,\n \t\t\tintel_de_write(display, BXT_MIPI_TRANS_VACTIVE(port),\n \t\t\t\t       adjusted_mode->crtc_vdisplay);\n \t\t\tintel_de_write(display, BXT_MIPI_TRANS_VTOTAL(port),\n-\t\t\t\t       adjusted_mode->crtc_vtotal);\n+\t\t\t\t       adjusted_mode->crtc_vtotal - 1);\n \t\t}\n \n \t\tintel_de_write(display, MIPI_HACTIVE_AREA_COUNT(display, port),\ndiff --git a/drivers/gpu/drm/i915/i915_pmu.c b/drivers/gpu/drm/i915/i915_pmu.c\nindex e5a188ce3185..990bfaba3ce4 100644\n--- a/drivers/gpu/drm/i915/i915_pmu.c\n+++ b/drivers/gpu/drm/i915/i915_pmu.c\n@@ -112,7 +112,7 @@ static u32 config_mask(const u64 config)\n {\n \tunsigned int bit = config_bit(config);\n \n-\tif (__builtin_constant_p(config))\n+\tif (__builtin_constant_p(bit))\n \t\tBUILD_BUG_ON(bit >\n \t\t\t     BITS_PER_TYPE(typeof_member(struct i915_pmu,\n \t\t\t\t\t\t\t enable)) - 1);\n@@ -121,7 +121,7 @@ static u32 config_mask(const u64 config)\n \t\t\t     BITS_PER_TYPE(typeof_member(struct i915_pmu,\n \t\t\t\t\t\t\t enable)) - 1);\n \n-\treturn BIT(config_bit(config));\n+\treturn BIT(bit);\n }\n \n static bool is_engine_event(struct perf_event *event)\ndiff --git a/drivers/gpu/drm/mgag200/mgag200_ddc.c b/drivers/gpu/drm/mgag200/mgag200_ddc.c\nindex 6d81ea8931e8..c31673eaa554 100644\n--- a/drivers/gpu/drm/mgag200/mgag200_ddc.c\n+++ b/drivers/gpu/drm/mgag200/mgag200_ddc.c\n@@ -26,7 +26,6 @@\n  * Authors: Dave Airlie <airlied@redhat.com>\n  */\n \n-#include <linux/export.h>\n #include <linux/i2c-algo-bit.h>\n #include <linux/i2c.h>\n #include <linux/pci.h>\ndiff --git a/drivers/gpu/drm/msm/adreno/a2xx_gpummu.c b/drivers/gpu/drm/msm/adreno/a2xx_gpummu.c\nindex 39641551eeb6..4280f71e472a 100644\n--- a/drivers/gpu/drm/msm/adreno/a2xx_gpummu.c\n+++ b/drivers/gpu/drm/msm/adreno/a2xx_gpummu.c\n@@ -71,10 +71,6 @@ static int a2xx_gpummu_unmap(struct msm_mmu *mmu, uint64_t iova, size_t len)\n \treturn 0;\n }\n \n-static void a2xx_gpummu_resume_translation(struct msm_mmu *mmu)\n-{\n-}\n-\n static void a2xx_gpummu_destroy(struct msm_mmu *mmu)\n {\n \tstruct a2xx_gpummu *gpummu = to_a2xx_gpummu(mmu);\n@@ -90,7 +86,6 @@ static const struct msm_mmu_funcs funcs = {\n \t\t.map = a2xx_gpummu_map,\n \t\t.unmap = a2xx_gpummu_unmap,\n \t\t.destroy = a2xx_gpummu_destroy,\n-\t\t.resume_translation = a2xx_gpummu_resume_translation,\n };\n \n struct msm_mmu *a2xx_gpummu_new(struct device *dev, struct msm_gpu *gpu)\ndiff --git a/drivers/gpu/drm/msm/adreno/a5xx_gpu.c b/drivers/gpu/drm/msm/adreno/a5xx_gpu.c\nindex 650e5bac225f..60aef0796236 100644\n--- a/drivers/gpu/drm/msm/adreno/a5xx_gpu.c\n+++ b/drivers/gpu/drm/msm/adreno/a5xx_gpu.c\n@@ -131,6 +131,8 @@ static void a5xx_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n \tstruct msm_ringbuffer *ring = submit->ring;\n \tunsigned int i, ibs = 0;\n \n+\tadreno_check_and_reenable_stall(adreno_gpu);\n+\n \tif (IS_ENABLED(CONFIG_DRM_MSM_GPU_SUDO) && submit->in_rb) {\n \t\tring->cur_ctx_seqno = 0;\n \t\ta5xx_submit_in_rb(gpu, submit);\ndiff --git a/drivers/gpu/drm/msm/adreno/a6xx_gpu.c b/drivers/gpu/drm/msm/adreno/a6xx_gpu.c\nindex bf3758f010f4..491fde0083a2 100644\n--- a/drivers/gpu/drm/msm/adreno/a6xx_gpu.c\n+++ b/drivers/gpu/drm/msm/adreno/a6xx_gpu.c\n@@ -130,6 +130,20 @@ static void a6xx_set_pagetable(struct a6xx_gpu *a6xx_gpu,\n \t\tOUT_RING(ring, lower_32_bits(rbmemptr(ring, fence)));\n \t\tOUT_RING(ring, upper_32_bits(rbmemptr(ring, fence)));\n \t\tOUT_RING(ring, submit->seqno - 1);\n+\n+\t\tOUT_PKT7(ring, CP_THREAD_CONTROL, 1);\n+\t\tOUT_RING(ring, CP_SET_THREAD_BOTH);\n+\n+\t\t/* Reset state used to synchronize BR and BV */\n+\t\tOUT_PKT7(ring, CP_RESET_CONTEXT_STATE, 1);\n+\t\tOUT_RING(ring,\n+\t\t\t CP_RESET_CONTEXT_STATE_0_CLEAR_ON_CHIP_TS |\n+\t\t\t CP_RESET_CONTEXT_STATE_0_CLEAR_RESOURCE_TABLE |\n+\t\t\t CP_RESET_CONTEXT_STATE_0_CLEAR_BV_BR_COUNTER |\n+\t\t\t CP_RESET_CONTEXT_STATE_0_RESET_GLOBAL_LOCAL_TS);\n+\n+\t\tOUT_PKT7(ring, CP_THREAD_CONTROL, 1);\n+\t\tOUT_RING(ring, CP_SET_THREAD_BR);\n \t}\n \n \tif (!sysprof) {\n@@ -212,6 +226,8 @@ static void a6xx_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n \tstruct msm_ringbuffer *ring = submit->ring;\n \tunsigned int i, ibs = 0;\n \n+\tadreno_check_and_reenable_stall(adreno_gpu);\n+\n \ta6xx_set_pagetable(a6xx_gpu, ring, submit);\n \n \tget_stats_counter(ring, REG_A6XX_RBBM_PERFCTR_CP(0),\n@@ -335,6 +351,8 @@ static void a7xx_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n \tstruct msm_ringbuffer *ring = submit->ring;\n \tunsigned int i, ibs = 0;\n \n+\tadreno_check_and_reenable_stall(adreno_gpu);\n+\n \t/*\n \t * Toggle concurrent binning for pagetable switch and set the thread to\n \t * BR since only it can execute the pagetable switch packets.\ndiff --git a/drivers/gpu/drm/msm/adreno/adreno_device.c b/drivers/gpu/drm/msm/adreno/adreno_device.c\nindex f5e1490d07c1..16e7ac444efd 100644\n--- a/drivers/gpu/drm/msm/adreno/adreno_device.c\n+++ b/drivers/gpu/drm/msm/adreno/adreno_device.c\n@@ -137,9 +137,8 @@ struct msm_gpu *adreno_load_gpu(struct drm_device *dev)\n \treturn NULL;\n }\n \n-static int find_chipid(struct device *dev, uint32_t *chipid)\n+static int find_chipid(struct device_node *node, uint32_t *chipid)\n {\n-\tstruct device_node *node = dev->of_node;\n \tconst char *compat;\n \tint ret;\n \n@@ -173,15 +172,36 @@ static int find_chipid(struct device *dev, uint32_t *chipid)\n \t/* and if that fails, fall back to legacy \"qcom,chipid\" property: */\n \tret = of_property_read_u32(node, \"qcom,chipid\", chipid);\n \tif (ret) {\n-\t\tDRM_DEV_ERROR(dev, \"could not parse qcom,chipid: %d\\n\", ret);\n+\t\tDRM_ERROR(\"%pOF: could not parse qcom,chipid: %d\\n\",\n+\t\t\t  node, ret);\n \t\treturn ret;\n \t}\n \n-\tdev_warn(dev, \"Using legacy qcom,chipid binding!\\n\");\n+\tpr_warn(\"%pOF: Using legacy qcom,chipid binding!\\n\", node);\n \n \treturn 0;\n }\n \n+bool adreno_has_gpu(struct device_node *node)\n+{\n+\tconst struct adreno_info *info;\n+\tuint32_t chip_id;\n+\tint ret;\n+\n+\tret = find_chipid(node, &chip_id);\n+\tif (ret)\n+\t\treturn false;\n+\n+\tinfo = adreno_info(chip_id);\n+\tif (!info) {\n+\t\tpr_warn(\"%pOF: Unknown GPU revision: %\"ADRENO_CHIPID_FMT\"\\n\",\n+\t\t\tnode, ADRENO_CHIPID_ARGS(chip_id));\n+\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\n+\n static int adreno_bind(struct device *dev, struct device *master, void *data)\n {\n \tstatic struct adreno_platform_config config = {};\n@@ -191,19 +211,18 @@ static int adreno_bind(struct device *dev, struct device *master, void *data)\n \tstruct msm_gpu *gpu;\n \tint ret;\n \n-\tret = find_chipid(dev, &config.chip_id);\n-\tif (ret)\n+\tret = find_chipid(dev->of_node, &config.chip_id);\n+\t/* We shouldn't have gotten this far if we can't parse the chip_id */\n+\tif (WARN_ON(ret))\n \t\treturn ret;\n \n \tdev->platform_data = &config;\n \tpriv->gpu_pdev = to_platform_device(dev);\n \n \tinfo = adreno_info(config.chip_id);\n-\tif (!info) {\n-\t\tdev_warn(drm->dev, \"Unknown GPU revision: %\"ADRENO_CHIPID_FMT\"\\n\",\n-\t\t\tADRENO_CHIPID_ARGS(config.chip_id));\n+\t/* We shouldn't have gotten this far if we don't recognize the GPU: */\n+\tif (WARN_ON(!info))\n \t\treturn -ENXIO;\n-\t}\n \n \tconfig.info = info;\n \ndiff --git a/drivers/gpu/drm/msm/adreno/adreno_gpu.c b/drivers/gpu/drm/msm/adreno/adreno_gpu.c\nindex 2348ffb35f7e..86bff915c3e7 100644\n--- a/drivers/gpu/drm/msm/adreno/adreno_gpu.c\n+++ b/drivers/gpu/drm/msm/adreno/adreno_gpu.c\n@@ -259,24 +259,54 @@ u64 adreno_private_address_space_size(struct msm_gpu *gpu)\n \treturn BIT(ttbr1_cfg->ias) - ADRENO_VM_START;\n }\n \n+void adreno_check_and_reenable_stall(struct adreno_gpu *adreno_gpu)\n+{\n+\tstruct msm_gpu *gpu = &adreno_gpu->base;\n+\tstruct msm_drm_private *priv = gpu->dev->dev_private;\n+\tunsigned long flags;\n+\n+\t/*\n+\t * Wait until the cooldown period has passed and we would actually\n+\t * collect a crashdump to re-enable stall-on-fault.\n+\t */\n+\tspin_lock_irqsave(&priv->fault_stall_lock, flags);\n+\tif (!priv->stall_enabled &&\n+\t\t\tktime_after(ktime_get(), priv->stall_reenable_time) &&\n+\t\t\t!READ_ONCE(gpu->crashstate)) {\n+\t\tpriv->stall_enabled = true;\n+\n+\t\tgpu->aspace->mmu->funcs->set_stall(gpu->aspace->mmu, true);\n+\t}\n+\tspin_unlock_irqrestore(&priv->fault_stall_lock, flags);\n+}\n+\n #define ARM_SMMU_FSR_TF                 BIT(1)\n #define ARM_SMMU_FSR_PF\t\t\tBIT(3)\n #define ARM_SMMU_FSR_EF\t\t\tBIT(4)\n+#define ARM_SMMU_FSR_SS\t\t\tBIT(30)\n \n int adreno_fault_handler(struct msm_gpu *gpu, unsigned long iova, int flags,\n \t\t\t struct adreno_smmu_fault_info *info, const char *block,\n \t\t\t u32 scratch[4])\n {\n+\tstruct msm_drm_private *priv = gpu->dev->dev_private;\n \tconst char *type = \"UNKNOWN\";\n-\tbool do_devcoredump = info && !READ_ONCE(gpu->crashstate);\n+\tbool do_devcoredump = info && (info->fsr & ARM_SMMU_FSR_SS) &&\n+\t\t!READ_ONCE(gpu->crashstate);\n+\tunsigned long irq_flags;\n \n \t/*\n-\t * If we aren't going to be resuming later from fault_worker, then do\n-\t * it now.\n+\t * In case there is a subsequent storm of pagefaults, disable\n+\t * stall-on-fault for at least half a second.\n \t */\n-\tif (!do_devcoredump) {\n-\t\tgpu->aspace->mmu->funcs->resume_translation(gpu->aspace->mmu);\n+\tspin_lock_irqsave(&priv->fault_stall_lock, irq_flags);\n+\tif (priv->stall_enabled) {\n+\t\tpriv->stall_enabled = false;\n+\n+\t\tgpu->aspace->mmu->funcs->set_stall(gpu->aspace->mmu, false);\n \t}\n+\tpriv->stall_reenable_time = ktime_add_ms(ktime_get(), 500);\n+\tspin_unlock_irqrestore(&priv->fault_stall_lock, irq_flags);\n \n \t/*\n \t * Print a default message if we couldn't get the data from the\n@@ -304,16 +334,18 @@ int adreno_fault_handler(struct msm_gpu *gpu, unsigned long iova, int flags,\n \t\t\tscratch[0], scratch[1], scratch[2], scratch[3]);\n \n \tif (do_devcoredump) {\n+\t\tstruct msm_gpu_fault_info fault_info = {};\n+\n \t\t/* Turn off the hangcheck timer to keep it from bothering us */\n \t\ttimer_delete(&gpu->hangcheck_timer);\n \n-\t\tgpu->fault_info.ttbr0 = info->ttbr0;\n-\t\tgpu->fault_info.iova  = iova;\n-\t\tgpu->fault_info.flags = flags;\n-\t\tgpu->fault_info.type  = type;\n-\t\tgpu->fault_info.block = block;\n+\t\tfault_info.ttbr0 = info->ttbr0;\n+\t\tfault_info.iova  = iova;\n+\t\tfault_info.flags = flags;\n+\t\tfault_info.type  = type;\n+\t\tfault_info.block = block;\n \n-\t\tkthread_queue_work(gpu->worker, &gpu->fault_work);\n+\t\tmsm_gpu_fault_crashstate_capture(gpu, &fault_info);\n \t}\n \n \treturn 0;\ndiff --git a/drivers/gpu/drm/msm/adreno/adreno_gpu.h b/drivers/gpu/drm/msm/adreno/adreno_gpu.h\nindex a8f4bf416e64..bc063594a359 100644\n--- a/drivers/gpu/drm/msm/adreno/adreno_gpu.h\n+++ b/drivers/gpu/drm/msm/adreno/adreno_gpu.h\n@@ -636,6 +636,8 @@ int adreno_fault_handler(struct msm_gpu *gpu, unsigned long iova, int flags,\n \t\t\t struct adreno_smmu_fault_info *info, const char *block,\n \t\t\t u32 scratch[4]);\n \n+void adreno_check_and_reenable_stall(struct adreno_gpu *gpu);\n+\n int adreno_read_speedbin(struct device *dev, u32 *speedbin);\n \n /*\ndiff --git a/drivers/gpu/drm/msm/disp/dpu1/dpu_encoder_phys_vid.c b/drivers/gpu/drm/msm/disp/dpu1/dpu_encoder_phys_vid.c\nindex 8a618841e3ea..1c468ca5d692 100644\n--- a/drivers/gpu/drm/msm/disp/dpu1/dpu_encoder_phys_vid.c\n+++ b/drivers/gpu/drm/msm/disp/dpu1/dpu_encoder_phys_vid.c\n@@ -94,17 +94,21 @@ static void drm_mode_to_intf_timing_params(\n \t\ttiming->vsync_polarity = 0;\n \t}\n \n-\t/* for DP/EDP, Shift timings to align it to bottom right */\n-\tif (phys_enc->hw_intf->cap->type == INTF_DP) {\n+\ttiming->wide_bus_en = dpu_encoder_is_widebus_enabled(phys_enc->parent);\n+\ttiming->compression_en = dpu_encoder_is_dsc_enabled(phys_enc->parent);\n+\n+\t/*\n+\t *  For DP/EDP, Shift timings to align it to bottom right.\n+\t *  wide_bus_en is set for everything excluding SDM845 &\n+\t *  porch changes cause DisplayPort failure and HDMI tearing.\n+\t */\n+\tif (phys_enc->hw_intf->cap->type == INTF_DP && timing->wide_bus_en) {\n \t\ttiming->h_back_porch += timing->h_front_porch;\n \t\ttiming->h_front_porch = 0;\n \t\ttiming->v_back_porch += timing->v_front_porch;\n \t\ttiming->v_front_porch = 0;\n \t}\n \n-\ttiming->wide_bus_en = dpu_encoder_is_widebus_enabled(phys_enc->parent);\n-\ttiming->compression_en = dpu_encoder_is_dsc_enabled(phys_enc->parent);\n-\n \t/*\n \t * for DP, divide the horizonal parameters by 2 when\n \t * widebus is enabled\ndiff --git a/drivers/gpu/drm/msm/dp/dp_display.c b/drivers/gpu/drm/msm/dp/dp_display.c\nindex 386c4669c831..a48e6db4f156 100644\n--- a/drivers/gpu/drm/msm/dp/dp_display.c\n+++ b/drivers/gpu/drm/msm/dp/dp_display.c\n@@ -128,6 +128,11 @@ static const struct msm_dp_desc msm_dp_desc_sa8775p[] = {\n \t{}\n };\n \n+static const struct msm_dp_desc msm_dp_desc_sdm845[] = {\n+\t{ .io_start = 0x0ae90000, .id = MSM_DP_CONTROLLER_0 },\n+\t{}\n+};\n+\n static const struct msm_dp_desc msm_dp_desc_sc7180[] = {\n \t{ .io_start = 0x0ae90000, .id = MSM_DP_CONTROLLER_0, .wide_bus_supported = true },\n \t{}\n@@ -180,7 +185,7 @@ static const struct of_device_id msm_dp_dt_match[] = {\n \t{ .compatible = \"qcom,sc8180x-edp\", .data = &msm_dp_desc_sc8180x },\n \t{ .compatible = \"qcom,sc8280xp-dp\", .data = &msm_dp_desc_sc8280xp },\n \t{ .compatible = \"qcom,sc8280xp-edp\", .data = &msm_dp_desc_sc8280xp },\n-\t{ .compatible = \"qcom,sdm845-dp\", .data = &msm_dp_desc_sc7180 },\n+\t{ .compatible = \"qcom,sdm845-dp\", .data = &msm_dp_desc_sdm845 },\n \t{ .compatible = \"qcom,sm8350-dp\", .data = &msm_dp_desc_sc7180 },\n \t{ .compatible = \"qcom,sm8650-dp\", .data = &msm_dp_desc_sm8650 },\n \t{ .compatible = \"qcom,x1e80100-dp\", .data = &msm_dp_desc_x1e80100 },\ndiff --git a/drivers/gpu/drm/msm/dsi/phy/dsi_phy_10nm.c b/drivers/gpu/drm/msm/dsi/phy/dsi_phy_10nm.c\nindex 9812b4d69197..af2e30f3f842 100644\n--- a/drivers/gpu/drm/msm/dsi/phy/dsi_phy_10nm.c\n+++ b/drivers/gpu/drm/msm/dsi/phy/dsi_phy_10nm.c\n@@ -704,6 +704,13 @@ static int dsi_pll_10nm_init(struct msm_dsi_phy *phy)\n \t/* TODO: Remove this when we have proper display handover support */\n \tmsm_dsi_phy_pll_save_state(phy);\n \n+\t/*\n+\t * Store also proper vco_current_rate, because its value will be used in\n+\t * dsi_10nm_pll_restore_state().\n+\t */\n+\tif (!dsi_pll_10nm_vco_recalc_rate(&pll_10nm->clk_hw, VCO_REF_CLK_RATE))\n+\t\tpll_10nm->vco_current_rate = pll_10nm->phy->cfg->min_pll_rate;\n+\n \treturn 0;\n }\n \ndiff --git a/drivers/gpu/drm/msm/msm_debugfs.c b/drivers/gpu/drm/msm/msm_debugfs.c\nindex 7ab607252d18..6af72162cda4 100644\n--- a/drivers/gpu/drm/msm/msm_debugfs.c\n+++ b/drivers/gpu/drm/msm/msm_debugfs.c\n@@ -208,6 +208,35 @@ DEFINE_DEBUGFS_ATTRIBUTE(shrink_fops,\n \t\t\t shrink_get, shrink_set,\n \t\t\t \"0x%08llx\\n\");\n \n+/*\n+ * Return the number of microseconds to wait until stall-on-fault is\n+ * re-enabled. If 0 then it is already enabled or will be re-enabled on the\n+ * next submit (unless there's a leftover devcoredump). This is useful for\n+ * kernel tests that intentionally produce a fault and check the devcoredump to\n+ * wait until the cooldown period is over.\n+ */\n+\n+static int\n+stall_reenable_time_get(void *data, u64 *val)\n+{\n+\tstruct msm_drm_private *priv = data;\n+\tunsigned long irq_flags;\n+\n+\tspin_lock_irqsave(&priv->fault_stall_lock, irq_flags);\n+\n+\tif (priv->stall_enabled)\n+\t\t*val = 0;\n+\telse\n+\t\t*val = max(ktime_us_delta(priv->stall_reenable_time, ktime_get()), 0);\n+\n+\tspin_unlock_irqrestore(&priv->fault_stall_lock, irq_flags);\n+\n+\treturn 0;\n+}\n+\n+DEFINE_DEBUGFS_ATTRIBUTE(stall_reenable_time_fops,\n+\t\t\t stall_reenable_time_get, NULL,\n+\t\t\t \"%lld\\n\");\n \n static int msm_gem_show(struct seq_file *m, void *arg)\n {\n@@ -319,6 +348,9 @@ static void msm_debugfs_gpu_init(struct drm_minor *minor)\n \tdebugfs_create_bool(\"disable_err_irq\", 0600, minor->debugfs_root,\n \t\t&priv->disable_err_irq);\n \n+\tdebugfs_create_file(\"stall_reenable_time_us\", 0400, minor->debugfs_root,\n+\t\tpriv, &stall_reenable_time_fops);\n+\n \tgpu_devfreq = debugfs_create_dir(\"devfreq\", minor->debugfs_root);\n \n \tdebugfs_create_bool(\"idle_clamp\",0600, gpu_devfreq,\ndiff --git a/drivers/gpu/drm/msm/msm_drv.c b/drivers/gpu/drm/msm/msm_drv.c\nindex f316e6776f67..d007687c2446 100644\n--- a/drivers/gpu/drm/msm/msm_drv.c\n+++ b/drivers/gpu/drm/msm/msm_drv.c\n@@ -245,6 +245,10 @@ static int msm_drm_init(struct device *dev, const struct drm_driver *drv)\n \tdrm_gem_lru_init(&priv->lru.willneed, &priv->lru.lock);\n \tdrm_gem_lru_init(&priv->lru.dontneed, &priv->lru.lock);\n \n+\t/* Initialize stall-on-fault */\n+\tspin_lock_init(&priv->fault_stall_lock);\n+\tpriv->stall_enabled = true;\n+\n \t/* Teach lockdep about lock ordering wrt. shrinker: */\n \tfs_reclaim_acquire(GFP_KERNEL);\n \tmight_lock(&priv->lru.lock);\n@@ -926,7 +930,7 @@ static const struct drm_driver msm_driver = {\n  * is no external component that we need to add since LVDS is within MDP4\n  * itself.\n  */\n-static int add_components_mdp(struct device *master_dev,\n+static int add_mdp_components(struct device *master_dev,\n \t\t\t      struct component_match **matchptr)\n {\n \tstruct device_node *np = master_dev->of_node;\n@@ -1030,7 +1034,7 @@ static int add_gpu_components(struct device *dev,\n \tif (!np)\n \t\treturn 0;\n \n-\tif (of_device_is_available(np))\n+\tif (of_device_is_available(np) && adreno_has_gpu(np))\n \t\tdrm_of_component_match_add(dev, matchptr, component_compare_of, np);\n \n \tof_node_put(np);\n@@ -1071,7 +1075,7 @@ int msm_drv_probe(struct device *master_dev,\n \n \t/* Add mdp components if we have KMS. */\n \tif (kms_init) {\n-\t\tret = add_components_mdp(master_dev, &match);\n+\t\tret = add_mdp_components(master_dev, &match);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}\ndiff --git a/drivers/gpu/drm/msm/msm_drv.h b/drivers/gpu/drm/msm/msm_drv.h\nindex a65077855201..c8afb1ea6040 100644\n--- a/drivers/gpu/drm/msm/msm_drv.h\n+++ b/drivers/gpu/drm/msm/msm_drv.h\n@@ -222,6 +222,29 @@ struct msm_drm_private {\n \t * the sw hangcheck mechanism.\n \t */\n \tbool disable_err_irq;\n+\n+\t/**\n+\t * @fault_stall_lock:\n+\t *\n+\t * Serialize changes to stall-on-fault state.\n+\t */\n+\tspinlock_t fault_stall_lock;\n+\n+\t/**\n+\t * @fault_stall_reenable_time:\n+\t *\n+\t * If stall_enabled is false, when to reenable stall-on-fault.\n+\t * Protected by @fault_stall_lock.\n+\t */\n+\tktime_t stall_reenable_time;\n+\n+\t/**\n+\t * @stall_enabled:\n+\t *\n+\t * Whether stall-on-fault is currently enabled. Protected by\n+\t * @fault_stall_lock.\n+\t */\n+\tbool stall_enabled;\n };\n \n const struct msm_format *mdp_get_format(struct msm_kms *kms, uint32_t format, uint64_t modifier);\ndiff --git a/drivers/gpu/drm/msm/msm_gem_submit.c b/drivers/gpu/drm/msm/msm_gem_submit.c\nindex 3e9aa2cc38ef..d4f71bb54e84 100644\n--- a/drivers/gpu/drm/msm/msm_gem_submit.c\n+++ b/drivers/gpu/drm/msm/msm_gem_submit.c\n@@ -85,6 +85,15 @@ void __msm_gem_submit_destroy(struct kref *kref)\n \t\t\tcontainer_of(kref, struct msm_gem_submit, ref);\n \tunsigned i;\n \n+\t/*\n+\t * In error paths, we could unref the submit without calling\n+\t * drm_sched_entity_push_job(), so msm_job_free() will never\n+\t * get called.  Since drm_sched_job_cleanup() will NULL out\n+\t * s_fence, we can use that to detect this case.\n+\t */\n+\tif (submit->base.s_fence)\n+\t\tdrm_sched_job_cleanup(&submit->base);\n+\n \tif (submit->fence_id) {\n \t\tspin_lock(&submit->queue->idr_lock);\n \t\tidr_remove(&submit->queue->fence_idr, submit->fence_id);\n@@ -649,6 +658,7 @@ int msm_ioctl_gem_submit(struct drm_device *dev, void *data,\n \tstruct msm_ringbuffer *ring;\n \tstruct msm_submit_post_dep *post_deps = NULL;\n \tstruct drm_syncobj **syncobjs_to_reset = NULL;\n+\tstruct sync_file *sync_file = NULL;\n \tint out_fence_fd = -1;\n \tunsigned i;\n \tint ret;\n@@ -858,7 +868,7 @@ int msm_ioctl_gem_submit(struct drm_device *dev, void *data,\n \t}\n \n \tif (ret == 0 && args->flags & MSM_SUBMIT_FENCE_FD_OUT) {\n-\t\tstruct sync_file *sync_file = sync_file_create(submit->user_fence);\n+\t\tsync_file = sync_file_create(submit->user_fence);\n \t\tif (!sync_file) {\n \t\t\tret = -ENOMEM;\n \t\t} else {\n@@ -892,8 +902,11 @@ int msm_ioctl_gem_submit(struct drm_device *dev, void *data,\n out_unlock:\n \tmutex_unlock(&queue->lock);\n out_post_unlock:\n-\tif (ret && (out_fence_fd >= 0))\n+\tif (ret && (out_fence_fd >= 0)) {\n \t\tput_unused_fd(out_fence_fd);\n+\t\tif (sync_file)\n+\t\t\tfput(sync_file->file);\n+\t}\n \n \tif (!IS_ERR_OR_NULL(submit)) {\n \t\tmsm_gem_submit_put(submit);\ndiff --git a/drivers/gpu/drm/msm/msm_gpu.c b/drivers/gpu/drm/msm/msm_gpu.c\nindex 197871fdf508..3947f7ba1421 100644\n--- a/drivers/gpu/drm/msm/msm_gpu.c\n+++ b/drivers/gpu/drm/msm/msm_gpu.c\n@@ -257,7 +257,8 @@ static void msm_gpu_crashstate_get_bo(struct msm_gpu_state *state,\n }\n \n static void msm_gpu_crashstate_capture(struct msm_gpu *gpu,\n-\t\tstruct msm_gem_submit *submit, char *comm, char *cmd)\n+\t\tstruct msm_gem_submit *submit, struct msm_gpu_fault_info *fault_info,\n+\t\tchar *comm, char *cmd)\n {\n \tstruct msm_gpu_state *state;\n \n@@ -276,7 +277,8 @@ static void msm_gpu_crashstate_capture(struct msm_gpu *gpu,\n \t/* Fill in the additional crash state information */\n \tstate->comm = kstrdup(comm, GFP_KERNEL);\n \tstate->cmd = kstrdup(cmd, GFP_KERNEL);\n-\tstate->fault_info = gpu->fault_info;\n+\tif (fault_info)\n+\t\tstate->fault_info = *fault_info;\n \n \tif (submit) {\n \t\tint i;\n@@ -308,7 +310,8 @@ static void msm_gpu_crashstate_capture(struct msm_gpu *gpu,\n }\n #else\n static void msm_gpu_crashstate_capture(struct msm_gpu *gpu,\n-\t\tstruct msm_gem_submit *submit, char *comm, char *cmd)\n+\t\tstruct msm_gem_submit *submit, struct msm_gpu_fault_info *fault_info,\n+\t\tchar *comm, char *cmd)\n {\n }\n #endif\n@@ -405,7 +408,7 @@ static void recover_worker(struct kthread_work *work)\n \n \t/* Record the crash state */\n \tpm_runtime_get_sync(&gpu->pdev->dev);\n-\tmsm_gpu_crashstate_capture(gpu, submit, comm, cmd);\n+\tmsm_gpu_crashstate_capture(gpu, submit, NULL, comm, cmd);\n \n \tkfree(cmd);\n \tkfree(comm);\n@@ -459,9 +462,8 @@ static void recover_worker(struct kthread_work *work)\n \tmsm_gpu_retire(gpu);\n }\n \n-static void fault_worker(struct kthread_work *work)\n+void msm_gpu_fault_crashstate_capture(struct msm_gpu *gpu, struct msm_gpu_fault_info *fault_info)\n {\n-\tstruct msm_gpu *gpu = container_of(work, struct msm_gpu, fault_work);\n \tstruct msm_gem_submit *submit;\n \tstruct msm_ringbuffer *cur_ring = gpu->funcs->active_ring(gpu);\n \tchar *comm = NULL, *cmd = NULL;\n@@ -484,16 +486,13 @@ static void fault_worker(struct kthread_work *work)\n \n \t/* Record the crash state */\n \tpm_runtime_get_sync(&gpu->pdev->dev);\n-\tmsm_gpu_crashstate_capture(gpu, submit, comm, cmd);\n+\tmsm_gpu_crashstate_capture(gpu, submit, fault_info, comm, cmd);\n \tpm_runtime_put_sync(&gpu->pdev->dev);\n \n \tkfree(cmd);\n \tkfree(comm);\n \n resume_smmu:\n-\tmemset(&gpu->fault_info, 0, sizeof(gpu->fault_info));\n-\tgpu->aspace->mmu->funcs->resume_translation(gpu->aspace->mmu);\n-\n \tmutex_unlock(&gpu->lock);\n }\n \n@@ -882,7 +881,6 @@ int msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,\n \tinit_waitqueue_head(&gpu->retire_event);\n \tkthread_init_work(&gpu->retire_work, retire_worker);\n \tkthread_init_work(&gpu->recover_work, recover_worker);\n-\tkthread_init_work(&gpu->fault_work, fault_worker);\n \n \tpriv->hangcheck_period = DRM_MSM_HANGCHECK_DEFAULT_PERIOD;\n \ndiff --git a/drivers/gpu/drm/msm/msm_gpu.h b/drivers/gpu/drm/msm/msm_gpu.h\nindex e25009150579..5bf7cd985b9c 100644\n--- a/drivers/gpu/drm/msm/msm_gpu.h\n+++ b/drivers/gpu/drm/msm/msm_gpu.h\n@@ -253,12 +253,6 @@ struct msm_gpu {\n #define DRM_MSM_HANGCHECK_PROGRESS_RETRIES 3\n \tstruct timer_list hangcheck_timer;\n \n-\t/* Fault info for most recent iova fault: */\n-\tstruct msm_gpu_fault_info fault_info;\n-\n-\t/* work for handling GPU ioval faults: */\n-\tstruct kthread_work fault_work;\n-\n \t/* work for handling GPU recovery: */\n \tstruct kthread_work recover_work;\n \n@@ -668,6 +662,7 @@ msm_gpu_create_private_address_space(struct msm_gpu *gpu, struct task_struct *ta\n void msm_gpu_cleanup(struct msm_gpu *gpu);\n \n struct msm_gpu *adreno_load_gpu(struct drm_device *dev);\n+bool adreno_has_gpu(struct device_node *node);\n void __init adreno_register(void);\n void __exit adreno_unregister(void);\n \n@@ -705,6 +700,8 @@ static inline void msm_gpu_crashstate_put(struct msm_gpu *gpu)\n \tmutex_unlock(&gpu->lock);\n }\n \n+void msm_gpu_fault_crashstate_capture(struct msm_gpu *gpu, struct msm_gpu_fault_info *fault_info);\n+\n /*\n  * Simple macro to semi-cleanly add the MAP_PRIV flag for targets that can\n  * support expanded privileges\ndiff --git a/drivers/gpu/drm/msm/msm_iommu.c b/drivers/gpu/drm/msm/msm_iommu.c\nindex fd73dcd3f30e..739ce2c283a4 100644\n--- a/drivers/gpu/drm/msm/msm_iommu.c\n+++ b/drivers/gpu/drm/msm/msm_iommu.c\n@@ -345,7 +345,6 @@ static int msm_gpu_fault_handler(struct iommu_domain *domain, struct device *dev\n \t\tunsigned long iova, int flags, void *arg)\n {\n \tstruct msm_iommu *iommu = arg;\n-\tstruct msm_mmu *mmu = &iommu->base;\n \tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(iommu->base.dev);\n \tstruct adreno_smmu_fault_info info, *ptr = NULL;\n \n@@ -359,9 +358,6 @@ static int msm_gpu_fault_handler(struct iommu_domain *domain, struct device *dev\n \n \tpr_warn_ratelimited(\"*** fault: iova=%16lx, flags=%d\\n\", iova, flags);\n \n-\tif (mmu->funcs->resume_translation)\n-\t\tmmu->funcs->resume_translation(mmu);\n-\n \treturn 0;\n }\n \n@@ -376,12 +372,12 @@ static int msm_disp_fault_handler(struct iommu_domain *domain, struct device *de\n \treturn -ENOSYS;\n }\n \n-static void msm_iommu_resume_translation(struct msm_mmu *mmu)\n+static void msm_iommu_set_stall(struct msm_mmu *mmu, bool enable)\n {\n \tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(mmu->dev);\n \n-\tif (adreno_smmu->resume_translation)\n-\t\tadreno_smmu->resume_translation(adreno_smmu->cookie, true);\n+\tif (adreno_smmu->set_stall)\n+\t\tadreno_smmu->set_stall(adreno_smmu->cookie, enable);\n }\n \n static void msm_iommu_detach(struct msm_mmu *mmu)\n@@ -431,7 +427,7 @@ static const struct msm_mmu_funcs funcs = {\n \t\t.map = msm_iommu_map,\n \t\t.unmap = msm_iommu_unmap,\n \t\t.destroy = msm_iommu_destroy,\n-\t\t.resume_translation = msm_iommu_resume_translation,\n+\t\t.set_stall = msm_iommu_set_stall,\n };\n \n struct msm_mmu *msm_iommu_new(struct device *dev, unsigned long quirks)\ndiff --git a/drivers/gpu/drm/msm/msm_mmu.h b/drivers/gpu/drm/msm/msm_mmu.h\nindex daf91529e02b..0c694907140d 100644\n--- a/drivers/gpu/drm/msm/msm_mmu.h\n+++ b/drivers/gpu/drm/msm/msm_mmu.h\n@@ -15,7 +15,7 @@ struct msm_mmu_funcs {\n \t\t\tsize_t len, int prot);\n \tint (*unmap)(struct msm_mmu *mmu, uint64_t iova, size_t len);\n \tvoid (*destroy)(struct msm_mmu *mmu);\n-\tvoid (*resume_translation)(struct msm_mmu *mmu);\n+\tvoid (*set_stall)(struct msm_mmu *mmu, bool enable);\n };\n \n enum msm_mmu_type {\ndiff --git a/drivers/gpu/drm/msm/registers/adreno/adreno_pm4.xml b/drivers/gpu/drm/msm/registers/adreno/adreno_pm4.xml\nindex 5a6ae9fc3194..462713401622 100644\n--- a/drivers/gpu/drm/msm/registers/adreno/adreno_pm4.xml\n+++ b/drivers/gpu/drm/msm/registers/adreno/adreno_pm4.xml\n@@ -2255,7 +2255,8 @@ opcode: CP_LOAD_STATE4 (30) (4 dwords)\n \t<reg32 offset=\"0\" name=\"0\">\n \t\t<bitfield name=\"CLEAR_ON_CHIP_TS\" pos=\"0\" type=\"boolean\"/>\n \t\t<bitfield name=\"CLEAR_RESOURCE_TABLE\" pos=\"1\" type=\"boolean\"/>\n-\t\t<bitfield name=\"CLEAR_GLOBAL_LOCAL_TS\" pos=\"2\" type=\"boolean\"/>\n+\t\t<bitfield name=\"CLEAR_BV_BR_COUNTER\" pos=\"2\" type=\"boolean\"/>\n+\t\t<bitfield name=\"RESET_GLOBAL_LOCAL_TS\" pos=\"3\" type=\"boolean\"/>\n \t</reg32>\n </domain>\n \ndiff --git a/drivers/gpu/drm/msm/registers/gen_header.py b/drivers/gpu/drm/msm/registers/gen_header.py\nindex 3926485bb197..a409404627c7 100644\n--- a/drivers/gpu/drm/msm/registers/gen_header.py\n+++ b/drivers/gpu/drm/msm/registers/gen_header.py\n@@ -11,6 +11,7 @@ import collections\n import argparse\n import time\n import datetime\n+import re\n \n class Error(Exception):\n \tdef __init__(self, message):\n@@ -877,13 +878,14 @@ The rules-ng-ng source files this header was generated from are:\n \"\"\")\n \tmaxlen = 0\n \tfor filepath in p.xml_files:\n-\t\tmaxlen = max(maxlen, len(filepath))\n+\t\tnew_filepath = re.sub(\"^.+drivers\",\"drivers\",filepath)\n+\t\tmaxlen = max(maxlen, len(new_filepath))\n \tfor filepath in p.xml_files:\n-\t\tpad = \" \" * (maxlen - len(filepath))\n+\t\tpad = \" \" * (maxlen - len(new_filepath))\n \t\tfilesize = str(os.path.getsize(filepath))\n \t\tfilesize = \" \" * (7 - len(filesize)) + filesize\n \t\tfiletime = time.ctime(os.path.getmtime(filepath))\n-\t\tprint(\"- \" + filepath + pad + \" (\" + filesize + \" bytes, from \" + filetime + \")\")\n+\t\tprint(\"- \" + new_filepath + pad + \" (\" + filesize + \" bytes, from <stripped>)\")\n \tif p.copyright_year:\n \t\tcurrent_year = str(datetime.date.today().year)\n \t\tprint()\ndiff --git a/drivers/gpu/drm/nouveau/nouveau_backlight.c b/drivers/gpu/drm/nouveau/nouveau_backlight.c\nindex d47442125fa1..9aae26eb7d8f 100644\n--- a/drivers/gpu/drm/nouveau/nouveau_backlight.c\n+++ b/drivers/gpu/drm/nouveau/nouveau_backlight.c\n@@ -42,7 +42,7 @@\n #include \"nouveau_acpi.h\"\n \n static struct ida bl_ida;\n-#define BL_NAME_SIZE 15 // 12 for name + 2 for digits + 1 for '\\0'\n+#define BL_NAME_SIZE 24 // 12 for name + 11 for digits + 1 for '\\0'\n \n static bool\n nouveau_get_backlight_name(char backlight_name[BL_NAME_SIZE],\ndiff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c\nindex 5acb98d137bd..9d06ff722fea 100644\n--- a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c\n+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/rpc.c\n@@ -637,12 +637,18 @@ r535_gsp_rpc_push(struct nvkm_gsp *gsp, void *payload,\n \tif (payload_size > max_payload_size) {\n \t\tconst u32 fn = rpc->function;\n \t\tu32 remain_payload_size = payload_size;\n+\t\tvoid *next;\n \n-\t\t/* Adjust length, and send initial RPC. */\n-\t\trpc->length = sizeof(*rpc) + max_payload_size;\n-\t\tmsg->checksum = rpc->length;\n+\t\t/* Send initial RPC. */\n+\t\tnext = r535_gsp_rpc_get(gsp, fn, max_payload_size);\n+\t\tif (IS_ERR(next)) {\n+\t\t\trepv = next;\n+\t\t\tgoto done;\n+\t\t}\n \n-\t\trepv = r535_gsp_rpc_send(gsp, payload, NVKM_GSP_RPC_REPLY_NOWAIT, 0);\n+\t\tmemcpy(next, payload, max_payload_size);\n+\n+\t\trepv = r535_gsp_rpc_send(gsp, next, NVKM_GSP_RPC_REPLY_NOWAIT, 0);\n \t\tif (IS_ERR(repv))\n \t\t\tgoto done;\n \n@@ -653,7 +659,6 @@ r535_gsp_rpc_push(struct nvkm_gsp *gsp, void *payload,\n \t\twhile (remain_payload_size) {\n \t\t\tu32 size = min(remain_payload_size,\n \t\t\t\t       max_payload_size);\n-\t\t\tvoid *next;\n \n \t\t\tnext = r535_gsp_rpc_get(gsp, NV_VGPU_MSG_FUNCTION_CONTINUATION_RECORD, size);\n \t\t\tif (IS_ERR(next)) {\n@@ -674,6 +679,8 @@ r535_gsp_rpc_push(struct nvkm_gsp *gsp, void *payload,\n \t\t/* Wait for reply. */\n \t\trepv = r535_gsp_rpc_handle_reply(gsp, fn, policy, payload_size +\n \t\t\t\t\t\t sizeof(*rpc));\n+\t\tif (!IS_ERR(repv))\n+\t\t\tkvfree(msg);\n \t} else {\n \t\trepv = r535_gsp_rpc_send(gsp, payload, policy, gsp_rpc_len);\n \t}\ndiff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/vmm.c b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/vmm.c\nindex 52f2e5f14517..f25ea610cd99 100644\n--- a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/vmm.c\n+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/vmm.c\n@@ -121,7 +121,7 @@ r535_mmu_vaspace_new(struct nvkm_vmm *vmm, u32 handle, bool external)\n \t\t\tpage_shift -= desc->bits;\n \n \t\t\tctrl->levels[i].physAddress = pd->pt[0]->addr;\n-\t\t\tctrl->levels[i].size = (1 << desc->bits) * desc->size;\n+\t\t\tctrl->levels[i].size = BIT_ULL(desc->bits) * desc->size;\n \t\t\tctrl->levels[i].aperture = 1;\n \t\t\tctrl->levels[i].pageShift = page_shift;\n \ndiff --git a/drivers/gpu/drm/solomon/ssd130x.c b/drivers/gpu/drm/solomon/ssd130x.c\nindex dd2006d51c7a..eec43d1a5595 100644\n--- a/drivers/gpu/drm/solomon/ssd130x.c\n+++ b/drivers/gpu/drm/solomon/ssd130x.c\n@@ -974,7 +974,7 @@ static void ssd130x_clear_screen(struct ssd130x_device *ssd130x, u8 *data_array)\n \n static void ssd132x_clear_screen(struct ssd130x_device *ssd130x, u8 *data_array)\n {\n-\tunsigned int columns = DIV_ROUND_UP(ssd130x->height, SSD132X_SEGMENT_WIDTH);\n+\tunsigned int columns = DIV_ROUND_UP(ssd130x->width, SSD132X_SEGMENT_WIDTH);\n \tunsigned int height = ssd130x->height;\n \n \tmemset(data_array, 0, columns * height);\ndiff --git a/drivers/gpu/drm/v3d/v3d_sched.c b/drivers/gpu/drm/v3d/v3d_sched.c\nindex 35f131a46d07..42df9d3567e7 100644\n--- a/drivers/gpu/drm/v3d/v3d_sched.c\n+++ b/drivers/gpu/drm/v3d/v3d_sched.c\n@@ -199,7 +199,6 @@ v3d_job_update_stats(struct v3d_job *job, enum v3d_queue queue)\n \tstruct v3d_dev *v3d = job->v3d;\n \tstruct v3d_file_priv *file = job->file->driver_priv;\n \tstruct v3d_stats *global_stats = &v3d->queue[queue].stats;\n-\tstruct v3d_stats *local_stats = &file->stats[queue];\n \tu64 now = local_clock();\n \tunsigned long flags;\n \n@@ -209,7 +208,12 @@ v3d_job_update_stats(struct v3d_job *job, enum v3d_queue queue)\n \telse\n \t\tpreempt_disable();\n \n-\tv3d_stats_update(local_stats, now);\n+\t/* Don't update the local stats if the file context has already closed */\n+\tif (file)\n+\t\tv3d_stats_update(&file->stats[queue], now);\n+\telse\n+\t\tdrm_dbg(&v3d->drm, \"The file descriptor was closed before job completion\\n\");\n+\n \tv3d_stats_update(global_stats, now);\n \n \tif (IS_ENABLED(CONFIG_LOCKDEP))\ndiff --git a/drivers/gpu/drm/xe/xe_gt.c b/drivers/gpu/drm/xe/xe_gt.c\nindex 0e5d243c9451..6c4cb9576fb6 100644\n--- a/drivers/gpu/drm/xe/xe_gt.c\n+++ b/drivers/gpu/drm/xe/xe_gt.c\n@@ -118,7 +118,7 @@ static void xe_gt_enable_host_l2_vram(struct xe_gt *gt)\n \t\txe_gt_mcr_multicast_write(gt, XE2_GAMREQSTRM_CTRL, reg);\n \t}\n \n-\txe_gt_mcr_multicast_write(gt, XEHPC_L3CLOS_MASK(3), 0x3);\n+\txe_gt_mcr_multicast_write(gt, XEHPC_L3CLOS_MASK(3), 0xF);\n \txe_force_wake_put(gt_to_fw(gt), fw_ref);\n }\n \ndiff --git a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\nindex 084cbdeba8ea..e1362e608146 100644\n--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n@@ -137,6 +137,14 @@ void xe_gt_tlb_invalidation_reset(struct xe_gt *gt)\n \tstruct xe_gt_tlb_invalidation_fence *fence, *next;\n \tint pending_seqno;\n \n+\t/*\n+\t * we can get here before the CTs are even initialized if we're wedging\n+\t * very early, in which case there are not going to be any pending\n+\t * fences so we can bail immediately.\n+\t */\n+\tif (!xe_guc_ct_initialized(&gt->uc.guc.ct))\n+\t\treturn;\n+\n \t/*\n \t * CT channel is already disabled at this point. No new TLB requests can\n \t * appear.\ndiff --git a/drivers/gpu/drm/xe/xe_guc_ct.c b/drivers/gpu/drm/xe/xe_guc_ct.c\nindex 2447de0ebedf..d0ac48d8f4f7 100644\n--- a/drivers/gpu/drm/xe/xe_guc_ct.c\n+++ b/drivers/gpu/drm/xe/xe_guc_ct.c\n@@ -514,6 +514,9 @@ void xe_guc_ct_disable(struct xe_guc_ct *ct)\n  */\n void xe_guc_ct_stop(struct xe_guc_ct *ct)\n {\n+\tif (!xe_guc_ct_initialized(ct))\n+\t\treturn;\n+\n \txe_guc_ct_set_state(ct, XE_GUC_CT_STATE_STOPPED);\n \tstop_g2h_handler(ct);\n }\n@@ -760,7 +763,7 @@ static int __guc_ct_send_locked(struct xe_guc_ct *ct, const u32 *action,\n \tu16 seqno;\n \tint ret;\n \n-\txe_gt_assert(gt, ct->state != XE_GUC_CT_STATE_NOT_INITIALIZED);\n+\txe_gt_assert(gt, xe_guc_ct_initialized(ct));\n \txe_gt_assert(gt, !g2h_len || !g2h_fence);\n \txe_gt_assert(gt, !num_g2h || !g2h_fence);\n \txe_gt_assert(gt, !g2h_len || num_g2h);\n@@ -1344,7 +1347,7 @@ static int g2h_read(struct xe_guc_ct *ct, u32 *msg, bool fast_path)\n \tu32 action;\n \tu32 *hxg;\n \n-\txe_gt_assert(gt, ct->state != XE_GUC_CT_STATE_NOT_INITIALIZED);\n+\txe_gt_assert(gt, xe_guc_ct_initialized(ct));\n \tlockdep_assert_held(&ct->fast_lock);\n \n \tif (ct->state == XE_GUC_CT_STATE_DISABLED)\ndiff --git a/drivers/gpu/drm/xe/xe_guc_ct.h b/drivers/gpu/drm/xe/xe_guc_ct.h\nindex 82c4ae458dda..582aac106469 100644\n--- a/drivers/gpu/drm/xe/xe_guc_ct.h\n+++ b/drivers/gpu/drm/xe/xe_guc_ct.h\n@@ -22,6 +22,11 @@ void xe_guc_ct_snapshot_print(struct xe_guc_ct_snapshot *snapshot, struct drm_pr\n void xe_guc_ct_snapshot_free(struct xe_guc_ct_snapshot *snapshot);\n void xe_guc_ct_print(struct xe_guc_ct *ct, struct drm_printer *p, bool want_ctb);\n \n+static inline bool xe_guc_ct_initialized(struct xe_guc_ct *ct)\n+{\n+\treturn ct->state != XE_GUC_CT_STATE_NOT_INITIALIZED;\n+}\n+\n static inline bool xe_guc_ct_enabled(struct xe_guc_ct *ct)\n {\n \treturn ct->state == XE_GUC_CT_STATE_ENABLED;\ndiff --git a/drivers/gpu/drm/xe/xe_guc_pc.c b/drivers/gpu/drm/xe/xe_guc_pc.c\nindex 18c623992035..3beaaa7b25c1 100644\n--- a/drivers/gpu/drm/xe/xe_guc_pc.c\n+++ b/drivers/gpu/drm/xe/xe_guc_pc.c\n@@ -1068,7 +1068,7 @@ int xe_guc_pc_start(struct xe_guc_pc *pc)\n \t\tgoto out;\n \t}\n \n-\tmemset(pc->bo->vmap.vaddr, 0, size);\n+\txe_map_memset(xe, &pc->bo->vmap, 0, 0, size);\n \tslpc_shared_data_write(pc, header.size, size);\n \n \tearlier = ktime_get();\ndiff --git a/drivers/gpu/drm/xe/xe_guc_submit.c b/drivers/gpu/drm/xe/xe_guc_submit.c\nindex 6d84a52b660a..9567f6700cf2 100644\n--- a/drivers/gpu/drm/xe/xe_guc_submit.c\n+++ b/drivers/gpu/drm/xe/xe_guc_submit.c\n@@ -1762,6 +1762,9 @@ int xe_guc_submit_reset_prepare(struct xe_guc *guc)\n {\n \tint ret;\n \n+\tif (!guc->submission_state.initialized)\n+\t\treturn 0;\n+\n \t/*\n \t * Using an atomic here rather than submission_state.lock as this\n \t * function can be called while holding the CT lock (engine reset",
    "stats": {
      "insertions": 598,
      "deletions": 200,
      "files": 72
    }
  },
  {
    "sha": "fba46a5d83ca8decb338722fb4899026d8d9ead2",
    "message": "maple_tree: fix MA_STATE_PREALLOC flag in mas_preallocate()\n\nTemporarily clear the preallocation flag when explicitly requesting\nallocations.  Pre-existing allocations are already counted against the\nrequest through mas_node_count_gfp(), but the allocations will not happen\nif the MA_STATE_PREALLOC flag is set.  This flag is meant to avoid\nre-allocating in bulk allocation mode, and to detect issues with\npreallocation calculations.\n\nThe MA_STATE_PREALLOC flag should also always be set on zero allocations\nso that detection of underflow allocations will print a WARN_ON() during\nconsumption.\n\nUser visible effect of this flaw is a WARN_ON() followed by a null pointer\ndereference when subsequent requests for larger number of nodes is\nignored, such as the vma merge retry in mmap_region() caused by drivers\naltering the vma flags (which happens in v6.6, at least)\n\nLink: https://lkml.kernel.org/r/20250616184521.3382795-3-Liam.Howlett@oracle.com\nFixes: 54a611b60590 (\"Maple Tree: add new data structure\")\nSigned-off-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nReported-by: Zhaoyang Huang <zhaoyang.huang@unisoc.com>\nReported-by: Hailong Liu <hailong.liu@oppo.com>\nLink: https://lore.kernel.org/all/1652f7eb-a51b-4fee-8058-c73af63bacd1@oppo.com/\nLink: https://lore.kernel.org/all/20250428184058.1416274-1-Liam.Howlett@oracle.com/\nLink: https://lore.kernel.org/all/20250429014754.1479118-1-Liam.Howlett@oracle.com/\nCc: Lorenzo Stoakes <lorenzo.stoakes@oracle.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Hailong Liu <hailong.liu@oppo.com>\nCc: zhangpeng.00@bytedance.com <zhangpeng.00@bytedance.com>\nCc: Steve Kang <Steve.Kang@unisoc.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Sidhartha Kumar <sidhartha.kumar@oracle.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
    "author": "Liam R. Howlett",
    "date": "2025-06-19T20:48:04-07:00",
    "files_changed": [
      "lib/maple_tree.c"
    ],
    "diff": "diff --git a/lib/maple_tree.c b/lib/maple_tree.c\nindex affe979bd14d..00524e55a21e 100644\n--- a/lib/maple_tree.c\n+++ b/lib/maple_tree.c\n@@ -5527,8 +5527,9 @@ int mas_preallocate(struct ma_state *mas, void *entry, gfp_t gfp)\n \tmas->store_type = mas_wr_store_type(&wr_mas);\n \trequest = mas_prealloc_calc(&wr_mas, entry);\n \tif (!request)\n-\t\treturn ret;\n+\t\tgoto set_flag;\n \n+\tmas->mas_flags &= ~MA_STATE_PREALLOC;\n \tmas_node_count_gfp(mas, request, gfp);\n \tif (mas_is_err(mas)) {\n \t\tmas_set_alloc_req(mas, 0);\n@@ -5538,6 +5539,7 @@ int mas_preallocate(struct ma_state *mas, void *entry, gfp_t gfp)\n \t\treturn ret;\n \t}\n \n+set_flag:\n \tmas->mas_flags |= MA_STATE_PREALLOC;\n \treturn ret;\n }",
    "stats": {
      "insertions": 3,
      "deletions": 1,
      "files": 1
    }
  }
]
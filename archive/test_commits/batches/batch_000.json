[
  {
    "sha": "3f31a806a62e44f7498e2d17719c03f816553f11",
    "message": "Merge tag 'mm-hotfixes-stable-2025-07-11-16-16' of git://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm\n\nPull misc fixes from Andrew Morton:\n \"19 hotfixes. A whopping 16 are cc:stable and the remainder address\n  post-6.15 issues or aren't considered necessary for -stable kernels.\n\n  14 are for MM.  Three gdb-script fixes and a kallsyms build fix\"\n\n* tag 'mm-hotfixes-stable-2025-07-11-16-16' of git://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm:\n  Revert \"sched/numa: add statistics of numa balance task\"\n  mm: fix the inaccurate memory statistics issue for users\n  mm/damon: fix divide by zero in damon_get_intervals_score()\n  samples/damon: fix damon sample mtier for start failure\n  samples/damon: fix damon sample wsse for start failure\n  samples/damon: fix damon sample prcl for start failure\n  kasan: remove kasan_find_vm_area() to prevent possible deadlock\n  scripts: gdb: vfs: support external dentry names\n  mm/migrate: fix do_pages_stat in compat mode\n  mm/damon/core: handle damon_call_control as normal under kdmond deactivation\n  mm/rmap: fix potential out-of-bounds page table access during batched unmap\n  mm/hugetlb: don't crash when allocating a folio if there are no resv\n  scripts/gdb: de-reference per-CPU MCE interrupts\n  scripts/gdb: fix interrupts.py after maple tree conversion\n  maple_tree: fix mt_destroy_walk() on root leaf node\n  mm/vmalloc: leave lazy MMU mode on PTE mapping error\n  scripts/gdb: fix interrupts display after MCP on x86\n  lib/alloc_tag: do not acquire non-existent lock in alloc_tag_top_users()\n  kallsyms: fix build without execinfo",
    "author": "Linus Torvalds",
    "date": "2025-07-12T10:30:47-07:00",
    "files_changed": [
      "fs/proc/task_mmu.c",
      "include/linux/mm.h",
      "include/linux/sched.h",
      "include/linux/vm_event_item.h",
      "kernel/sched/core.c",
      "kernel/sched/debug.c",
      "lib/alloc_tag.c",
      "lib/maple_tree.c",
      "mm/damon/core.c",
      "mm/hugetlb.c",
      "mm/kasan/report.c",
      "mm/memcontrol.c",
      "mm/migrate.c",
      "mm/rmap.c",
      "mm/vmalloc.c",
      "mm/vmstat.c",
      "samples/damon/mtier.c",
      "samples/damon/prcl.c",
      "samples/damon/wsse.c",
      "tools/include/linux/kallsyms.h"
    ],
    "diff": "diff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 0cc35a14afbe..bd98ea3175ec 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1732,12 +1732,6 @@ The following nested keys are defined.\n \t  numa_hint_faults (npn)\n \t\tNumber of NUMA hinting faults.\n \n-\t  numa_task_migrated (npn)\n-\t\tNumber of task migration by NUMA balancing.\n-\n-\t  numa_task_swapped (npn)\n-\t\tNumber of task swap by NUMA balancing.\n-\n \t  pgdemote_kswapd\n \t\tNumber of pages demoted by kswapd.\n \ndiff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c\nindex 4be91eb6ea5c..751479eb128f 100644\n--- a/fs/proc/task_mmu.c\n+++ b/fs/proc/task_mmu.c\n@@ -36,9 +36,9 @@ void task_mem(struct seq_file *m, struct mm_struct *mm)\n \tunsigned long text, lib, swap, anon, file, shmem;\n \tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n \n-\tanon = get_mm_counter(mm, MM_ANONPAGES);\n-\tfile = get_mm_counter(mm, MM_FILEPAGES);\n-\tshmem = get_mm_counter(mm, MM_SHMEMPAGES);\n+\tanon = get_mm_counter_sum(mm, MM_ANONPAGES);\n+\tfile = get_mm_counter_sum(mm, MM_FILEPAGES);\n+\tshmem = get_mm_counter_sum(mm, MM_SHMEMPAGES);\n \n \t/*\n \t * Note: to minimize their overhead, mm maintains hiwater_vm and\n@@ -59,7 +59,7 @@ void task_mem(struct seq_file *m, struct mm_struct *mm)\n \ttext = min(text, mm->exec_vm << PAGE_SHIFT);\n \tlib = (mm->exec_vm << PAGE_SHIFT) - text;\n \n-\tswap = get_mm_counter(mm, MM_SWAPENTS);\n+\tswap = get_mm_counter_sum(mm, MM_SWAPENTS);\n \tSEQ_PUT_DEC(\"VmPeak:\\t\", hiwater_vm);\n \tSEQ_PUT_DEC(\" kB\\nVmSize:\\t\", total_vm);\n \tSEQ_PUT_DEC(\" kB\\nVmLck:\\t\", mm->locked_vm);\n@@ -92,12 +92,12 @@ unsigned long task_statm(struct mm_struct *mm,\n \t\t\t unsigned long *shared, unsigned long *text,\n \t\t\t unsigned long *data, unsigned long *resident)\n {\n-\t*shared = get_mm_counter(mm, MM_FILEPAGES) +\n-\t\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n+\t*shared = get_mm_counter_sum(mm, MM_FILEPAGES) +\n+\t\t\tget_mm_counter_sum(mm, MM_SHMEMPAGES);\n \t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n \t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n \t*data = mm->data_vm + mm->stack_vm;\n-\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n+\t*resident = *shared + get_mm_counter_sum(mm, MM_ANONPAGES);\n \treturn mm->total_vm;\n }\n \ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 0ef2ba0c667a..fa538feaa8d9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2568,6 +2568,11 @@ static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n \treturn percpu_counter_read_positive(&mm->rss_stat[member]);\n }\n \n+static inline unsigned long get_mm_counter_sum(struct mm_struct *mm, int member)\n+{\n+\treturn percpu_counter_sum_positive(&mm->rss_stat[member]);\n+}\n+\n void mm_trace_rss_stat(struct mm_struct *mm, int member);\n \n static inline void add_mm_counter(struct mm_struct *mm, int member, long value)\ndiff --git a/include/linux/sched.h b/include/linux/sched.h\nindex 4f78a64beb52..aa9c5be7a632 100644\n--- a/include/linux/sched.h\n+++ b/include/linux/sched.h\n@@ -548,10 +548,6 @@ struct sched_statistics {\n \tu64\t\t\t\tnr_failed_migrations_running;\n \tu64\t\t\t\tnr_failed_migrations_hot;\n \tu64\t\t\t\tnr_forced_migrations;\n-#ifdef CONFIG_NUMA_BALANCING\n-\tu64\t\t\t\tnuma_task_migrated;\n-\tu64\t\t\t\tnuma_task_swapped;\n-#endif\n \n \tu64\t\t\t\tnr_wakeups;\n \tu64\t\t\t\tnr_wakeups_sync;\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 91a3ce9a2687..9e15a088ba38 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -66,8 +66,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tNUMA_HINT_FAULTS,\n \t\tNUMA_HINT_FAULTS_LOCAL,\n \t\tNUMA_PAGE_MIGRATE,\n-\t\tNUMA_TASK_MIGRATE,\n-\t\tNUMA_TASK_SWAP,\n #endif\n #ifdef CONFIG_MIGRATION\n \t\tPGMIGRATE_SUCCESS, PGMIGRATE_FAIL,\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex ec68fc686bd7..81c6df746df1 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -3362,10 +3362,6 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n #ifdef CONFIG_NUMA_BALANCING\n static void __migrate_swap_task(struct task_struct *p, int cpu)\n {\n-\t__schedstat_inc(p->stats.numa_task_swapped);\n-\tcount_vm_numa_event(NUMA_TASK_SWAP);\n-\tcount_memcg_event_mm(p->mm, NUMA_TASK_SWAP);\n-\n \tif (task_on_rq_queued(p)) {\n \t\tstruct rq *src_rq, *dst_rq;\n \t\tstruct rq_flags srf, drf;\n@@ -7939,9 +7935,8 @@ int migrate_task_to(struct task_struct *p, int target_cpu)\n \tif (!cpumask_test_cpu(target_cpu, p->cpus_ptr))\n \t\treturn -EINVAL;\n \n-\t__schedstat_inc(p->stats.numa_task_migrated);\n-\tcount_vm_numa_event(NUMA_TASK_MIGRATE);\n-\tcount_memcg_event_mm(p->mm, NUMA_TASK_MIGRATE);\n+\t/* TODO: This is not properly updating schedstats */\n+\n \ttrace_sched_move_numa(p, curr_cpu, target_cpu);\n \treturn stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);\n }\ndiff --git a/kernel/sched/debug.c b/kernel/sched/debug.c\nindex 9d71baf08075..557246880a7e 100644\n--- a/kernel/sched/debug.c\n+++ b/kernel/sched/debug.c\n@@ -1210,10 +1210,6 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,\n \t\tP_SCHEDSTAT(nr_failed_migrations_running);\n \t\tP_SCHEDSTAT(nr_failed_migrations_hot);\n \t\tP_SCHEDSTAT(nr_forced_migrations);\n-#ifdef CONFIG_NUMA_BALANCING\n-\t\tP_SCHEDSTAT(numa_task_migrated);\n-\t\tP_SCHEDSTAT(numa_task_swapped);\n-#endif\n \t\tP_SCHEDSTAT(nr_wakeups);\n \t\tP_SCHEDSTAT(nr_wakeups_sync);\n \t\tP_SCHEDSTAT(nr_wakeups_migrate);\ndiff --git a/lib/alloc_tag.c b/lib/alloc_tag.c\nindex 3a74d63a959e..0142bc916f73 100644\n--- a/lib/alloc_tag.c\n+++ b/lib/alloc_tag.c\n@@ -135,6 +135,9 @@ size_t alloc_tag_top_users(struct codetag_bytes *tags, size_t count, bool can_sl\n \tstruct codetag_bytes n;\n \tunsigned int i, nr = 0;\n \n+\tif (IS_ERR_OR_NULL(alloc_tag_cttype))\n+\t\treturn 0;\n+\n \tif (can_sleep)\n \t\tcodetag_lock_module_list(alloc_tag_cttype, true);\n \telse if (!codetag_trylock_module_list(alloc_tag_cttype))\ndiff --git a/lib/maple_tree.c b/lib/maple_tree.c\nindex 00524e55a21e..ef66be963798 100644\n--- a/lib/maple_tree.c\n+++ b/lib/maple_tree.c\n@@ -5319,6 +5319,7 @@ static void mt_destroy_walk(struct maple_enode *enode, struct maple_tree *mt,\n \tstruct maple_enode *start;\n \n \tif (mte_is_leaf(enode)) {\n+\t\tmte_set_node_dead(enode);\n \t\tnode->type = mte_node_type(enode);\n \t\tgoto free_leaf;\n \t}\ndiff --git a/mm/damon/core.c b/mm/damon/core.c\nindex b217e0120e09..979b29e16ef4 100644\n--- a/mm/damon/core.c\n+++ b/mm/damon/core.c\n@@ -1449,6 +1449,7 @@ static unsigned long damon_get_intervals_score(struct damon_ctx *c)\n \t\t}\n \t}\n \ttarget_access_events = max_access_events * goal_bp / 10000;\n+\ttarget_access_events = target_access_events ? : 1;\n \treturn access_events * 10000 / target_access_events;\n }\n \n@@ -2355,9 +2356,8 @@ static void kdamond_usleep(unsigned long usecs)\n  *\n  * If there is a &struct damon_call_control request that registered via\n  * &damon_call() on @ctx, do or cancel the invocation of the function depending\n- * on @cancel.  @cancel is set when the kdamond is deactivated by DAMOS\n- * watermarks, or the kdamond is already out of the main loop and therefore\n- * will be terminated.\n+ * on @cancel.  @cancel is set when the kdamond is already out of the main loop\n+ * and therefore will be terminated.\n  */\n static void kdamond_call(struct damon_ctx *ctx, bool cancel)\n {\n@@ -2405,7 +2405,7 @@ static int kdamond_wait_activation(struct damon_ctx *ctx)\n \t\tif (ctx->callback.after_wmarks_check &&\n \t\t\t\tctx->callback.after_wmarks_check(ctx))\n \t\t\tbreak;\n-\t\tkdamond_call(ctx, true);\n+\t\tkdamond_call(ctx, false);\n \t\tdamos_walk_cancel(ctx);\n \t}\n \treturn -EBUSY;\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 9dc95eac558c..a0d285d20992 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -2340,12 +2340,15 @@ struct folio *alloc_hugetlb_folio_reserve(struct hstate *h, int preferred_nid,\n \tstruct folio *folio;\n \n \tspin_lock_irq(&hugetlb_lock);\n+\tif (!h->resv_huge_pages) {\n+\t\tspin_unlock_irq(&hugetlb_lock);\n+\t\treturn NULL;\n+\t}\n+\n \tfolio = dequeue_hugetlb_folio_nodemask(h, gfp_mask, preferred_nid,\n \t\t\t\t\t       nmask);\n-\tif (folio) {\n-\t\tVM_BUG_ON(!h->resv_huge_pages);\n+\tif (folio)\n \t\th->resv_huge_pages--;\n-\t}\n \n \tspin_unlock_irq(&hugetlb_lock);\n \treturn folio;\ndiff --git a/mm/kasan/report.c b/mm/kasan/report.c\nindex 8357e1a33699..b0877035491f 100644\n--- a/mm/kasan/report.c\n+++ b/mm/kasan/report.c\n@@ -370,36 +370,6 @@ static inline bool init_task_stack_addr(const void *addr)\n \t\t\tsizeof(init_thread_union.stack));\n }\n \n-/*\n- * This function is invoked with report_lock (a raw_spinlock) held. A\n- * PREEMPT_RT kernel cannot call find_vm_area() as it will acquire a sleeping\n- * rt_spinlock.\n- *\n- * For !RT kernel, the PROVE_RAW_LOCK_NESTING config option will print a\n- * lockdep warning for this raw_spinlock -> spinlock dependency. This config\n- * option is enabled by default to ensure better test coverage to expose this\n- * kind of RT kernel problem. This lockdep splat, however, can be suppressed\n- * by using DEFINE_WAIT_OVERRIDE_MAP() if it serves a useful purpose and the\n- * invalid PREEMPT_RT case has been taken care of.\n- */\n-static inline struct vm_struct *kasan_find_vm_area(void *addr)\n-{\n-\tstatic DEFINE_WAIT_OVERRIDE_MAP(vmalloc_map, LD_WAIT_SLEEP);\n-\tstruct vm_struct *va;\n-\n-\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n-\t\treturn NULL;\n-\n-\t/*\n-\t * Suppress lockdep warning and fetch vmalloc area of the\n-\t * offending address.\n-\t */\n-\tlock_map_acquire_try(&vmalloc_map);\n-\tva = find_vm_area(addr);\n-\tlock_map_release(&vmalloc_map);\n-\treturn va;\n-}\n-\n static void print_address_description(void *addr, u8 tag,\n \t\t\t\t      struct kasan_report_info *info)\n {\n@@ -429,19 +399,8 @@ static void print_address_description(void *addr, u8 tag,\n \t}\n \n \tif (is_vmalloc_addr(addr)) {\n-\t\tstruct vm_struct *va = kasan_find_vm_area(addr);\n-\n-\t\tif (va) {\n-\t\t\tpr_err(\"The buggy address belongs to the virtual mapping at\\n\"\n-\t\t\t       \" [%px, %px) created by:\\n\"\n-\t\t\t       \" %pS\\n\",\n-\t\t\t       va->addr, va->addr + va->size, va->caller);\n-\t\t\tpr_err(\"\\n\");\n-\n-\t\t\tpage = vmalloc_to_page(addr);\n-\t\t} else {\n-\t\t\tpr_err(\"The buggy address %px belongs to a vmalloc virtual mapping\\n\", addr);\n-\t\t}\n+\t\tpr_err(\"The buggy address %px belongs to a vmalloc virtual mapping\\n\", addr);\n+\t\tpage = vmalloc_to_page(addr);\n \t}\n \n \tif (page) {\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 902da8a9c643..70fdeda1120b 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -474,8 +474,6 @@ static const unsigned int memcg_vm_event_stat[] = {\n \tNUMA_PAGE_MIGRATE,\n \tNUMA_PTE_UPDATES,\n \tNUMA_HINT_FAULTS,\n-\tNUMA_TASK_MIGRATE,\n-\tNUMA_TASK_SWAP,\n #endif\n };\n \ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 8cf0f9c9599d..2c88f3b33833 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2399,6 +2399,7 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \n static int get_compat_pages_array(const void __user *chunk_pages[],\n \t\t\t\t  const void __user * __user *pages,\n+\t\t\t\t  unsigned long chunk_offset,\n \t\t\t\t  unsigned long chunk_nr)\n {\n \tcompat_uptr_t __user *pages32 = (compat_uptr_t __user *)pages;\n@@ -2406,7 +2407,7 @@ static int get_compat_pages_array(const void __user *chunk_pages[],\n \tint i;\n \n \tfor (i = 0; i < chunk_nr; i++) {\n-\t\tif (get_user(p, pages32 + i))\n+\t\tif (get_user(p, pages32 + chunk_offset + i))\n \t\t\treturn -EFAULT;\n \t\tchunk_pages[i] = compat_ptr(p);\n \t}\n@@ -2425,27 +2426,28 @@ static int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n #define DO_PAGES_STAT_CHUNK_NR 16UL\n \tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n \tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n+\tunsigned long chunk_offset = 0;\n \n \twhile (nr_pages) {\n \t\tunsigned long chunk_nr = min(nr_pages, DO_PAGES_STAT_CHUNK_NR);\n \n \t\tif (in_compat_syscall()) {\n \t\t\tif (get_compat_pages_array(chunk_pages, pages,\n-\t\t\t\t\t\t   chunk_nr))\n+\t\t\t\t\t\t   chunk_offset, chunk_nr))\n \t\t\t\tbreak;\n \t\t} else {\n-\t\t\tif (copy_from_user(chunk_pages, pages,\n+\t\t\tif (copy_from_user(chunk_pages, pages + chunk_offset,\n \t\t\t\t      chunk_nr * sizeof(*chunk_pages)))\n \t\t\t\tbreak;\n \t\t}\n \n \t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n \n-\t\tif (copy_to_user(status, chunk_status, chunk_nr * sizeof(*status)))\n+\t\tif (copy_to_user(status + chunk_offset, chunk_status,\n+\t\t\t\t chunk_nr * sizeof(*status)))\n \t\t\tbreak;\n \n-\t\tpages += chunk_nr;\n-\t\tstatus += chunk_nr;\n+\t\tchunk_offset += chunk_nr;\n \t\tnr_pages -= chunk_nr;\n \t}\n \treturn nr_pages ? -EFAULT : 0;\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex fb63d9256f09..1320b88fab74 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -1845,23 +1845,32 @@ void folio_remove_rmap_pud(struct folio *folio, struct page *page,\n #endif\n }\n \n-/* We support batch unmapping of PTEs for lazyfree large folios */\n-static inline bool can_batch_unmap_folio_ptes(unsigned long addr,\n-\t\t\tstruct folio *folio, pte_t *ptep)\n+static inline unsigned int folio_unmap_pte_batch(struct folio *folio,\n+\t\t\tstruct page_vma_mapped_walk *pvmw,\n+\t\t\tenum ttu_flags flags, pte_t pte)\n {\n \tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;\n-\tint max_nr = folio_nr_pages(folio);\n-\tpte_t pte = ptep_get(ptep);\n+\tunsigned long end_addr, addr = pvmw->address;\n+\tstruct vm_area_struct *vma = pvmw->vma;\n+\tunsigned int max_nr;\n+\n+\tif (flags & TTU_HWPOISON)\n+\t\treturn 1;\n+\tif (!folio_test_large(folio))\n+\t\treturn 1;\n \n+\t/* We may only batch within a single VMA and a single page table. */\n+\tend_addr = pmd_addr_end(addr, vma->vm_end);\n+\tmax_nr = (end_addr - addr) >> PAGE_SHIFT;\n+\n+\t/* We only support lazyfree batching for now ... */\n \tif (!folio_test_anon(folio) || folio_test_swapbacked(folio))\n-\t\treturn false;\n+\t\treturn 1;\n \tif (pte_unused(pte))\n-\t\treturn false;\n-\tif (pte_pfn(pte) != folio_pfn(folio))\n-\t\treturn false;\n+\t\treturn 1;\n \n-\treturn folio_pte_batch(folio, addr, ptep, pte, max_nr, fpb_flags, NULL,\n-\t\t\t       NULL, NULL) == max_nr;\n+\treturn folio_pte_batch(folio, addr, pvmw->pte, pte, max_nr, fpb_flags,\n+\t\t\t       NULL, NULL, NULL);\n }\n \n /*\n@@ -2024,9 +2033,7 @@ static bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,\n \t\t\tif (pte_dirty(pteval))\n \t\t\t\tfolio_mark_dirty(folio);\n \t\t} else if (likely(pte_present(pteval))) {\n-\t\t\tif (folio_test_large(folio) && !(flags & TTU_HWPOISON) &&\n-\t\t\t    can_batch_unmap_folio_ptes(address, folio, pvmw.pte))\n-\t\t\t\tnr_pages = folio_nr_pages(folio);\n+\t\t\tnr_pages = folio_unmap_pte_batch(folio, &pvmw, flags, pteval);\n \t\t\tend_addr = address + nr_pages * PAGE_SIZE;\n \t\t\tflush_cache_range(vma, address, end_addr);\n \n@@ -2206,13 +2213,16 @@ static bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,\n \t\t\thugetlb_remove_rmap(folio);\n \t\t} else {\n \t\t\tfolio_remove_rmap_ptes(folio, subpage, nr_pages, vma);\n-\t\t\tfolio_ref_sub(folio, nr_pages - 1);\n \t\t}\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_drain_local();\n-\t\tfolio_put(folio);\n-\t\t/* We have already batched the entire folio */\n-\t\tif (nr_pages > 1)\n+\t\tfolio_put_refs(folio, nr_pages);\n+\n+\t\t/*\n+\t\t * If we are sure that we batched the entire folio and cleared\n+\t\t * all PTEs, we can just optimize and stop right here.\n+\t\t */\n+\t\tif (nr_pages == folio_nr_pages(folio))\n \t\t\tgoto walk_done;\n \t\tcontinue;\n walk_abort:\ndiff --git a/mm/vmalloc.c b/mm/vmalloc.c\nindex ab986dd09b6a..6dbcdceecae1 100644\n--- a/mm/vmalloc.c\n+++ b/mm/vmalloc.c\n@@ -514,6 +514,7 @@ static int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,\n \t\tpgtbl_mod_mask *mask)\n {\n+\tint err = 0;\n \tpte_t *pte;\n \n \t/*\n@@ -530,12 +531,18 @@ static int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,\n \tdo {\n \t\tstruct page *page = pages[*nr];\n \n-\t\tif (WARN_ON(!pte_none(ptep_get(pte))))\n-\t\t\treturn -EBUSY;\n-\t\tif (WARN_ON(!page))\n-\t\t\treturn -ENOMEM;\n-\t\tif (WARN_ON(!pfn_valid(page_to_pfn(page))))\n-\t\t\treturn -EINVAL;\n+\t\tif (WARN_ON(!pte_none(ptep_get(pte)))) {\n+\t\t\terr = -EBUSY;\n+\t\t\tbreak;\n+\t\t}\n+\t\tif (WARN_ON(!page)) {\n+\t\t\terr = -ENOMEM;\n+\t\t\tbreak;\n+\t\t}\n+\t\tif (WARN_ON(!pfn_valid(page_to_pfn(page)))) {\n+\t\t\terr = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n \n \t\tset_pte_at(&init_mm, addr, pte, mk_pte(page, prot));\n \t\t(*nr)++;\n@@ -543,7 +550,8 @@ static int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,\n \n \tarch_leave_lazy_mmu_mode();\n \t*mask |= PGTBL_PTE_MODIFIED;\n-\treturn 0;\n+\n+\treturn err;\n }\n \n static int vmap_pages_pmd_range(pud_t *pud, unsigned long addr,\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 429ae5339bfe..a78d70ddeacd 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1346,8 +1346,6 @@ const char * const vmstat_text[] = {\n \t\"numa_hint_faults\",\n \t\"numa_hint_faults_local\",\n \t\"numa_pages_migrated\",\n-\t\"numa_task_migrated\",\n-\t\"numa_task_swapped\",\n #endif\n #ifdef CONFIG_MIGRATION\n \t\"pgmigrate_success\",\ndiff --git a/samples/damon/mtier.c b/samples/damon/mtier.c\nindex 36d2cd933f5a..c94254b77fc9 100644\n--- a/samples/damon/mtier.c\n+++ b/samples/damon/mtier.c\n@@ -164,8 +164,12 @@ static int damon_sample_mtier_enable_store(\n \tif (enable == enabled)\n \t\treturn 0;\n \n-\tif (enable)\n-\t\treturn damon_sample_mtier_start();\n+\tif (enable) {\n+\t\terr = damon_sample_mtier_start();\n+\t\tif (err)\n+\t\t\tenable = false;\n+\t\treturn err;\n+\t}\n \tdamon_sample_mtier_stop();\n \treturn 0;\n }\ndiff --git a/samples/damon/prcl.c b/samples/damon/prcl.c\nindex 056b1b21a0fe..5597e6a08ab2 100644\n--- a/samples/damon/prcl.c\n+++ b/samples/damon/prcl.c\n@@ -122,8 +122,12 @@ static int damon_sample_prcl_enable_store(\n \tif (enable == enabled)\n \t\treturn 0;\n \n-\tif (enable)\n-\t\treturn damon_sample_prcl_start();\n+\tif (enable) {\n+\t\terr = damon_sample_prcl_start();\n+\t\tif (err)\n+\t\t\tenable = false;\n+\t\treturn err;\n+\t}\n \tdamon_sample_prcl_stop();\n \treturn 0;\n }\ndiff --git a/samples/damon/wsse.c b/samples/damon/wsse.c\nindex 11be25803274..e20238a249e7 100644\n--- a/samples/damon/wsse.c\n+++ b/samples/damon/wsse.c\n@@ -102,8 +102,12 @@ static int damon_sample_wsse_enable_store(\n \tif (enable == enabled)\n \t\treturn 0;\n \n-\tif (enable)\n-\t\treturn damon_sample_wsse_start();\n+\tif (enable) {\n+\t\terr = damon_sample_wsse_start();\n+\t\tif (err)\n+\t\t\tenable = false;\n+\t\treturn err;\n+\t}\n \tdamon_sample_wsse_stop();\n \treturn 0;\n }\ndiff --git a/scripts/gdb/linux/constants.py.in b/scripts/gdb/linux/constants.py.in\nindex fd6bd69c5096..f795302ddfa8 100644\n--- a/scripts/gdb/linux/constants.py.in\n+++ b/scripts/gdb/linux/constants.py.in\n@@ -20,6 +20,7 @@\n #include <linux/of_fdt.h>\n #include <linux/page_ext.h>\n #include <linux/radix-tree.h>\n+#include <linux/maple_tree.h>\n #include <linux/slab.h>\n #include <linux/threads.h>\n #include <linux/vmalloc.h>\n@@ -93,6 +94,12 @@ LX_GDBPARSED(RADIX_TREE_MAP_SIZE)\n LX_GDBPARSED(RADIX_TREE_MAP_SHIFT)\n LX_GDBPARSED(RADIX_TREE_MAP_MASK)\n \n+/* linux/maple_tree.h */\n+LX_VALUE(MAPLE_NODE_SLOTS)\n+LX_VALUE(MAPLE_RANGE64_SLOTS)\n+LX_VALUE(MAPLE_ARANGE64_SLOTS)\n+LX_GDBPARSED(MAPLE_NODE_MASK)\n+\n /* linux/vmalloc.h */\n LX_VALUE(VM_IOREMAP)\n LX_VALUE(VM_ALLOC)\ndiff --git a/scripts/gdb/linux/interrupts.py b/scripts/gdb/linux/interrupts.py\nindex 616a5f26377a..f4f715a8f0e3 100644\n--- a/scripts/gdb/linux/interrupts.py\n+++ b/scripts/gdb/linux/interrupts.py\n@@ -7,7 +7,7 @@ import gdb\n from linux import constants\n from linux import cpus\n from linux import utils\n-from linux import radixtree\n+from linux import mapletree\n \n irq_desc_type = utils.CachedType(\"struct irq_desc\")\n \n@@ -23,12 +23,12 @@ def irqd_is_level(desc):\n def show_irq_desc(prec, irq):\n     text = \"\"\n \n-    desc = radixtree.lookup(gdb.parse_and_eval(\"&irq_desc_tree\"), irq)\n+    desc = mapletree.mtree_load(gdb.parse_and_eval(\"&sparse_irqs\"), irq)\n     if desc is None:\n         return text\n \n-    desc = desc.cast(irq_desc_type.get_type())\n-    if desc is None:\n+    desc = desc.cast(irq_desc_type.get_type().pointer())\n+    if desc == 0:\n         return text\n \n     if irq_settings_is_hidden(desc):\n@@ -110,7 +110,7 @@ def x86_show_mce(prec, var, pfx, desc):\n     pvar = gdb.parse_and_eval(var)\n     text = \"%*s: \" % (prec, pfx)\n     for cpu in cpus.each_online_cpu():\n-        text += \"%10u \" % (cpus.per_cpu(pvar, cpu))\n+        text += \"%10u \" % (cpus.per_cpu(pvar, cpu).dereference())\n     text += \"  %s\\n\" % (desc)\n     return text\n \n@@ -142,7 +142,7 @@ def x86_show_interupts(prec):\n \n     if constants.LX_CONFIG_X86_MCE:\n         text += x86_show_mce(prec, \"&mce_exception_count\", \"MCE\", \"Machine check exceptions\")\n-        text == x86_show_mce(prec, \"&mce_poll_count\", \"MCP\", \"Machine check polls\")\n+        text += x86_show_mce(prec, \"&mce_poll_count\", \"MCP\", \"Machine check polls\")\n \n     text += show_irq_err_count(prec)\n \n@@ -221,8 +221,8 @@ class LxInterruptList(gdb.Command):\n             gdb.write(\"CPU%-8d\" % cpu)\n         gdb.write(\"\\n\")\n \n-        if utils.gdb_eval_or_none(\"&irq_desc_tree\") is None:\n-            return\n+        if utils.gdb_eval_or_none(\"&sparse_irqs\") is None:\n+            raise gdb.GdbError(\"Unable to find the sparse IRQ tree, is CONFIG_SPARSE_IRQ enabled?\")\n \n         for irq in range(nr_irqs):\n             gdb.write(show_irq_desc(prec, irq))\ndiff --git a/scripts/gdb/linux/mapletree.py b/scripts/gdb/linux/mapletree.py\nnew file mode 100644\nindex 000000000000..d52d51c0a03f\n--- /dev/null\n+++ b/scripts/gdb/linux/mapletree.py\n@@ -0,0 +1,252 @@\n+# SPDX-License-Identifier: GPL-2.0\n+#\n+#  Maple tree helpers\n+#\n+# Copyright (c) 2025 Broadcom\n+#\n+# Authors:\n+#  Florian Fainelli <florian.fainelli@broadcom.com>\n+\n+import gdb\n+\n+from linux import utils\n+from linux import constants\n+from linux import xarray\n+\n+maple_tree_root_type = utils.CachedType(\"struct maple_tree\")\n+maple_node_type = utils.CachedType(\"struct maple_node\")\n+maple_enode_type = utils.CachedType(\"void\")\n+\n+maple_dense = 0\n+maple_leaf_64 = 1\n+maple_range_64 = 2\n+maple_arange_64 = 3\n+\n+class Mas(object):\n+    ma_active = 0\n+    ma_start = 1\n+    ma_root = 2\n+    ma_none = 3\n+    ma_pause = 4\n+    ma_overflow = 5\n+    ma_underflow = 6\n+    ma_error = 7\n+\n+    def __init__(self, mt, first, end):\n+        if mt.type == maple_tree_root_type.get_type().pointer():\n+            self.tree = mt.dereference()\n+        elif mt.type != maple_tree_root_type.get_type():\n+            raise gdb.GdbError(\"must be {} not {}\"\n+                               .format(maple_tree_root_type.get_type().pointer(), mt.type))\n+        self.tree = mt\n+        self.index = first\n+        self.last = end\n+        self.node = None\n+        self.status = self.ma_start\n+        self.min = 0\n+        self.max = -1\n+\n+    def is_start(self):\n+        # mas_is_start()\n+        return self.status == self.ma_start\n+\n+    def is_ptr(self):\n+        # mas_is_ptr()\n+        return self.status == self.ma_root\n+\n+    def is_none(self):\n+        # mas_is_none()\n+        return self.status == self.ma_none\n+\n+    def root(self):\n+        # mas_root()\n+        return self.tree['ma_root'].cast(maple_enode_type.get_type().pointer())\n+\n+    def start(self):\n+        # mas_start()\n+        if self.is_start() is False:\n+            return None\n+\n+        self.min = 0\n+        self.max = ~0\n+\n+        while True:\n+            self.depth = 0\n+            root = self.root()\n+            if xarray.xa_is_node(root):\n+                self.depth = 0\n+                self.status = self.ma_active\n+                self.node = mte_safe_root(root)\n+                self.offset = 0\n+                if mte_dead_node(self.node) is True:\n+                    continue\n+\n+                return None\n+\n+            self.node = None\n+            # Empty tree\n+            if root is None:\n+                self.status = self.ma_none\n+                self.offset = constants.LX_MAPLE_NODE_SLOTS\n+                return None\n+\n+            # Single entry tree\n+            self.status = self.ma_root\n+            self.offset = constants.LX_MAPLE_NODE_SLOTS\n+\n+            if self.index != 0:\n+                return None\n+\n+            return root\n+\n+        return None\n+\n+    def reset(self):\n+        # mas_reset()\n+        self.status = self.ma_start\n+        self.node = None\n+\n+def mte_safe_root(node):\n+    if node.type != maple_enode_type.get_type().pointer():\n+        raise gdb.GdbError(\"{} must be {} not {}\"\n+                           .format(mte_safe_root.__name__, maple_enode_type.get_type().pointer(), node.type))\n+    ulong_type = utils.get_ulong_type()\n+    indirect_ptr = node.cast(ulong_type) & ~0x2\n+    val = indirect_ptr.cast(maple_enode_type.get_type().pointer())\n+    return val\n+\n+def mte_node_type(entry):\n+    ulong_type = utils.get_ulong_type()\n+    val = None\n+    if entry.type == maple_enode_type.get_type().pointer():\n+        val = entry.cast(ulong_type)\n+    elif entry.type == ulong_type:\n+        val = entry\n+    else:\n+        raise gdb.GdbError(\"{} must be {} not {}\"\n+                           .format(mte_node_type.__name__, maple_enode_type.get_type().pointer(), entry.type))\n+    return (val >> 0x3) & 0xf\n+\n+def ma_dead_node(node):\n+    if node.type != maple_node_type.get_type().pointer():\n+        raise gdb.GdbError(\"{} must be {} not {}\"\n+                           .format(ma_dead_node.__name__, maple_node_type.get_type().pointer(), node.type))\n+    ulong_type = utils.get_ulong_type()\n+    parent = node['parent']\n+    indirect_ptr = node['parent'].cast(ulong_type) & ~constants.LX_MAPLE_NODE_MASK\n+    return indirect_ptr == node\n+\n+def mte_to_node(enode):\n+    ulong_type = utils.get_ulong_type()\n+    if enode.type == maple_enode_type.get_type().pointer():\n+        indirect_ptr = enode.cast(ulong_type)\n+    elif enode.type == ulong_type:\n+        indirect_ptr = enode\n+    else:\n+        raise gdb.GdbError(\"{} must be {} not {}\"\n+                           .format(mte_to_node.__name__, maple_enode_type.get_type().pointer(), enode.type))\n+    indirect_ptr = indirect_ptr & ~constants.LX_MAPLE_NODE_MASK\n+    return indirect_ptr.cast(maple_node_type.get_type().pointer())\n+\n+def mte_dead_node(enode):\n+    if enode.type != maple_enode_type.get_type().pointer():\n+        raise gdb.GdbError(\"{} must be {} not {}\"\n+                           .format(mte_dead_node.__name__, maple_enode_type.get_type().pointer(), enode.type))\n+    node = mte_to_node(enode)\n+    return ma_dead_node(node)\n+\n+def ma_is_leaf(tp):\n+    result = tp < maple_range_64\n+    return tp < maple_range_64\n+\n+def mt_pivots(t):\n+    if t == maple_dense:\n+        return 0\n+    elif t == maple_leaf_64 or t == maple_range_64:\n+        return constants.LX_MAPLE_RANGE64_SLOTS - 1\n+    elif t == maple_arange_64:\n+        return constants.LX_MAPLE_ARANGE64_SLOTS - 1\n+\n+def ma_pivots(node, t):\n+    if node.type != maple_node_type.get_type().pointer():\n+        raise gdb.GdbError(\"{}: must be {} not {}\"\n+                           .format(ma_pivots.__name__, maple_node_type.get_type().pointer(), node.type))\n+    if t == maple_arange_64:\n+        return node['ma64']['pivot']\n+    elif t == maple_leaf_64 or t == maple_range_64:\n+        return node['mr64']['pivot']\n+    else:\n+        return None\n+\n+def ma_slots(node, tp):\n+    if node.type != maple_node_type.get_type().pointer():\n+        raise gdb.GdbError(\"{}: must be {} not {}\"\n+                           .format(ma_slots.__name__, maple_node_type.get_type().pointer(), node.type))\n+    if tp == maple_arange_64:\n+        return node['ma64']['slot']\n+    elif tp == maple_range_64 or tp == maple_leaf_64:\n+        return node['mr64']['slot']\n+    elif tp == maple_dense:\n+        return node['slot']\n+    else:\n+        return None\n+\n+def mt_slot(mt, slots, offset):\n+    ulong_type = utils.get_ulong_type()\n+    return slots[offset].cast(ulong_type)\n+\n+def mtree_lookup_walk(mas):\n+    ulong_type = utils.get_ulong_type()\n+    n = mas.node\n+\n+    while True:\n+        node = mte_to_node(n)\n+        tp = mte_node_type(n)\n+        pivots = ma_pivots(node, tp)\n+        end = mt_pivots(tp)\n+        offset = 0\n+        while True:\n+            if pivots[offset] >= mas.index:\n+                break\n+            if offset >= end:\n+                break\n+            offset += 1\n+\n+        slots = ma_slots(node, tp)\n+        n = mt_slot(mas.tree, slots, offset)\n+        if ma_dead_node(node) is True:\n+            mas.reset()\n+            return None\n+            break\n+\n+        if ma_is_leaf(tp) is True:\n+            break\n+\n+    return n\n+\n+def mtree_load(mt, index):\n+    ulong_type = utils.get_ulong_type()\n+    # MT_STATE(...)\n+    mas = Mas(mt, index, index)\n+    entry = None\n+\n+    while True:\n+        entry = mas.start()\n+        if mas.is_none():\n+            return None\n+\n+        if mas.is_ptr():\n+            if index != 0:\n+                entry = None\n+            return entry\n+\n+        entry = mtree_lookup_walk(mas)\n+        if entry is None and mas.is_start():\n+            continue\n+        else:\n+            break\n+\n+    if xarray.xa_is_zero(entry):\n+        return None\n+\n+    return entry\ndiff --git a/scripts/gdb/linux/vfs.py b/scripts/gdb/linux/vfs.py\nindex b5fbb18ccb77..9e921b645a68 100644\n--- a/scripts/gdb/linux/vfs.py\n+++ b/scripts/gdb/linux/vfs.py\n@@ -22,7 +22,7 @@ def dentry_name(d):\n     if parent == d or parent == 0:\n         return \"\"\n     p = dentry_name(d['d_parent']) + \"/\"\n-    return p + d['d_shortname']['string'].string()\n+    return p + d['d_name']['name'].string()\n \n class DentryName(gdb.Function):\n     \"\"\"Return string of the full path of a dentry.\ndiff --git a/scripts/gdb/linux/xarray.py b/scripts/gdb/linux/xarray.py\nnew file mode 100644\nindex 000000000000..f4477b5def75\n--- /dev/null\n+++ b/scripts/gdb/linux/xarray.py\n@@ -0,0 +1,28 @@\n+# SPDX-License-Identifier: GPL-2.0\n+#\n+#  Xarray helpers\n+#\n+# Copyright (c) 2025 Broadcom\n+#\n+# Authors:\n+#  Florian Fainelli <florian.fainelli@broadcom.com>\n+\n+import gdb\n+\n+from linux import utils\n+from linux import constants\n+\n+def xa_is_internal(entry):\n+    ulong_type = utils.get_ulong_type()\n+    return ((entry.cast(ulong_type) & 3) == 2)\n+\n+def xa_mk_internal(v):\n+    return ((v << 2) | 2)\n+\n+def xa_is_zero(entry):\n+    ulong_type = utils.get_ulong_type()\n+    return entry.cast(ulong_type) == xa_mk_internal(257)\n+\n+def xa_is_node(entry):\n+    ulong_type = utils.get_ulong_type()\n+    return xa_is_internal(entry) and (entry.cast(ulong_type) > 4096)\ndiff --git a/tools/include/linux/kallsyms.h b/tools/include/linux/kallsyms.h\nindex 5a37ccbec54f..f61a01dd7eb7 100644\n--- a/tools/include/linux/kallsyms.h\n+++ b/tools/include/linux/kallsyms.h\n@@ -18,6 +18,7 @@ static inline const char *kallsyms_lookup(unsigned long addr,\n \treturn NULL;\n }\n \n+#ifdef HAVE_BACKTRACE_SUPPORT\n #include <execinfo.h>\n #include <stdlib.h>\n static inline void print_ip_sym(const char *loglvl, unsigned long ip)\n@@ -30,5 +31,8 @@ static inline void print_ip_sym(const char *loglvl, unsigned long ip)\n \n \tfree(name);\n }\n+#else\n+static inline void print_ip_sym(const char *loglvl, unsigned long ip) {}\n+#endif\n \n #endif",
    "stats": {
      "insertions": 399,
      "deletions": 130,
      "files": 26
    }
  },
  {
    "sha": "3b428e1cfcc4c5f063bb8b367beb71ee06470d4b",
    "message": "Merge tag 'erofs-for-6.16-rc6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs\n\nPull erofs fixes from Gao Xiang:\n \"Fix for a cache aliasing issue by adding missing flush_dcache_folio(),\n  which causes execution failures on some arm32 setups.\n\n  Fix for large compressed fragments, which could be generated by\n  -Eall-fragments option (but should be rare) and was rejected by\n  mistake due to an on-disk hardening commit.\n\n  The remaining ones are small fixes. Summary:\n\n   - Address cache aliasing for mappable page cache folios\n\n   - Allow readdir() to be interrupted\n\n   - Fix large fragment handling which was errored out by mistake\n\n   - Add missing tracepoints\n\n   - Use memcpy_to_folio() to replace copy_to_iter() for inline data\"\n\n* tag 'erofs-for-6.16-rc6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs:\n  erofs: fix large fragment handling\n  erofs: allow readdir() to be interrupted\n  erofs: address D-cache aliasing\n  erofs: use memcpy_to_folio() to replace copy_to_iter()\n  erofs: fix to add missing tracepoint in erofs_read_folio()\n  erofs: fix to add missing tracepoint in erofs_readahead()",
    "author": "Linus Torvalds",
    "date": "2025-07-12T10:20:03-07:00",
    "files_changed": [
      "fs/erofs/data.c",
      "fs/erofs/decompressor.c",
      "fs/erofs/dir.c",
      "fs/erofs/fileio.c",
      "fs/erofs/internal.h",
      "fs/erofs/zdata.c",
      "fs/erofs/zmap.c"
    ],
    "diff": "diff --git a/fs/erofs/data.c b/fs/erofs/data.c\nindex 6a329c329f43..16e4a6bd9b97 100644\n--- a/fs/erofs/data.c\n+++ b/fs/erofs/data.c\n@@ -214,9 +214,11 @@ int erofs_map_dev(struct super_block *sb, struct erofs_map_dev *map)\n \n /*\n  * bit 30: I/O error occurred on this folio\n+ * bit 29: CPU has dirty data in D-cache (needs aliasing handling);\n  * bit 0 - 29: remaining parts to complete this folio\n  */\n-#define EROFS_ONLINEFOLIO_EIO\t\t\t(1 << 30)\n+#define EROFS_ONLINEFOLIO_EIO\t\t30\n+#define EROFS_ONLINEFOLIO_DIRTY\t\t29\n \n void erofs_onlinefolio_init(struct folio *folio)\n {\n@@ -233,19 +235,23 @@ void erofs_onlinefolio_split(struct folio *folio)\n \tatomic_inc((atomic_t *)&folio->private);\n }\n \n-void erofs_onlinefolio_end(struct folio *folio, int err)\n+void erofs_onlinefolio_end(struct folio *folio, int err, bool dirty)\n {\n \tint orig, v;\n \n \tdo {\n \t\torig = atomic_read((atomic_t *)&folio->private);\n-\t\tv = (orig - 1) | (err ? EROFS_ONLINEFOLIO_EIO : 0);\n+\t\tDBG_BUGON(orig <= 0);\n+\t\tv = dirty << EROFS_ONLINEFOLIO_DIRTY;\n+\t\tv |= (orig - 1) | (!!err << EROFS_ONLINEFOLIO_EIO);\n \t} while (atomic_cmpxchg((atomic_t *)&folio->private, orig, v) != orig);\n \n-\tif (v & ~EROFS_ONLINEFOLIO_EIO)\n+\tif (v & (BIT(EROFS_ONLINEFOLIO_DIRTY) - 1))\n \t\treturn;\n \tfolio->private = 0;\n-\tfolio_end_read(folio, !(v & EROFS_ONLINEFOLIO_EIO));\n+\tif (v & BIT(EROFS_ONLINEFOLIO_DIRTY))\n+\t\tflush_dcache_folio(folio);\n+\tfolio_end_read(folio, !(v & BIT(EROFS_ONLINEFOLIO_EIO)));\n }\n \n static int erofs_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n@@ -351,11 +357,16 @@ int erofs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n  */\n static int erofs_read_folio(struct file *file, struct folio *folio)\n {\n+\ttrace_erofs_read_folio(folio, true);\n+\n \treturn iomap_read_folio(folio, &erofs_iomap_ops);\n }\n \n static void erofs_readahead(struct readahead_control *rac)\n {\n+\ttrace_erofs_readahead(rac->mapping->host, readahead_index(rac),\n+\t\t\t\t\treadahead_count(rac), true);\n+\n \treturn iomap_readahead(rac, &erofs_iomap_ops);\n }\n \ndiff --git a/fs/erofs/decompressor.c b/fs/erofs/decompressor.c\nindex bf62e2836b60..358061d7b660 100644\n--- a/fs/erofs/decompressor.c\n+++ b/fs/erofs/decompressor.c\n@@ -301,13 +301,11 @@ static int z_erofs_transform_plain(struct z_erofs_decompress_req *rq,\n \t\tcur = min(cur, rq->outputsize);\n \t\tif (cur && rq->out[0]) {\n \t\t\tkin = kmap_local_page(rq->in[nrpages_in - 1]);\n-\t\t\tif (rq->out[0] == rq->in[nrpages_in - 1]) {\n+\t\t\tif (rq->out[0] == rq->in[nrpages_in - 1])\n \t\t\t\tmemmove(kin + rq->pageofs_out, kin + pi, cur);\n-\t\t\t\tflush_dcache_page(rq->out[0]);\n-\t\t\t} else {\n+\t\t\telse\n \t\t\t\tmemcpy_to_page(rq->out[0], rq->pageofs_out,\n \t\t\t\t\t       kin + pi, cur);\n-\t\t\t}\n \t\t\tkunmap_local(kin);\n \t\t}\n \t\trq->outputsize -= cur;\n@@ -325,14 +323,12 @@ static int z_erofs_transform_plain(struct z_erofs_decompress_req *rq,\n \t\t\tpo = (rq->pageofs_out + cur + pi) & ~PAGE_MASK;\n \t\t\tDBG_BUGON(no >= nrpages_out);\n \t\t\tcnt = min(insz - pi, PAGE_SIZE - po);\n-\t\t\tif (rq->out[no] == rq->in[ni]) {\n+\t\t\tif (rq->out[no] == rq->in[ni])\n \t\t\t\tmemmove(kin + po,\n \t\t\t\t\tkin + rq->pageofs_in + pi, cnt);\n-\t\t\t\tflush_dcache_page(rq->out[no]);\n-\t\t\t} else if (rq->out[no]) {\n+\t\t\telse if (rq->out[no])\n \t\t\t\tmemcpy_to_page(rq->out[no], po,\n \t\t\t\t\t       kin + rq->pageofs_in + pi, cnt);\n-\t\t\t}\n \t\t\tpi += cnt;\n \t\t} while (pi < insz);\n \t\tkunmap_local(kin);\ndiff --git a/fs/erofs/dir.c b/fs/erofs/dir.c\nindex 2fae209d0274..3e4b38bec0aa 100644\n--- a/fs/erofs/dir.c\n+++ b/fs/erofs/dir.c\n@@ -58,6 +58,11 @@ static int erofs_readdir(struct file *f, struct dir_context *ctx)\n \t\tstruct erofs_dirent *de;\n \t\tunsigned int nameoff, maxsize;\n \n+\t\tif (fatal_signal_pending(current)) {\n+\t\t\terr = -ERESTARTSYS;\n+\t\t\tbreak;\n+\t\t}\n+\n \t\tde = erofs_bread(&buf, dbstart, true);\n \t\tif (IS_ERR(de)) {\n \t\t\terofs_err(sb, \"failed to readdir of logical block %llu of nid %llu\",\n@@ -88,6 +93,7 @@ static int erofs_readdir(struct file *f, struct dir_context *ctx)\n \t\t\tbreak;\n \t\tctx->pos = dbstart + maxsize;\n \t\tofs = 0;\n+\t\tcond_resched();\n \t}\n \terofs_put_metabuf(&buf);\n \tif (EROFS_I(dir)->dot_omitted && ctx->pos == dir->i_size) {\ndiff --git a/fs/erofs/fileio.c b/fs/erofs/fileio.c\nindex df5cc63f2c01..91781718199e 100644\n--- a/fs/erofs/fileio.c\n+++ b/fs/erofs/fileio.c\n@@ -38,7 +38,7 @@ static void erofs_fileio_ki_complete(struct kiocb *iocb, long ret)\n \t} else {\n \t\tbio_for_each_folio_all(fi, &rq->bio) {\n \t\t\tDBG_BUGON(folio_test_uptodate(fi.folio));\n-\t\t\terofs_onlinefolio_end(fi.folio, ret);\n+\t\t\terofs_onlinefolio_end(fi.folio, ret, false);\n \t\t}\n \t}\n \tbio_uninit(&rq->bio);\n@@ -96,8 +96,6 @@ static int erofs_fileio_scan_folio(struct erofs_fileio *io, struct folio *folio)\n \tstruct erofs_map_blocks *map = &io->map;\n \tunsigned int cur = 0, end = folio_size(folio), len, attached = 0;\n \tloff_t pos = folio_pos(folio), ofs;\n-\tstruct iov_iter iter;\n-\tstruct bio_vec bv;\n \tint err = 0;\n \n \terofs_onlinefolio_init(folio);\n@@ -122,13 +120,7 @@ static int erofs_fileio_scan_folio(struct erofs_fileio *io, struct folio *folio)\n \t\t\t\terr = PTR_ERR(src);\n \t\t\t\tbreak;\n \t\t\t}\n-\t\t\tbvec_set_folio(&bv, folio, len, cur);\n-\t\t\tiov_iter_bvec(&iter, ITER_DEST, &bv, 1, len);\n-\t\t\tif (copy_to_iter(src, len, &iter) != len) {\n-\t\t\t\terofs_put_metabuf(&buf);\n-\t\t\t\terr = -EIO;\n-\t\t\t\tbreak;\n-\t\t\t}\n+\t\t\tmemcpy_to_folio(folio, cur, src, len);\n \t\t\terofs_put_metabuf(&buf);\n \t\t} else if (!(map->m_flags & EROFS_MAP_MAPPED)) {\n \t\t\tfolio_zero_segment(folio, cur, cur + len);\n@@ -162,7 +154,7 @@ static int erofs_fileio_scan_folio(struct erofs_fileio *io, struct folio *folio)\n \t\t}\n \t\tcur += len;\n \t}\n-\terofs_onlinefolio_end(folio, err);\n+\terofs_onlinefolio_end(folio, err, false);\n \treturn err;\n }\n \ndiff --git a/fs/erofs/internal.h b/fs/erofs/internal.h\nindex a32c03a80c70..06b867d2fc3b 100644\n--- a/fs/erofs/internal.h\n+++ b/fs/erofs/internal.h\n@@ -315,10 +315,12 @@ static inline struct folio *erofs_grab_folio_nowait(struct address_space *as,\n /* The length of extent is full */\n #define EROFS_MAP_FULL_MAPPED\t0x0008\n /* Located in the special packed inode */\n-#define EROFS_MAP_FRAGMENT\t0x0010\n+#define __EROFS_MAP_FRAGMENT\t0x0010\n /* The extent refers to partial decompressed data */\n #define EROFS_MAP_PARTIAL_REF\t0x0020\n \n+#define EROFS_MAP_FRAGMENT\t(EROFS_MAP_MAPPED | __EROFS_MAP_FRAGMENT)\n+\n struct erofs_map_blocks {\n \tstruct erofs_buf buf;\n \n@@ -390,7 +392,7 @@ int erofs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n int erofs_map_blocks(struct inode *inode, struct erofs_map_blocks *map);\n void erofs_onlinefolio_init(struct folio *folio);\n void erofs_onlinefolio_split(struct folio *folio);\n-void erofs_onlinefolio_end(struct folio *folio, int err);\n+void erofs_onlinefolio_end(struct folio *folio, int err, bool dirty);\n struct inode *erofs_iget(struct super_block *sb, erofs_nid_t nid);\n int erofs_getattr(struct mnt_idmap *idmap, const struct path *path,\n \t\t  struct kstat *stat, u32 request_mask,\ndiff --git a/fs/erofs/zdata.c b/fs/erofs/zdata.c\nindex fe8071844724..e3f28a1bb945 100644\n--- a/fs/erofs/zdata.c\n+++ b/fs/erofs/zdata.c\n@@ -1034,7 +1034,7 @@ static int z_erofs_scan_folio(struct z_erofs_frontend *f,\n \t\tif (!(map->m_flags & EROFS_MAP_MAPPED)) {\n \t\t\tfolio_zero_segment(folio, cur, end);\n \t\t\ttight = false;\n-\t\t} else if (map->m_flags & EROFS_MAP_FRAGMENT) {\n+\t\t} else if (map->m_flags & __EROFS_MAP_FRAGMENT) {\n \t\t\terofs_off_t fpos = offset + cur - map->m_la;\n \n \t\t\terr = z_erofs_read_fragment(inode->i_sb, folio, cur,\n@@ -1091,7 +1091,7 @@ static int z_erofs_scan_folio(struct z_erofs_frontend *f,\n \t\t\ttight = (bs == PAGE_SIZE);\n \t\t}\n \t} while ((end = cur) > 0);\n-\terofs_onlinefolio_end(folio, err);\n+\terofs_onlinefolio_end(folio, err, false);\n \treturn err;\n }\n \n@@ -1196,7 +1196,7 @@ static void z_erofs_fill_other_copies(struct z_erofs_backend *be, int err)\n \t\t\tcur += len;\n \t\t}\n \t\tkunmap_local(dst);\n-\t\terofs_onlinefolio_end(page_folio(bvi->bvec.page), err);\n+\t\terofs_onlinefolio_end(page_folio(bvi->bvec.page), err, true);\n \t\tlist_del(p);\n \t\tkfree(bvi);\n \t}\n@@ -1355,7 +1355,7 @@ static int z_erofs_decompress_pcluster(struct z_erofs_backend *be, int err)\n \n \t\tDBG_BUGON(z_erofs_page_is_invalidated(page));\n \t\tif (!z_erofs_is_shortlived_page(page)) {\n-\t\t\terofs_onlinefolio_end(page_folio(page), err);\n+\t\t\terofs_onlinefolio_end(page_folio(page), err, true);\n \t\t\tcontinue;\n \t\t}\n \t\tif (pcl->algorithmformat != Z_EROFS_COMPRESSION_LZ4) {\ndiff --git a/fs/erofs/zmap.c b/fs/erofs/zmap.c\nindex 0bebc6e3a4d7..f1a15ff22147 100644\n--- a/fs/erofs/zmap.c\n+++ b/fs/erofs/zmap.c\n@@ -413,8 +413,7 @@ static int z_erofs_map_blocks_fo(struct inode *inode,\n \t    !vi->z_tailextent_headlcn) {\n \t\tmap->m_la = 0;\n \t\tmap->m_llen = inode->i_size;\n-\t\tmap->m_flags = EROFS_MAP_MAPPED |\n-\t\t\tEROFS_MAP_FULL_MAPPED | EROFS_MAP_FRAGMENT;\n+\t\tmap->m_flags = EROFS_MAP_FRAGMENT;\n \t\treturn 0;\n \t}\n \tinitial_lcn = ofs >> lclusterbits;\n@@ -489,7 +488,7 @@ static int z_erofs_map_blocks_fo(struct inode *inode,\n \t\t\tgoto unmap_out;\n \t\t}\n \t} else if (fragment && m.lcn == vi->z_tailextent_headlcn) {\n-\t\tmap->m_flags |= EROFS_MAP_FRAGMENT;\n+\t\tmap->m_flags = EROFS_MAP_FRAGMENT;\n \t} else {\n \t\tmap->m_pa = erofs_pos(sb, m.pblk);\n \t\terr = z_erofs_get_extent_compressedlen(&m, initial_lcn);\n@@ -617,7 +616,7 @@ static int z_erofs_map_blocks_ext(struct inode *inode,\n \tif (lstart < lend) {\n \t\tmap->m_la = lstart;\n \t\tif (last && (vi->z_advise & Z_EROFS_ADVISE_FRAGMENT_PCLUSTER)) {\n-\t\t\tmap->m_flags |= EROFS_MAP_MAPPED | EROFS_MAP_FRAGMENT;\n+\t\t\tmap->m_flags = EROFS_MAP_FRAGMENT;\n \t\t\tvi->z_fragmentoff = map->m_plen;\n \t\t\tif (recsz > offsetof(struct z_erofs_extent, pstart_lo))\n \t\t\t\tvi->z_fragmentoff |= map->m_pa << 32;\n@@ -797,7 +796,7 @@ static int z_erofs_iomap_begin_report(struct inode *inode, loff_t offset,\n \tiomap->length = map.m_llen;\n \tif (map.m_flags & EROFS_MAP_MAPPED) {\n \t\tiomap->type = IOMAP_MAPPED;\n-\t\tiomap->addr = map.m_flags & EROFS_MAP_FRAGMENT ?\n+\t\tiomap->addr = map.m_flags & __EROFS_MAP_FRAGMENT ?\n \t\t\t      IOMAP_NULL_ADDR : map.m_pa;\n \t} else {\n \t\tiomap->type = IOMAP_HOLE;",
    "stats": {
      "insertions": 41,
      "deletions": 35,
      "files": 7
    }
  },
  {
    "sha": "2632d81f5a02b65e6131cd57ba092bd321446e91",
    "message": "Merge tag 'v6.16-rc5-ksmbd-server-fixes' of git://git.samba.org/ksmbd\n\nPull smb server fixes from Steve French:\n\n - fix use after free in lease break\n\n - small fix for freeing rdma transport (fixes missing logging of\n   cm_qp_destroy)\n\n - fix write count leak\n\n* tag 'v6.16-rc5-ksmbd-server-fixes' of git://git.samba.org/ksmbd:\n  ksmbd: fix potential use-after-free in oplock/lease break ack\n  ksmbd: fix a mount write count leak in ksmbd_vfs_kern_path_locked()\n  smb: server: make use of rdma_destroy_qp()",
    "author": "Linus Torvalds",
    "date": "2025-07-12T10:06:06-07:00",
    "files_changed": [
      "fs/smb/server/smb2pdu.c",
      "fs/smb/server/transport_rdma.c",
      "fs/smb/server/vfs.c"
    ],
    "diff": "diff --git a/fs/smb/server/smb2pdu.c b/fs/smb/server/smb2pdu.c\nindex fafa86273f12..63d17cea2e95 100644\n--- a/fs/smb/server/smb2pdu.c\n+++ b/fs/smb/server/smb2pdu.c\n@@ -8573,11 +8573,6 @@ static void smb20_oplock_break_ack(struct ksmbd_work *work)\n \t\tgoto err_out;\n \t}\n \n-\topinfo->op_state = OPLOCK_STATE_NONE;\n-\twake_up_interruptible_all(&opinfo->oplock_q);\n-\topinfo_put(opinfo);\n-\tksmbd_fd_put(work, fp);\n-\n \trsp->StructureSize = cpu_to_le16(24);\n \trsp->OplockLevel = rsp_oplevel;\n \trsp->Reserved = 0;\n@@ -8585,16 +8580,15 @@ static void smb20_oplock_break_ack(struct ksmbd_work *work)\n \trsp->VolatileFid = volatile_id;\n \trsp->PersistentFid = persistent_id;\n \tret = ksmbd_iov_pin_rsp(work, rsp, sizeof(struct smb2_oplock_break));\n-\tif (!ret)\n-\t\treturn;\n-\n+\tif (ret) {\n err_out:\n+\t\tsmb2_set_err_rsp(work);\n+\t}\n+\n \topinfo->op_state = OPLOCK_STATE_NONE;\n \twake_up_interruptible_all(&opinfo->oplock_q);\n-\n \topinfo_put(opinfo);\n \tksmbd_fd_put(work, fp);\n-\tsmb2_set_err_rsp(work);\n }\n \n static int check_lease_state(struct lease *lease, __le32 req_state)\n@@ -8724,11 +8718,6 @@ static void smb21_lease_break_ack(struct ksmbd_work *work)\n \t}\n \n \tlease_state = lease->state;\n-\topinfo->op_state = OPLOCK_STATE_NONE;\n-\twake_up_interruptible_all(&opinfo->oplock_q);\n-\tatomic_dec(&opinfo->breaking_cnt);\n-\twake_up_interruptible_all(&opinfo->oplock_brk);\n-\topinfo_put(opinfo);\n \n \trsp->StructureSize = cpu_to_le16(36);\n \trsp->Reserved = 0;\n@@ -8737,16 +8726,16 @@ static void smb21_lease_break_ack(struct ksmbd_work *work)\n \trsp->LeaseState = lease_state;\n \trsp->LeaseDuration = 0;\n \tret = ksmbd_iov_pin_rsp(work, rsp, sizeof(struct smb2_lease_ack));\n-\tif (!ret)\n-\t\treturn;\n-\n+\tif (ret) {\n err_out:\n+\t\tsmb2_set_err_rsp(work);\n+\t}\n+\n+\topinfo->op_state = OPLOCK_STATE_NONE;\n \twake_up_interruptible_all(&opinfo->oplock_q);\n \tatomic_dec(&opinfo->breaking_cnt);\n \twake_up_interruptible_all(&opinfo->oplock_brk);\n-\n \topinfo_put(opinfo);\n-\tsmb2_set_err_rsp(work);\n }\n \n /**\ndiff --git a/fs/smb/server/transport_rdma.c b/fs/smb/server/transport_rdma.c\nindex 64a428a06ace..c6cbe0d56e32 100644\n--- a/fs/smb/server/transport_rdma.c\n+++ b/fs/smb/server/transport_rdma.c\n@@ -433,7 +433,8 @@ static void free_transport(struct smb_direct_transport *t)\n \tif (t->qp) {\n \t\tib_drain_qp(t->qp);\n \t\tib_mr_pool_destroy(t->qp, &t->qp->rdma_mrs);\n-\t\tib_destroy_qp(t->qp);\n+\t\tt->qp = NULL;\n+\t\trdma_destroy_qp(t->cm_id);\n \t}\n \n \tksmbd_debug(RDMA, \"drain the reassembly queue\\n\");\n@@ -1940,8 +1941,8 @@ static int smb_direct_create_qpair(struct smb_direct_transport *t,\n \treturn 0;\n err:\n \tif (t->qp) {\n-\t\tib_destroy_qp(t->qp);\n \t\tt->qp = NULL;\n+\t\trdma_destroy_qp(t->cm_id);\n \t}\n \tif (t->recv_cq) {\n \t\tib_destroy_cq(t->recv_cq);\ndiff --git a/fs/smb/server/vfs.c b/fs/smb/server/vfs.c\nindex 0f3aad12e495..d3437f6644e3 100644\n--- a/fs/smb/server/vfs.c\n+++ b/fs/smb/server/vfs.c\n@@ -1282,6 +1282,7 @@ int ksmbd_vfs_kern_path_locked(struct ksmbd_work *work, char *name,\n \n \t\terr = ksmbd_vfs_lock_parent(parent_path->dentry, path->dentry);\n \t\tif (err) {\n+\t\t\tmnt_drop_write(parent_path->mnt);\n \t\t\tpath_put(path);\n \t\t\tpath_put(parent_path);\n \t\t}",
    "stats": {
      "insertions": 13,
      "deletions": 22,
      "files": 3
    }
  },
  {
    "sha": "3c2fe27971c3c9cc27de6e369385f6428db6c0b5",
    "message": "Merge tag 'drm-fixes-2025-07-12' of https://gitlab.freedesktop.org/drm/kernel\n\nPull drm fixes from Simona Vetter:\n \"Cross-subsystem Changes:\n   - agp/amd64 binding dmesg noise regression fix\n\n  Core Changes:\n   - fix race in gem_handle_create_tail\n   - fixup handle_count fb refcount regression from -rc5, popular with\n     reports ...\n   - call rust dtor for drm_device release\n\n  Driver Changes:\n   - nouveau: magic 50ms suspend fix, acpi leak fix\n   - tegra: dma api error in nvdec\n   - pvr: fix device reset\n   - habanalbs maintainer update\n   - intel display: fix some dsi mipi sequences\n   - xe fixes: SRIOV fixes, small GuC fixes, disable indirect ring due\n     to issues, compression fix for fragmented BO, doc update\n\n* tag 'drm-fixes-2025-07-12' of https://gitlab.freedesktop.org/drm/kernel: (22 commits)\n  drm/xe/guc: Default log level to non-verbose\n  drm/xe/bmg: Don't use WA 16023588340 and 22019338487 on VF\n  drm/xe/guc: Recommend GuC v70.46.2 for BMG, LNL, DG2\n  drm/xe/pm: Correct comment of xe_pm_set_vram_threshold()\n  drm/xe: Release runtime pm for error path of xe_devcoredump_read()\n  drm/xe/pm: Restore display pm if there is error after display suspend\n  drm/i915/bios: Apply vlv_fixup_mipi_sequences() to v2 mipi-sequences too\n  drm/gem: Fix race in drm_gem_handle_create_tail()\n  drm/framebuffer: Acquire internal references on GEM handles\n  agp/amd64: Check AGP Capability before binding to unsupported devices\n  drm/xe/bmg: fix compressed VRAM handling\n  Revert \"drm/xe/xe2: Enable Indirect Ring State support for Xe2\"\n  drm/xe: Allocate PF queue size on pow2 boundary\n  drm/xe/pf: Clear all LMTT pages on alloc\n  drm/nouveau/gsp: fix potential leak of memory used during acpi init\n  rust: drm: remove unnecessary imports\n  MAINTAINERS: Change habanalabs maintainer\n  drm/imagination: Fix kernel crash when hard resetting the GPU\n  drm/tegra: nvdec: Fix dma_alloc_coherent error check\n  rust: drm: device: drop_in_place() the drm::Device in release()\n  ...",
    "author": "Linus Torvalds",
    "date": "2025-07-11T17:18:40-07:00",
    "files_changed": [
      "drivers/char/agp/amd64-agp.c",
      "drivers/gpu/drm/drm_framebuffer.c",
      "drivers/gpu/drm/drm_gem.c",
      "drivers/gpu/drm/drm_gem_framebuffer_helper.c",
      "drivers/gpu/drm/drm_internal.h",
      "drivers/gpu/drm/i915/display/intel_bios.c",
      "drivers/gpu/drm/imagination/pvr_power.c",
      "drivers/gpu/drm/nouveau/nouveau_debugfs.c",
      "drivers/gpu/drm/nouveau/nouveau_debugfs.h",
      "drivers/gpu/drm/nouveau/nouveau_drm.c",
      "drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/gsp.c",
      "drivers/gpu/drm/tegra/nvdec.c",
      "drivers/gpu/drm/xe/xe_devcoredump.c",
      "drivers/gpu/drm/xe/xe_gt_pagefault.c",
      "drivers/gpu/drm/xe/xe_lmtt.c",
      "drivers/gpu/drm/xe/xe_migrate.c",
      "drivers/gpu/drm/xe/xe_module.c",
      "drivers/gpu/drm/xe/xe_pci.c",
      "drivers/gpu/drm/xe/xe_pm.c",
      "drivers/gpu/drm/xe/xe_uc_fw.c",
      "include/drm/drm_file.h",
      "include/drm/drm_framebuffer.h"
    ],
    "diff": "diff --git a/MAINTAINERS b/MAINTAINERS\nindex 05841dc1d372..c255bbb49d6b 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -10506,7 +10506,7 @@ S:\tMaintained\n F:\tblock/partitions/efi.*\n \n HABANALABS PCI DRIVER\n-M:\tOfir Bitton <obitton@habana.ai>\n+M:\tYaron Avizrat <yaron.avizrat@intel.com>\n L:\tdri-devel@lists.freedesktop.org\n S:\tSupported\n C:\tirc://irc.oftc.net/dri-devel\ndiff --git a/drivers/char/agp/amd64-agp.c b/drivers/char/agp/amd64-agp.c\nindex bf490967241a..2505df1f4e69 100644\n--- a/drivers/char/agp/amd64-agp.c\n+++ b/drivers/char/agp/amd64-agp.c\n@@ -720,11 +720,6 @@ static const struct pci_device_id agp_amd64_pci_table[] = {\n \n MODULE_DEVICE_TABLE(pci, agp_amd64_pci_table);\n \n-static const struct pci_device_id agp_amd64_pci_promisc_table[] = {\n-\t{ PCI_DEVICE_CLASS(0, 0) },\n-\t{ }\n-};\n-\n static DEFINE_SIMPLE_DEV_PM_OPS(agp_amd64_pm_ops, NULL, agp_amd64_resume);\n \n static struct pci_driver agp_amd64_pci_driver = {\n@@ -739,6 +734,7 @@ static struct pci_driver agp_amd64_pci_driver = {\n /* Not static due to IOMMU code calling it early. */\n int __init agp_amd64_init(void)\n {\n+\tstruct pci_dev *pdev = NULL;\n \tint err = 0;\n \n \tif (agp_off)\n@@ -767,9 +763,13 @@ int __init agp_amd64_init(void)\n \t\t}\n \n \t\t/* Look for any AGP bridge */\n-\t\tagp_amd64_pci_driver.id_table = agp_amd64_pci_promisc_table;\n-\t\terr = driver_attach(&agp_amd64_pci_driver.driver);\n-\t\tif (err == 0 && agp_bridges_found == 0) {\n+\t\tfor_each_pci_dev(pdev)\n+\t\t\tif (pci_find_capability(pdev, PCI_CAP_ID_AGP))\n+\t\t\t\tpci_add_dynid(&agp_amd64_pci_driver,\n+\t\t\t\t\t      pdev->vendor, pdev->device,\n+\t\t\t\t\t      pdev->subsystem_vendor,\n+\t\t\t\t\t      pdev->subsystem_device, 0, 0, 0);\n+\t\tif (agp_bridges_found == 0) {\n \t\t\tpci_unregister_driver(&agp_amd64_pci_driver);\n \t\t\terr = -ENODEV;\n \t\t}\ndiff --git a/drivers/gpu/drm/drm_framebuffer.c b/drivers/gpu/drm/drm_framebuffer.c\nindex b781601946db..63a70f285cce 100644\n--- a/drivers/gpu/drm/drm_framebuffer.c\n+++ b/drivers/gpu/drm/drm_framebuffer.c\n@@ -862,11 +862,23 @@ EXPORT_SYMBOL_FOR_TESTS_ONLY(drm_framebuffer_free);\n int drm_framebuffer_init(struct drm_device *dev, struct drm_framebuffer *fb,\n \t\t\t const struct drm_framebuffer_funcs *funcs)\n {\n+\tunsigned int i;\n \tint ret;\n+\tbool exists;\n \n \tif (WARN_ON_ONCE(fb->dev != dev || !fb->format))\n \t\treturn -EINVAL;\n \n+\tfor (i = 0; i < fb->format->num_planes; i++) {\n+\t\tif (drm_WARN_ON_ONCE(dev, fb->internal_flags & DRM_FRAMEBUFFER_HAS_HANDLE_REF(i)))\n+\t\t\tfb->internal_flags &= ~DRM_FRAMEBUFFER_HAS_HANDLE_REF(i);\n+\t\tif (fb->obj[i]) {\n+\t\t\texists = drm_gem_object_handle_get_if_exists_unlocked(fb->obj[i]);\n+\t\t\tif (exists)\n+\t\t\t\tfb->internal_flags |= DRM_FRAMEBUFFER_HAS_HANDLE_REF(i);\n+\t\t}\n+\t}\n+\n \tINIT_LIST_HEAD(&fb->filp_head);\n \n \tfb->funcs = funcs;\n@@ -875,7 +887,7 @@ int drm_framebuffer_init(struct drm_device *dev, struct drm_framebuffer *fb,\n \tret = __drm_mode_object_add(dev, &fb->base, DRM_MODE_OBJECT_FB,\n \t\t\t\t    false, drm_framebuffer_free);\n \tif (ret)\n-\t\tgoto out;\n+\t\tgoto err;\n \n \tmutex_lock(&dev->mode_config.fb_lock);\n \tdev->mode_config.num_fb++;\n@@ -883,7 +895,16 @@ int drm_framebuffer_init(struct drm_device *dev, struct drm_framebuffer *fb,\n \tmutex_unlock(&dev->mode_config.fb_lock);\n \n \tdrm_mode_object_register(dev, &fb->base);\n-out:\n+\n+\treturn 0;\n+\n+err:\n+\tfor (i = 0; i < fb->format->num_planes; i++) {\n+\t\tif (fb->internal_flags & DRM_FRAMEBUFFER_HAS_HANDLE_REF(i)) {\n+\t\t\tdrm_gem_object_handle_put_unlocked(fb->obj[i]);\n+\t\t\tfb->internal_flags &= ~DRM_FRAMEBUFFER_HAS_HANDLE_REF(i);\n+\t\t}\n+\t}\n \treturn ret;\n }\n EXPORT_SYMBOL(drm_framebuffer_init);\n@@ -960,6 +981,12 @@ EXPORT_SYMBOL(drm_framebuffer_unregister_private);\n void drm_framebuffer_cleanup(struct drm_framebuffer *fb)\n {\n \tstruct drm_device *dev = fb->dev;\n+\tunsigned int i;\n+\n+\tfor (i = 0; i < fb->format->num_planes; i++) {\n+\t\tif (fb->internal_flags & DRM_FRAMEBUFFER_HAS_HANDLE_REF(i))\n+\t\t\tdrm_gem_object_handle_put_unlocked(fb->obj[i]);\n+\t}\n \n \tmutex_lock(&dev->mode_config.fb_lock);\n \tlist_del(&fb->head);\ndiff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c\nindex 4bf0a76bb35e..ac0524595bd6 100644\n--- a/drivers/gpu/drm/drm_gem.c\n+++ b/drivers/gpu/drm/drm_gem.c\n@@ -223,23 +223,34 @@ static void drm_gem_object_handle_get(struct drm_gem_object *obj)\n }\n \n /**\n- * drm_gem_object_handle_get_unlocked - acquire reference on user-space handles\n+ * drm_gem_object_handle_get_if_exists_unlocked - acquire reference on user-space handle, if any\n  * @obj: GEM object\n  *\n- * Acquires a reference on the GEM buffer object's handle. Required\n- * to keep the GEM object alive. Call drm_gem_object_handle_put_unlocked()\n- * to release the reference.\n+ * Acquires a reference on the GEM buffer object's handle. Required to keep\n+ * the GEM object alive. Call drm_gem_object_handle_put_if_exists_unlocked()\n+ * to release the reference. Does nothing if the buffer object has no handle.\n+ *\n+ * Returns:\n+ * True if a handle exists, or false otherwise\n  */\n-void drm_gem_object_handle_get_unlocked(struct drm_gem_object *obj)\n+bool drm_gem_object_handle_get_if_exists_unlocked(struct drm_gem_object *obj)\n {\n \tstruct drm_device *dev = obj->dev;\n \n \tguard(mutex)(&dev->object_name_lock);\n \n-\tdrm_WARN_ON(dev, !obj->handle_count); /* first ref taken in create-tail helper */\n+\t/*\n+\t * First ref taken during GEM object creation, if any. Some\n+\t * drivers set up internal framebuffers with GEM objects that\n+\t * do not have a GEM handle. Hence, this counter can be zero.\n+\t */\n+\tif (!obj->handle_count)\n+\t\treturn false;\n+\n \tdrm_gem_object_handle_get(obj);\n+\n+\treturn true;\n }\n-EXPORT_SYMBOL(drm_gem_object_handle_get_unlocked);\n \n /**\n  * drm_gem_object_handle_free - release resources bound to userspace handles\n@@ -272,7 +283,7 @@ static void drm_gem_object_exported_dma_buf_free(struct drm_gem_object *obj)\n }\n \n /**\n- * drm_gem_object_handle_put_unlocked - releases reference on user-space handles\n+ * drm_gem_object_handle_put_unlocked - releases reference on user-space handle\n  * @obj: GEM object\n  *\n  * Releases a reference on the GEM buffer object's handle. Possibly releases\n@@ -283,14 +294,14 @@ void drm_gem_object_handle_put_unlocked(struct drm_gem_object *obj)\n \tstruct drm_device *dev = obj->dev;\n \tbool final = false;\n \n-\tif (WARN_ON(READ_ONCE(obj->handle_count) == 0))\n+\tif (drm_WARN_ON(dev, READ_ONCE(obj->handle_count) == 0))\n \t\treturn;\n \n \t/*\n-\t* Must bump handle count first as this may be the last\n-\t* ref, in which case the object would disappear before we\n-\t* checked for a name\n-\t*/\n+\t * Must bump handle count first as this may be the last\n+\t * ref, in which case the object would disappear before\n+\t * we checked for a name.\n+\t */\n \n \tmutex_lock(&dev->object_name_lock);\n \tif (--obj->handle_count == 0) {\n@@ -303,7 +314,6 @@ void drm_gem_object_handle_put_unlocked(struct drm_gem_object *obj)\n \tif (final)\n \t\tdrm_gem_object_put(obj);\n }\n-EXPORT_SYMBOL(drm_gem_object_handle_put_unlocked);\n \n /*\n  * Called at device or object close to release the file's\n@@ -315,6 +325,9 @@ drm_gem_object_release_handle(int id, void *ptr, void *data)\n \tstruct drm_file *file_priv = data;\n \tstruct drm_gem_object *obj = ptr;\n \n+\tif (drm_WARN_ON(obj->dev, !data))\n+\t\treturn 0;\n+\n \tif (obj->funcs->close)\n \t\tobj->funcs->close(obj, file_priv);\n \n@@ -435,7 +448,7 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,\n \tidr_preload(GFP_KERNEL);\n \tspin_lock(&file_priv->table_lock);\n \n-\tret = idr_alloc(&file_priv->object_idr, obj, 1, 0, GFP_NOWAIT);\n+\tret = idr_alloc(&file_priv->object_idr, NULL, 1, 0, GFP_NOWAIT);\n \n \tspin_unlock(&file_priv->table_lock);\n \tidr_preload_end();\n@@ -456,6 +469,11 @@ drm_gem_handle_create_tail(struct drm_file *file_priv,\n \t\t\tgoto err_revoke;\n \t}\n \n+\t/* mirrors drm_gem_handle_delete to avoid races */\n+\tspin_lock(&file_priv->table_lock);\n+\tobj = idr_replace(&file_priv->object_idr, obj, handle);\n+\tWARN_ON(obj != NULL);\n+\tspin_unlock(&file_priv->table_lock);\n \t*handlep = handle;\n \treturn 0;\n \ndiff --git a/drivers/gpu/drm/drm_gem_framebuffer_helper.c b/drivers/gpu/drm/drm_gem_framebuffer_helper.c\nindex 14a87788695d..6f72e7a0f427 100644\n--- a/drivers/gpu/drm/drm_gem_framebuffer_helper.c\n+++ b/drivers/gpu/drm/drm_gem_framebuffer_helper.c\n@@ -99,7 +99,7 @@ void drm_gem_fb_destroy(struct drm_framebuffer *fb)\n \tunsigned int i;\n \n \tfor (i = 0; i < fb->format->num_planes; i++)\n-\t\tdrm_gem_object_handle_put_unlocked(fb->obj[i]);\n+\t\tdrm_gem_object_put(fb->obj[i]);\n \n \tdrm_framebuffer_cleanup(fb);\n \tkfree(fb);\n@@ -182,10 +182,8 @@ int drm_gem_fb_init_with_funcs(struct drm_device *dev,\n \t\tif (!objs[i]) {\n \t\t\tdrm_dbg_kms(dev, \"Failed to lookup GEM object\\n\");\n \t\t\tret = -ENOENT;\n-\t\t\tgoto err_gem_object_handle_put_unlocked;\n+\t\t\tgoto err_gem_object_put;\n \t\t}\n-\t\tdrm_gem_object_handle_get_unlocked(objs[i]);\n-\t\tdrm_gem_object_put(objs[i]);\n \n \t\tmin_size = (height - 1) * mode_cmd->pitches[i]\n \t\t\t + drm_format_info_min_pitch(info, i, width)\n@@ -195,22 +193,22 @@ int drm_gem_fb_init_with_funcs(struct drm_device *dev,\n \t\t\tdrm_dbg_kms(dev,\n \t\t\t\t    \"GEM object size (%zu) smaller than minimum size (%u) for plane %d\\n\",\n \t\t\t\t    objs[i]->size, min_size, i);\n-\t\t\tdrm_gem_object_handle_put_unlocked(objs[i]);\n+\t\t\tdrm_gem_object_put(objs[i]);\n \t\t\tret = -EINVAL;\n-\t\t\tgoto err_gem_object_handle_put_unlocked;\n+\t\t\tgoto err_gem_object_put;\n \t\t}\n \t}\n \n \tret = drm_gem_fb_init(dev, fb, mode_cmd, objs, i, funcs);\n \tif (ret)\n-\t\tgoto err_gem_object_handle_put_unlocked;\n+\t\tgoto err_gem_object_put;\n \n \treturn 0;\n \n-err_gem_object_handle_put_unlocked:\n+err_gem_object_put:\n \twhile (i > 0) {\n \t\t--i;\n-\t\tdrm_gem_object_handle_put_unlocked(objs[i]);\n+\t\tdrm_gem_object_put(objs[i]);\n \t}\n \treturn ret;\n }\ndiff --git a/drivers/gpu/drm/drm_internal.h b/drivers/gpu/drm/drm_internal.h\nindex be77d61a16ce..60c282881958 100644\n--- a/drivers/gpu/drm/drm_internal.h\n+++ b/drivers/gpu/drm/drm_internal.h\n@@ -161,7 +161,7 @@ void drm_sysfs_lease_event(struct drm_device *dev);\n \n /* drm_gem.c */\n int drm_gem_init(struct drm_device *dev);\n-void drm_gem_object_handle_get_unlocked(struct drm_gem_object *obj);\n+bool drm_gem_object_handle_get_if_exists_unlocked(struct drm_gem_object *obj);\n void drm_gem_object_handle_put_unlocked(struct drm_gem_object *obj);\n int drm_gem_handle_create_tail(struct drm_file *file_priv,\n \t\t\t       struct drm_gem_object *obj,\ndiff --git a/drivers/gpu/drm/drm_panic_qr.rs b/drivers/gpu/drm/drm_panic_qr.rs\nindex dd55b1cb764d..18492daae4b3 100644\n--- a/drivers/gpu/drm/drm_panic_qr.rs\n+++ b/drivers/gpu/drm/drm_panic_qr.rs\n@@ -27,7 +27,7 @@\n //! * <https://github.com/erwanvivien/fast_qr>\n //! * <https://github.com/bjguillot/qr>\n \n-use kernel::{prelude::*, str::CStr};\n+use kernel::prelude::*;\n \n #[derive(Debug, Clone, Copy, PartialEq, Eq, Ord, PartialOrd)]\n struct Version(usize);\ndiff --git a/drivers/gpu/drm/i915/display/intel_bios.c b/drivers/gpu/drm/i915/display/intel_bios.c\nindex ba7b8938b17c..166ee11831ab 100644\n--- a/drivers/gpu/drm/i915/display/intel_bios.c\n+++ b/drivers/gpu/drm/i915/display/intel_bios.c\n@@ -1938,7 +1938,7 @@ static int get_init_otp_deassert_fragment_len(struct intel_display *display,\n \tint index, len;\n \n \tif (drm_WARN_ON(display->drm,\n-\t\t\t!data || panel->vbt.dsi.seq_version != 1))\n+\t\t\t!data || panel->vbt.dsi.seq_version >= 3))\n \t\treturn 0;\n \n \t/* index = 1 to skip sequence byte */\n@@ -1961,7 +1961,7 @@ static int get_init_otp_deassert_fragment_len(struct intel_display *display,\n }\n \n /*\n- * Some v1 VBT MIPI sequences do the deassert in the init OTP sequence.\n+ * Some v1/v2 VBT MIPI sequences do the deassert in the init OTP sequence.\n  * The deassert must be done before calling intel_dsi_device_ready, so for\n  * these devices we split the init OTP sequence into a deassert sequence and\n  * the actual init OTP part.\n@@ -1972,9 +1972,9 @@ static void vlv_fixup_mipi_sequences(struct intel_display *display,\n \tu8 *init_otp;\n \tint len;\n \n-\t/* Limit this to v1 vid-mode sequences */\n+\t/* Limit this to v1/v2 vid-mode sequences */\n \tif (panel->vbt.dsi.config->is_cmd_mode ||\n-\t    panel->vbt.dsi.seq_version != 1)\n+\t    panel->vbt.dsi.seq_version >= 3)\n \t\treturn;\n \n \t/* Only do this if there are otp and assert seqs and no deassert seq */\ndiff --git a/drivers/gpu/drm/imagination/pvr_power.c b/drivers/gpu/drm/imagination/pvr_power.c\nindex 41f5d89e78b8..3e349d039fc0 100644\n--- a/drivers/gpu/drm/imagination/pvr_power.c\n+++ b/drivers/gpu/drm/imagination/pvr_power.c\n@@ -386,13 +386,13 @@ pvr_power_reset(struct pvr_device *pvr_dev, bool hard_reset)\n \t\tif (!err) {\n \t\t\tif (hard_reset) {\n \t\t\t\tpvr_dev->fw_dev.booted = false;\n-\t\t\t\tWARN_ON(pm_runtime_force_suspend(from_pvr_device(pvr_dev)->dev));\n+\t\t\t\tWARN_ON(pvr_power_device_suspend(from_pvr_device(pvr_dev)->dev));\n \n \t\t\t\terr = pvr_fw_hard_reset(pvr_dev);\n \t\t\t\tif (err)\n \t\t\t\t\tgoto err_device_lost;\n \n-\t\t\t\terr = pm_runtime_force_resume(from_pvr_device(pvr_dev)->dev);\n+\t\t\t\terr = pvr_power_device_resume(from_pvr_device(pvr_dev)->dev);\n \t\t\t\tpvr_dev->fw_dev.booted = true;\n \t\t\t\tif (err)\n \t\t\t\t\tgoto err_device_lost;\ndiff --git a/drivers/gpu/drm/nouveau/nouveau_debugfs.c b/drivers/gpu/drm/nouveau/nouveau_debugfs.c\nindex 200e65a7cefc..c7869a639bef 100644\n--- a/drivers/gpu/drm/nouveau/nouveau_debugfs.c\n+++ b/drivers/gpu/drm/nouveau/nouveau_debugfs.c\n@@ -314,14 +314,10 @@ nouveau_debugfs_fini(struct nouveau_drm *drm)\n \tdrm->debugfs = NULL;\n }\n \n-int\n+void\n nouveau_module_debugfs_init(void)\n {\n \tnouveau_debugfs_root = debugfs_create_dir(\"nouveau\", NULL);\n-\tif (IS_ERR(nouveau_debugfs_root))\n-\t\treturn PTR_ERR(nouveau_debugfs_root);\n-\n-\treturn 0;\n }\n \n void\ndiff --git a/drivers/gpu/drm/nouveau/nouveau_debugfs.h b/drivers/gpu/drm/nouveau/nouveau_debugfs.h\nindex b7617b344ee2..d05ed0e641c4 100644\n--- a/drivers/gpu/drm/nouveau/nouveau_debugfs.h\n+++ b/drivers/gpu/drm/nouveau/nouveau_debugfs.h\n@@ -24,7 +24,7 @@ extern void nouveau_debugfs_fini(struct nouveau_drm *);\n \n extern struct dentry *nouveau_debugfs_root;\n \n-int  nouveau_module_debugfs_init(void);\n+void nouveau_module_debugfs_init(void);\n void nouveau_module_debugfs_fini(void);\n #else\n static inline void\n@@ -42,10 +42,9 @@ nouveau_debugfs_fini(struct nouveau_drm *drm)\n {\n }\n \n-static inline int\n+static inline void\n nouveau_module_debugfs_init(void)\n {\n-\treturn 0;\n }\n \n static inline void\ndiff --git a/drivers/gpu/drm/nouveau/nouveau_drm.c b/drivers/gpu/drm/nouveau/nouveau_drm.c\nindex 0c82a63cd49d..1527b801f013 100644\n--- a/drivers/gpu/drm/nouveau/nouveau_drm.c\n+++ b/drivers/gpu/drm/nouveau/nouveau_drm.c\n@@ -1461,9 +1461,7 @@ nouveau_drm_init(void)\n \tif (!nouveau_modeset)\n \t\treturn 0;\n \n-\tret = nouveau_module_debugfs_init();\n-\tif (ret)\n-\t\treturn ret;\n+\tnouveau_module_debugfs_init();\n \n #ifdef CONFIG_NOUVEAU_PLATFORM_DRIVER\n \tplatform_driver_register(&nouveau_platform_driver);\ndiff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/gsp.c b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/gsp.c\nindex baf42339f93e..588cb4ab85cb 100644\n--- a/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/gsp.c\n+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/gsp/rm/r535/gsp.c\n@@ -719,7 +719,6 @@ r535_gsp_acpi_caps(acpi_handle handle, CAPS_METHOD_DATA *caps)\n \tunion acpi_object argv4 = {\n \t\t.buffer.type    = ACPI_TYPE_BUFFER,\n \t\t.buffer.length  = 4,\n-\t\t.buffer.pointer = kmalloc(argv4.buffer.length, GFP_KERNEL),\n \t}, *obj;\n \n \tcaps->status = 0xffff;\n@@ -727,17 +726,22 @@ r535_gsp_acpi_caps(acpi_handle handle, CAPS_METHOD_DATA *caps)\n \tif (!acpi_check_dsm(handle, &NVOP_DSM_GUID, NVOP_DSM_REV, BIT_ULL(0x1a)))\n \t\treturn;\n \n+\targv4.buffer.pointer = kmalloc(argv4.buffer.length, GFP_KERNEL);\n+\tif (!argv4.buffer.pointer)\n+\t\treturn;\n+\n \tobj = acpi_evaluate_dsm(handle, &NVOP_DSM_GUID, NVOP_DSM_REV, 0x1a, &argv4);\n \tif (!obj)\n-\t\treturn;\n+\t\tgoto done;\n \n \tif (WARN_ON(obj->type != ACPI_TYPE_BUFFER) ||\n \t    WARN_ON(obj->buffer.length != 4))\n-\t\treturn;\n+\t\tgoto done;\n \n \tcaps->status = 0;\n \tcaps->optimusCaps = *(u32 *)obj->buffer.pointer;\n \n+done:\n \tACPI_FREE(obj);\n \n \tkfree(argv4.buffer.pointer);\n@@ -754,24 +758,28 @@ r535_gsp_acpi_jt(acpi_handle handle, JT_METHOD_DATA *jt)\n \tunion acpi_object argv4 = {\n \t\t.buffer.type    = ACPI_TYPE_BUFFER,\n \t\t.buffer.length  = sizeof(caps),\n-\t\t.buffer.pointer = kmalloc(argv4.buffer.length, GFP_KERNEL),\n \t}, *obj;\n \n \tjt->status = 0xffff;\n \n+\targv4.buffer.pointer = kmalloc(argv4.buffer.length, GFP_KERNEL);\n+\tif (!argv4.buffer.pointer)\n+\t\treturn;\n+\n \tobj = acpi_evaluate_dsm(handle, &JT_DSM_GUID, JT_DSM_REV, 0x1, &argv4);\n \tif (!obj)\n-\t\treturn;\n+\t\tgoto done;\n \n \tif (WARN_ON(obj->type != ACPI_TYPE_BUFFER) ||\n \t    WARN_ON(obj->buffer.length != 4))\n-\t\treturn;\n+\t\tgoto done;\n \n \tjt->status = 0;\n \tjt->jtCaps = *(u32 *)obj->buffer.pointer;\n \tjt->jtRevId = (jt->jtCaps & 0xfff00000) >> 20;\n \tjt->bSBIOSCaps = 0;\n \n+done:\n \tACPI_FREE(obj);\n \n \tkfree(argv4.buffer.pointer);\n@@ -1744,6 +1752,13 @@ r535_gsp_fini(struct nvkm_gsp *gsp, bool suspend)\n \t\t\tnvkm_gsp_sg_free(gsp->subdev.device, &gsp->sr.sgt);\n \t\t\treturn ret;\n \t\t}\n+\n+\t\t/*\n+\t\t * TODO: Debug the GSP firmware / RPC handling to find out why\n+\t\t * without this Turing (but none of the other architectures)\n+\t\t * ends up resetting all channels after resume.\n+\t\t */\n+\t\tmsleep(50);\n \t}\n \n \tret = r535_gsp_rpc_unloading_guest_driver(gsp, suspend);\ndiff --git a/drivers/gpu/drm/tegra/nvdec.c b/drivers/gpu/drm/tegra/nvdec.c\nindex 2d9a0a3f6c38..7a38664e890e 100644\n--- a/drivers/gpu/drm/tegra/nvdec.c\n+++ b/drivers/gpu/drm/tegra/nvdec.c\n@@ -261,10 +261,8 @@ static int nvdec_load_falcon_firmware(struct nvdec *nvdec)\n \n \tif (!client->group) {\n \t\tvirt = dma_alloc_coherent(nvdec->dev, size, &iova, GFP_KERNEL);\n-\n-\t\terr = dma_mapping_error(nvdec->dev, iova);\n-\t\tif (err < 0)\n-\t\t\treturn err;\n+\t\tif (!virt)\n+\t\t\treturn -ENOMEM;\n \t} else {\n \t\tvirt = tegra_drm_alloc(tegra, size, &iova);\n \t\tif (IS_ERR(virt))\ndiff --git a/drivers/gpu/drm/xe/xe_devcoredump.c b/drivers/gpu/drm/xe/xe_devcoredump.c\nindex 7a8af2311318..11e60d687572 100644\n--- a/drivers/gpu/drm/xe/xe_devcoredump.c\n+++ b/drivers/gpu/drm/xe/xe_devcoredump.c\n@@ -171,14 +171,32 @@ static void xe_devcoredump_snapshot_free(struct xe_devcoredump_snapshot *ss)\n \n #define XE_DEVCOREDUMP_CHUNK_MAX\t(SZ_512M + SZ_1G)\n \n+/**\n+ * xe_devcoredump_read() - Read data from the Xe device coredump snapshot\n+ * @buffer: Destination buffer to copy the coredump data into\n+ * @offset: Offset in the coredump data to start reading from\n+ * @count: Number of bytes to read\n+ * @data: Pointer to the xe_devcoredump structure\n+ * @datalen: Length of the data (unused)\n+ *\n+ * Reads a chunk of the coredump snapshot data into the provided buffer.\n+ * If the devcoredump is smaller than 1.5 GB (XE_DEVCOREDUMP_CHUNK_MAX),\n+ * it is read directly from a pre-written buffer. For larger devcoredumps,\n+ * the pre-written buffer must be periodically repopulated from the snapshot\n+ * state due to kmalloc size limitations.\n+ *\n+ * Return: Number of bytes copied on success, or a negative error code on failure.\n+ */\n static ssize_t xe_devcoredump_read(char *buffer, loff_t offset,\n \t\t\t\t   size_t count, void *data, size_t datalen)\n {\n \tstruct xe_devcoredump *coredump = data;\n \tstruct xe_devcoredump_snapshot *ss;\n-\tssize_t byte_copied;\n+\tssize_t byte_copied = 0;\n \tu32 chunk_offset;\n \tssize_t new_chunk_position;\n+\tbool pm_needed = false;\n+\tint ret = 0;\n \n \tif (!coredump)\n \t\treturn -ENODEV;\n@@ -188,20 +206,19 @@ static ssize_t xe_devcoredump_read(char *buffer, loff_t offset,\n \t/* Ensure delayed work is captured before continuing */\n \tflush_work(&ss->work);\n \n-\tif (ss->read.size > XE_DEVCOREDUMP_CHUNK_MAX)\n+\tpm_needed = ss->read.size > XE_DEVCOREDUMP_CHUNK_MAX;\n+\tif (pm_needed)\n \t\txe_pm_runtime_get(gt_to_xe(ss->gt));\n \n \tmutex_lock(&coredump->lock);\n \n \tif (!ss->read.buffer) {\n-\t\tmutex_unlock(&coredump->lock);\n-\t\treturn -ENODEV;\n+\t\tret = -ENODEV;\n+\t\tgoto unlock;\n \t}\n \n-\tif (offset >= ss->read.size) {\n-\t\tmutex_unlock(&coredump->lock);\n-\t\treturn 0;\n-\t}\n+\tif (offset >= ss->read.size)\n+\t\tgoto unlock;\n \n \tnew_chunk_position = div_u64_rem(offset,\n \t\t\t\t\t XE_DEVCOREDUMP_CHUNK_MAX,\n@@ -221,12 +238,13 @@ static ssize_t xe_devcoredump_read(char *buffer, loff_t offset,\n \t\tss->read.size - offset;\n \tmemcpy(buffer, ss->read.buffer + chunk_offset, byte_copied);\n \n+unlock:\n \tmutex_unlock(&coredump->lock);\n \n-\tif (ss->read.size > XE_DEVCOREDUMP_CHUNK_MAX)\n+\tif (pm_needed)\n \t\txe_pm_runtime_put(gt_to_xe(ss->gt));\n \n-\treturn byte_copied;\n+\treturn byte_copied ? byte_copied : ret;\n }\n \n static void xe_devcoredump_free(void *data)\ndiff --git a/drivers/gpu/drm/xe/xe_gt_pagefault.c b/drivers/gpu/drm/xe/xe_gt_pagefault.c\nindex 10622ca471a2..6717a636b1d9 100644\n--- a/drivers/gpu/drm/xe/xe_gt_pagefault.c\n+++ b/drivers/gpu/drm/xe/xe_gt_pagefault.c\n@@ -444,6 +444,7 @@ static int xe_alloc_pf_queue(struct xe_gt *gt, struct pf_queue *pf_queue)\n #define PF_MULTIPLIER\t8\n \tpf_queue->num_dw =\n \t\t(num_eus + XE_NUM_HW_ENGINES) * PF_MSG_LEN_DW * PF_MULTIPLIER;\n+\tpf_queue->num_dw = roundup_pow_of_two(pf_queue->num_dw);\n #undef PF_MULTIPLIER\n \n \tpf_queue->gt = gt;\ndiff --git a/drivers/gpu/drm/xe/xe_lmtt.c b/drivers/gpu/drm/xe/xe_lmtt.c\nindex 63db66df064b..023ed6a6b49d 100644\n--- a/drivers/gpu/drm/xe/xe_lmtt.c\n+++ b/drivers/gpu/drm/xe/xe_lmtt.c\n@@ -78,6 +78,9 @@ static struct xe_lmtt_pt *lmtt_pt_alloc(struct xe_lmtt *lmtt, unsigned int level\n \t}\n \n \tlmtt_assert(lmtt, xe_bo_is_vram(bo));\n+\tlmtt_debug(lmtt, \"level=%u addr=%#llx\\n\", level, (u64)xe_bo_main_addr(bo, XE_PAGE_SIZE));\n+\n+\txe_map_memset(lmtt_to_xe(lmtt), &bo->vmap, 0, 0, bo->size);\n \n \tpt->level = level;\n \tpt->bo = bo;\n@@ -91,6 +94,9 @@ static struct xe_lmtt_pt *lmtt_pt_alloc(struct xe_lmtt *lmtt, unsigned int level\n \n static void lmtt_pt_free(struct xe_lmtt_pt *pt)\n {\n+\tlmtt_debug(&pt->bo->tile->sriov.pf.lmtt, \"level=%u addr=%llx\\n\",\n+\t\t   pt->level, (u64)xe_bo_main_addr(pt->bo, XE_PAGE_SIZE));\n+\n \txe_bo_unpin_map_no_vm(pt->bo);\n \tkfree(pt);\n }\n@@ -226,9 +232,14 @@ static void lmtt_write_pte(struct xe_lmtt *lmtt, struct xe_lmtt_pt *pt,\n \n \tswitch (lmtt->ops->lmtt_pte_size(level)) {\n \tcase sizeof(u32):\n+\t\tlmtt_assert(lmtt, !overflows_type(pte, u32));\n+\t\tlmtt_assert(lmtt, !pte || !iosys_map_rd(&pt->bo->vmap, idx * sizeof(u32), u32));\n+\n \t\txe_map_wr(lmtt_to_xe(lmtt), &pt->bo->vmap, idx * sizeof(u32), u32, pte);\n \t\tbreak;\n \tcase sizeof(u64):\n+\t\tlmtt_assert(lmtt, !pte || !iosys_map_rd(&pt->bo->vmap, idx * sizeof(u64), u64));\n+\n \t\txe_map_wr(lmtt_to_xe(lmtt), &pt->bo->vmap, idx * sizeof(u64), u64, pte);\n \t\tbreak;\n \tdefault:\ndiff --git a/drivers/gpu/drm/xe/xe_migrate.c b/drivers/gpu/drm/xe/xe_migrate.c\nindex 7acdc4c78866..66bc02302c55 100644\n--- a/drivers/gpu/drm/xe/xe_migrate.c\n+++ b/drivers/gpu/drm/xe/xe_migrate.c\n@@ -863,7 +863,7 @@ struct dma_fence *xe_migrate_copy(struct xe_migrate *m,\n \t\tif (src_is_vram && xe_migrate_allow_identity(src_L0, &src_it))\n \t\t\txe_res_next(&src_it, src_L0);\n \t\telse\n-\t\t\temit_pte(m, bb, src_L0_pt, src_is_vram, copy_system_ccs,\n+\t\t\temit_pte(m, bb, src_L0_pt, src_is_vram, copy_system_ccs || use_comp_pat,\n \t\t\t\t &src_it, src_L0, src);\n \n \t\tif (dst_is_vram && xe_migrate_allow_identity(src_L0, &dst_it))\ndiff --git a/drivers/gpu/drm/xe/xe_module.c b/drivers/gpu/drm/xe/xe_module.c\nindex e4742e27e2cd..da6793c2f991 100644\n--- a/drivers/gpu/drm/xe/xe_module.c\n+++ b/drivers/gpu/drm/xe/xe_module.c\n@@ -20,7 +20,7 @@\n \n struct xe_modparam xe_modparam = {\n \t.probe_display = true,\n-\t.guc_log_level = 3,\n+\t.guc_log_level = IS_ENABLED(CONFIG_DRM_XE_DEBUG) ? 3 : 1,\n \t.force_probe = CONFIG_DRM_XE_FORCE_PROBE,\n \t.wedged_mode = 1,\n \t.svm_notifier_size = 512,\ndiff --git a/drivers/gpu/drm/xe/xe_pci.c b/drivers/gpu/drm/xe/xe_pci.c\nindex ac4beaed58ff..278af53c74dc 100644\n--- a/drivers/gpu/drm/xe/xe_pci.c\n+++ b/drivers/gpu/drm/xe/xe_pci.c\n@@ -140,7 +140,6 @@ static const struct xe_graphics_desc graphics_xelpg = {\n \t.has_asid = 1, \\\n \t.has_atomic_enable_pte_bit = 1, \\\n \t.has_flat_ccs = 1, \\\n-\t.has_indirect_ring_state = 1, \\\n \t.has_range_tlb_invalidation = 1, \\\n \t.has_usm = 1, \\\n \t.has_64bit_timestamp = 1, \\\ndiff --git a/drivers/gpu/drm/xe/xe_pm.c b/drivers/gpu/drm/xe/xe_pm.c\nindex ff749edc005b..ad263de44111 100644\n--- a/drivers/gpu/drm/xe/xe_pm.c\n+++ b/drivers/gpu/drm/xe/xe_pm.c\n@@ -134,7 +134,7 @@ int xe_pm_suspend(struct xe_device *xe)\n \t/* FIXME: Super racey... */\n \terr = xe_bo_evict_all(xe);\n \tif (err)\n-\t\tgoto err_pxp;\n+\t\tgoto err_display;\n \n \tfor_each_gt(gt, xe, id) {\n \t\terr = xe_gt_suspend(gt);\n@@ -151,7 +151,6 @@ int xe_pm_suspend(struct xe_device *xe)\n \n err_display:\n \txe_display_pm_resume(xe);\n-err_pxp:\n \txe_pxp_pm_resume(xe->pxp);\n err:\n \tdrm_dbg(&xe->drm, \"Device suspend failed %d\\n\", err);\n@@ -753,11 +752,13 @@ void xe_pm_assert_unbounded_bridge(struct xe_device *xe)\n }\n \n /**\n- * xe_pm_set_vram_threshold - Set a vram threshold for allowing/blocking D3Cold\n+ * xe_pm_set_vram_threshold - Set a VRAM threshold for allowing/blocking D3Cold\n  * @xe: xe device instance\n- * @threshold: VRAM size in bites for the D3cold threshold\n+ * @threshold: VRAM size in MiB for the D3cold threshold\n  *\n- * Returns 0 for success, negative error code otherwise.\n+ * Return:\n+ * * 0\t\t- success\n+ * * -EINVAL\t- invalid argument\n  */\n int xe_pm_set_vram_threshold(struct xe_device *xe, u32 threshold)\n {\ndiff --git a/drivers/gpu/drm/xe/xe_uc_fw.c b/drivers/gpu/drm/xe/xe_uc_fw.c\nindex 2741849bbf4d..a6612105201a 100644\n--- a/drivers/gpu/drm/xe/xe_uc_fw.c\n+++ b/drivers/gpu/drm/xe/xe_uc_fw.c\n@@ -114,10 +114,10 @@ struct fw_blobs_by_type {\n #define XE_GT_TYPE_ANY XE_GT_TYPE_UNINITIALIZED\n \n #define XE_GUC_FIRMWARE_DEFS(fw_def, mmp_ver, major_ver)\t\t\t\t\t\\\n-\tfw_def(BATTLEMAGE,\tGT_TYPE_ANY,\tmajor_ver(xe,\tguc,\tbmg,\t70, 44, 1))\t\\\n-\tfw_def(LUNARLAKE,\tGT_TYPE_ANY,\tmajor_ver(xe,\tguc,\tlnl,\t70, 44, 1))\t\\\n+\tfw_def(BATTLEMAGE,\tGT_TYPE_ANY,\tmajor_ver(xe,\tguc,\tbmg,\t70, 45, 2))\t\\\n+\tfw_def(LUNARLAKE,\tGT_TYPE_ANY,\tmajor_ver(xe,\tguc,\tlnl,\t70, 45, 2))\t\\\n \tfw_def(METEORLAKE,\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\tmtl,\t70, 44, 1))\t\\\n-\tfw_def(DG2,\t\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\tdg2,\t70, 44, 1))\t\\\n+\tfw_def(DG2,\t\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\tdg2,\t70, 45, 2))\t\\\n \tfw_def(DG1,\t\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\tdg1,\t70, 44, 1))\t\\\n \tfw_def(ALDERLAKE_N,\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\ttgl,\t70, 44, 1))\t\\\n \tfw_def(ALDERLAKE_P,\tGT_TYPE_ANY,\tmajor_ver(i915,\tguc,\tadlp,\t70, 44, 1))\t\\\ndiff --git a/drivers/gpu/drm/xe/xe_wa_oob.rules b/drivers/gpu/drm/xe/xe_wa_oob.rules\nindex 69c1d7fc695e..6d70109fcc43 100644\n--- a/drivers/gpu/drm/xe/xe_wa_oob.rules\n+++ b/drivers/gpu/drm/xe/xe_wa_oob.rules\n@@ -38,10 +38,10 @@\n \t\tGRAPHICS_VERSION(2004)\n \t\tGRAPHICS_VERSION_RANGE(3000, 3001)\n 22019338487\tMEDIA_VERSION(2000)\n-\t\tGRAPHICS_VERSION(2001)\n+\t\tGRAPHICS_VERSION(2001), FUNC(xe_rtp_match_not_sriov_vf)\n \t\tMEDIA_VERSION(3000), MEDIA_STEP(A0, B0), FUNC(xe_rtp_match_not_sriov_vf)\n 22019338487_display\tPLATFORM(LUNARLAKE)\n-16023588340\tGRAPHICS_VERSION(2001)\n+16023588340\tGRAPHICS_VERSION(2001), FUNC(xe_rtp_match_not_sriov_vf)\n 14019789679\tGRAPHICS_VERSION(1255)\n \t\tGRAPHICS_VERSION_RANGE(1270, 2004)\n no_media_l3\tMEDIA_VERSION(3000)\ndiff --git a/include/drm/drm_file.h b/include/drm/drm_file.h\nindex 5c3b2aa3e69d..d344d41e6cfe 100644\n--- a/include/drm/drm_file.h\n+++ b/include/drm/drm_file.h\n@@ -300,6 +300,9 @@ struct drm_file {\n \t *\n \t * Mapping of mm object handles to object pointers. Used by the GEM\n \t * subsystem. Protected by @table_lock.\n+\t *\n+\t * Note that allocated entries might be NULL as a transient state when\n+\t * creating or deleting a handle.\n \t */\n \tstruct idr object_idr;\n \ndiff --git a/include/drm/drm_framebuffer.h b/include/drm/drm_framebuffer.h\nindex 668077009fce..38b24fc8978d 100644\n--- a/include/drm/drm_framebuffer.h\n+++ b/include/drm/drm_framebuffer.h\n@@ -23,6 +23,7 @@\n #ifndef __DRM_FRAMEBUFFER_H__\n #define __DRM_FRAMEBUFFER_H__\n \n+#include <linux/bits.h>\n #include <linux/ctype.h>\n #include <linux/list.h>\n #include <linux/sched.h>\n@@ -100,6 +101,8 @@ struct drm_framebuffer_funcs {\n \t\t     unsigned num_clips);\n };\n \n+#define DRM_FRAMEBUFFER_HAS_HANDLE_REF(_i)\tBIT(0u + (_i))\n+\n /**\n  * struct drm_framebuffer - frame buffer object\n  *\n@@ -188,6 +191,10 @@ struct drm_framebuffer {\n \t * DRM_MODE_FB_MODIFIERS.\n \t */\n \tint flags;\n+\t/**\n+\t * @internal_flags: Framebuffer flags like DRM_FRAMEBUFFER_HAS_HANDLE_REF.\n+\t */\n+\tunsigned int internal_flags;\n \t/**\n \t * @filp_head: Placed on &drm_file.fbs, protected by &drm_file.fbs_lock.\n \t */\ndiff --git a/rust/kernel/drm/device.rs b/rust/kernel/drm/device.rs\nindex 624d7a4c83ea..14c1aa402951 100644\n--- a/rust/kernel/drm/device.rs\n+++ b/rust/kernel/drm/device.rs\n@@ -66,7 +66,7 @@ impl<T: drm::Driver> Device<T> {\n         open: Some(drm::File::<T::File>::open_callback),\n         postclose: Some(drm::File::<T::File>::postclose_callback),\n         unload: None,\n-        release: None,\n+        release: Some(Self::release),\n         master_set: None,\n         master_drop: None,\n         debugfs_init: None,\n@@ -162,6 +162,16 @@ pub unsafe fn as_ref<'a>(ptr: *const bindings::drm_device) -> &'a Self {\n         // SAFETY: `ptr` is valid by the safety requirements of this function.\n         unsafe { &*ptr.cast() }\n     }\n+\n+    extern \"C\" fn release(ptr: *mut bindings::drm_device) {\n+        // SAFETY: `ptr` is a valid pointer to a `struct drm_device` and embedded in `Self`.\n+        let this = unsafe { Self::from_drm_device(ptr) };\n+\n+        // SAFETY:\n+        // - When `release` runs it is guaranteed that there is no further access to `this`.\n+        // - `this` is valid for dropping.\n+        unsafe { core::ptr::drop_in_place(this) };\n+    }\n }\n \n impl<T: drm::Driver> Deref for Device<T> {\ndiff --git a/rust/kernel/drm/driver.rs b/rust/kernel/drm/driver.rs\nindex acb638086131..af93d46d03d3 100644\n--- a/rust/kernel/drm/driver.rs\n+++ b/rust/kernel/drm/driver.rs\n@@ -10,7 +10,6 @@\n     drm,\n     error::{to_result, Result},\n     prelude::*,\n-    str::CStr,\n     types::ARef,\n };\n use macros::vtable;",
    "stats": {
      "insertions": 187,
      "deletions": 89,
      "files": 27
    }
  },
  {
    "sha": "5f02b80c21e1511c32a37f642497751041069076",
    "message": "Revert \"eventpoll: Fix priority inversion problem\"\n\nThis reverts commit 8c44dac8add7503c345c0f6c7962e4863b88ba42.\n\nI haven't figured out what the actual bug in this commit is, but I did\nspend a lot of time chasing it down and eventually succeeded in\nbisecting it down to this.\n\nFor some reason, this eventpoll commit ends up causing delays and stuck\nuser space processes, but it only happens on one of my machines, and\nonly during early boot or during the flurry of initial activity when\nlogging in.\n\nI must be triggering some very subtle timing issue, but once I figured\nout the behavior pattern that made it reasonably reliable to trigger, it\ndid bisect right to this, and reverting the commit fixes the problem.\n\nOf course, that was only after I had failed at bisecting it several\ntimes, and had flailed around blaming both the drm people and the\nnetlink people for the odd problems.  The most obvious of which happened\nat the time of the first graphical login (the most common symptom being\nthat some gnome app aborted due to a 30s timeout, often leading to the\nwhole session then failing if it was some critical component like\ngnome-shell or similar).\n\nAcked-by: Nam Cao <namcao@linutronix.de>\nCc: Frederic Weisbecker <frederic@kernel.org>\nCc: Valentin Schneider <vschneid@redhat.com>\nCc: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
    "author": "Linus Torvalds",
    "date": "2025-07-11T17:10:32-07:00",
    "files_changed": [
      "fs/eventpoll.c"
    ],
    "diff": "diff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 895256cd2786..0fbf5dfedb24 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -137,7 +137,13 @@ struct epitem {\n \t};\n \n \t/* List header used to link this structure to the eventpoll ready list */\n-\tstruct llist_node rdllink;\n+\tstruct list_head rdllink;\n+\n+\t/*\n+\t * Works together \"struct eventpoll\"->ovflist in keeping the\n+\t * single linked chain of items.\n+\t */\n+\tstruct epitem *next;\n \n \t/* The file descriptor information this item refers to */\n \tstruct epoll_filefd ffd;\n@@ -185,15 +191,22 @@ struct eventpoll {\n \t/* Wait queue used by file->poll() */\n \twait_queue_head_t poll_wait;\n \n-\t/*\n-\t * List of ready file descriptors. Adding to this list is lockless. Items can be removed\n-\t * only with eventpoll::mtx\n-\t */\n-\tstruct llist_head rdllist;\n+\t/* List of ready file descriptors */\n+\tstruct list_head rdllist;\n+\n+\t/* Lock which protects rdllist and ovflist */\n+\trwlock_t lock;\n \n \t/* RB tree root used to store monitored fd structs */\n \tstruct rb_root_cached rbr;\n \n+\t/*\n+\t * This is a single linked list that chains all the \"struct epitem\" that\n+\t * happened while transferring ready events to userspace w/out\n+\t * holding ->lock.\n+\t */\n+\tstruct epitem *ovflist;\n+\n \t/* wakeup_source used when ep_send_events or __ep_eventpoll_poll is running */\n \tstruct wakeup_source *ws;\n \n@@ -348,14 +361,10 @@ static inline int ep_cmp_ffd(struct epoll_filefd *p1,\n \t        (p1->file < p2->file ? -1 : p1->fd - p2->fd));\n }\n \n-/*\n- * Add the item to its container eventpoll's rdllist; do nothing if the item is already on rdllist.\n- */\n-static void epitem_ready(struct epitem *epi)\n+/* Tells us if the item is currently linked */\n+static inline int ep_is_linked(struct epitem *epi)\n {\n-\tif (&epi->rdllink == cmpxchg(&epi->rdllink.next, &epi->rdllink, NULL))\n-\t\tllist_add(&epi->rdllink, &epi->ep->rdllist);\n-\n+\treturn !list_empty(&epi->rdllink);\n }\n \n static inline struct eppoll_entry *ep_pwq_from_wait(wait_queue_entry_t *p)\n@@ -374,26 +383,13 @@ static inline struct epitem *ep_item_from_wait(wait_queue_entry_t *p)\n  *\n  * @ep: Pointer to the eventpoll context.\n  *\n- * Return: true if ready events might be available, false otherwise.\n+ * Return: a value different than %zero if ready events are available,\n+ *          or %zero otherwise.\n  */\n-static inline bool ep_events_available(struct eventpoll *ep)\n+static inline int ep_events_available(struct eventpoll *ep)\n {\n-\tbool available;\n-\tint locked;\n-\n-\tlocked = mutex_trylock(&ep->mtx);\n-\tif (!locked) {\n-\t\t/*\n-\t\t * The lock held and someone might have removed all items while inspecting it. The\n-\t\t * llist_empty() check in this case is futile. Assume that something is enqueued and\n-\t\t * let ep_try_send_events() figure it out.\n-\t\t */\n-\t\treturn true;\n-\t}\n-\n-\tavailable = !llist_empty(&ep->rdllist);\n-\tmutex_unlock(&ep->mtx);\n-\treturn available;\n+\treturn !list_empty_careful(&ep->rdllist) ||\n+\t\tREAD_ONCE(ep->ovflist) != EP_UNACTIVE_PTR;\n }\n \n #ifdef CONFIG_NET_RX_BUSY_POLL\n@@ -728,6 +724,77 @@ static inline void ep_pm_stay_awake_rcu(struct epitem *epi)\n \trcu_read_unlock();\n }\n \n+\n+/*\n+ * ep->mutex needs to be held because we could be hit by\n+ * eventpoll_release_file() and epoll_ctl().\n+ */\n+static void ep_start_scan(struct eventpoll *ep, struct list_head *txlist)\n+{\n+\t/*\n+\t * Steal the ready list, and re-init the original one to the\n+\t * empty list. Also, set ep->ovflist to NULL so that events\n+\t * happening while looping w/out locks, are not lost. We cannot\n+\t * have the poll callback to queue directly on ep->rdllist,\n+\t * because we want the \"sproc\" callback to be able to do it\n+\t * in a lockless way.\n+\t */\n+\tlockdep_assert_irqs_enabled();\n+\twrite_lock_irq(&ep->lock);\n+\tlist_splice_init(&ep->rdllist, txlist);\n+\tWRITE_ONCE(ep->ovflist, NULL);\n+\twrite_unlock_irq(&ep->lock);\n+}\n+\n+static void ep_done_scan(struct eventpoll *ep,\n+\t\t\t struct list_head *txlist)\n+{\n+\tstruct epitem *epi, *nepi;\n+\n+\twrite_lock_irq(&ep->lock);\n+\t/*\n+\t * During the time we spent inside the \"sproc\" callback, some\n+\t * other events might have been queued by the poll callback.\n+\t * We re-insert them inside the main ready-list here.\n+\t */\n+\tfor (nepi = READ_ONCE(ep->ovflist); (epi = nepi) != NULL;\n+\t     nepi = epi->next, epi->next = EP_UNACTIVE_PTR) {\n+\t\t/*\n+\t\t * We need to check if the item is already in the list.\n+\t\t * During the \"sproc\" callback execution time, items are\n+\t\t * queued into ->ovflist but the \"txlist\" might already\n+\t\t * contain them, and the list_splice() below takes care of them.\n+\t\t */\n+\t\tif (!ep_is_linked(epi)) {\n+\t\t\t/*\n+\t\t\t * ->ovflist is LIFO, so we have to reverse it in order\n+\t\t\t * to keep in FIFO.\n+\t\t\t */\n+\t\t\tlist_add(&epi->rdllink, &ep->rdllist);\n+\t\t\tep_pm_stay_awake(epi);\n+\t\t}\n+\t}\n+\t/*\n+\t * We need to set back ep->ovflist to EP_UNACTIVE_PTR, so that after\n+\t * releasing the lock, events will be queued in the normal way inside\n+\t * ep->rdllist.\n+\t */\n+\tWRITE_ONCE(ep->ovflist, EP_UNACTIVE_PTR);\n+\n+\t/*\n+\t * Quickly re-inject items left on \"txlist\".\n+\t */\n+\tlist_splice(txlist, &ep->rdllist);\n+\t__pm_relax(ep->ws);\n+\n+\tif (!list_empty(&ep->rdllist)) {\n+\t\tif (waitqueue_active(&ep->wq))\n+\t\t\twake_up(&ep->wq);\n+\t}\n+\n+\twrite_unlock_irq(&ep->lock);\n+}\n+\n static void ep_get(struct eventpoll *ep)\n {\n \trefcount_inc(&ep->refcount);\n@@ -765,12 +832,10 @@ static void ep_free(struct eventpoll *ep)\n static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n {\n \tstruct file *file = epi->ffd.file;\n-\tstruct llist_node *put_back_last;\n \tstruct epitems_head *to_free;\n \tstruct hlist_head *head;\n-\tLLIST_HEAD(put_back);\n \n-\tlockdep_assert_held(&ep->mtx);\n+\tlockdep_assert_irqs_enabled();\n \n \t/*\n \t * Removes poll wait queue hooks.\n@@ -802,20 +867,10 @@ static bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n \n \trb_erase_cached(&epi->rbn, &ep->rbr);\n \n-\tif (llist_on_list(&epi->rdllink)) {\n-\t\tput_back_last = NULL;\n-\t\twhile (true) {\n-\t\t\tstruct llist_node *n = llist_del_first(&ep->rdllist);\n-\n-\t\t\tif (&epi->rdllink == n || WARN_ON(!n))\n-\t\t\t\tbreak;\n-\t\t\tif (!put_back_last)\n-\t\t\t\tput_back_last = n;\n-\t\t\t__llist_add(n, &put_back);\n-\t\t}\n-\t\tif (put_back_last)\n-\t\t\tllist_add_batch(put_back.first, put_back_last, &ep->rdllist);\n-\t}\n+\twrite_lock_irq(&ep->lock);\n+\tif (ep_is_linked(epi))\n+\t\tlist_del_init(&epi->rdllink);\n+\twrite_unlock_irq(&ep->lock);\n \n \twakeup_source_unregister(ep_wakeup_source(epi));\n \t/*\n@@ -917,9 +972,8 @@ static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt, int depth\n static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int depth)\n {\n \tstruct eventpoll *ep = file->private_data;\n-\tstruct wakeup_source *ws;\n-\tstruct llist_node *n;\n-\tstruct epitem *epi;\n+\tLIST_HEAD(txlist);\n+\tstruct epitem *epi, *tmp;\n \tpoll_table pt;\n \t__poll_t res = 0;\n \n@@ -933,39 +987,22 @@ static __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int dep\n \t * the ready list.\n \t */\n \tmutex_lock_nested(&ep->mtx, depth);\n-\twhile (true) {\n-\t\tn = llist_del_first_init(&ep->rdllist);\n-\t\tif (!n)\n-\t\t\tbreak;\n-\n-\t\tepi = llist_entry(n, struct epitem, rdllink);\n-\n+\tep_start_scan(ep, &txlist);\n+\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n \t\tif (ep_item_poll(epi, &pt, depth + 1)) {\n \t\t\tres = EPOLLIN | EPOLLRDNORM;\n-\t\t\tepitem_ready(epi);\n \t\t\tbreak;\n \t\t} else {\n \t\t\t/*\n-\t\t\t * We need to activate ep before deactivating epi, to prevent autosuspend\n-\t\t\t * just in case epi becomes active after ep_item_poll() above.\n-\t\t\t *\n-\t\t\t * This is similar to ep_send_events().\n+\t\t\t * Item has been dropped into the ready list by the poll\n+\t\t\t * callback, but it's not actually ready, as far as\n+\t\t\t * caller requested events goes. We can remove it here.\n \t\t\t */\n-\t\t\tws = ep_wakeup_source(epi);\n-\t\t\tif (ws) {\n-\t\t\t\tif (ws->active)\n-\t\t\t\t\t__pm_stay_awake(ep->ws);\n-\t\t\t\t__pm_relax(ws);\n-\t\t\t}\n \t\t\t__pm_relax(ep_wakeup_source(epi));\n-\n-\t\t\t/* Just in case epi becomes active right before __pm_relax() */\n-\t\t\tif (unlikely(ep_item_poll(epi, &pt, depth + 1)))\n-\t\t\t\tep_pm_stay_awake(epi);\n-\n-\t\t\t__pm_relax(ep->ws);\n+\t\t\tlist_del_init(&epi->rdllink);\n \t\t}\n \t}\n+\tep_done_scan(ep, &txlist);\n \tmutex_unlock(&ep->mtx);\n \treturn res;\n }\n@@ -1114,10 +1151,12 @@ static int ep_alloc(struct eventpoll **pep)\n \t\treturn -ENOMEM;\n \n \tmutex_init(&ep->mtx);\n+\trwlock_init(&ep->lock);\n \tinit_waitqueue_head(&ep->wq);\n \tinit_waitqueue_head(&ep->poll_wait);\n-\tinit_llist_head(&ep->rdllist);\n+\tINIT_LIST_HEAD(&ep->rdllist);\n \tep->rbr = RB_ROOT_CACHED;\n+\tep->ovflist = EP_UNACTIVE_PTR;\n \tep->user = get_current_user();\n \trefcount_set(&ep->refcount, 1);\n \n@@ -1199,11 +1238,94 @@ struct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,\n }\n #endif /* CONFIG_KCMP */\n \n+/*\n+ * Adds a new entry to the tail of the list in a lockless way, i.e.\n+ * multiple CPUs are allowed to call this function concurrently.\n+ *\n+ * Beware: it is necessary to prevent any other modifications of the\n+ *         existing list until all changes are completed, in other words\n+ *         concurrent list_add_tail_lockless() calls should be protected\n+ *         with a read lock, where write lock acts as a barrier which\n+ *         makes sure all list_add_tail_lockless() calls are fully\n+ *         completed.\n+ *\n+ *        Also an element can be locklessly added to the list only in one\n+ *        direction i.e. either to the tail or to the head, otherwise\n+ *        concurrent access will corrupt the list.\n+ *\n+ * Return: %false if element has been already added to the list, %true\n+ * otherwise.\n+ */\n+static inline bool list_add_tail_lockless(struct list_head *new,\n+\t\t\t\t\t  struct list_head *head)\n+{\n+\tstruct list_head *prev;\n+\n+\t/*\n+\t * This is simple 'new->next = head' operation, but cmpxchg()\n+\t * is used in order to detect that same element has been just\n+\t * added to the list from another CPU: the winner observes\n+\t * new->next == new.\n+\t */\n+\tif (!try_cmpxchg(&new->next, &new, head))\n+\t\treturn false;\n+\n+\t/*\n+\t * Initially ->next of a new element must be updated with the head\n+\t * (we are inserting to the tail) and only then pointers are atomically\n+\t * exchanged.  XCHG guarantees memory ordering, thus ->next should be\n+\t * updated before pointers are actually swapped and pointers are\n+\t * swapped before prev->next is updated.\n+\t */\n+\n+\tprev = xchg(&head->prev, new);\n+\n+\t/*\n+\t * It is safe to modify prev->next and new->prev, because a new element\n+\t * is added only to the tail and new->next is updated before XCHG.\n+\t */\n+\n+\tprev->next = new;\n+\tnew->prev = prev;\n+\n+\treturn true;\n+}\n+\n+/*\n+ * Chains a new epi entry to the tail of the ep->ovflist in a lockless way,\n+ * i.e. multiple CPUs are allowed to call this function concurrently.\n+ *\n+ * Return: %false if epi element has been already chained, %true otherwise.\n+ */\n+static inline bool chain_epi_lockless(struct epitem *epi)\n+{\n+\tstruct eventpoll *ep = epi->ep;\n+\n+\t/* Fast preliminary check */\n+\tif (epi->next != EP_UNACTIVE_PTR)\n+\t\treturn false;\n+\n+\t/* Check that the same epi has not been just chained from another CPU */\n+\tif (cmpxchg(&epi->next, EP_UNACTIVE_PTR, NULL) != EP_UNACTIVE_PTR)\n+\t\treturn false;\n+\n+\t/* Atomically exchange tail */\n+\tepi->next = xchg(&ep->ovflist, epi);\n+\n+\treturn true;\n+}\n+\n /*\n  * This is the callback that is passed to the wait queue wakeup\n  * mechanism. It is called by the stored file descriptors when they\n  * have events to report.\n  *\n+ * This callback takes a read lock in order not to contend with concurrent\n+ * events from another file descriptor, thus all modifications to ->rdllist\n+ * or ->ovflist are lockless.  Read lock is paired with the write lock from\n+ * ep_start/done_scan(), which stops all list modifications and guarantees\n+ * that lists state is seen correctly.\n+ *\n  * Another thing worth to mention is that ep_poll_callback() can be called\n  * concurrently for the same @epi from different CPUs if poll table was inited\n  * with several wait queues entries.  Plural wakeup from different CPUs of a\n@@ -1213,11 +1335,15 @@ struct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,\n  */\n static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n {\n+\tint pwake = 0;\n \tstruct epitem *epi = ep_item_from_wait(wait);\n \tstruct eventpoll *ep = epi->ep;\n \t__poll_t pollflags = key_to_poll(key);\n+\tunsigned long flags;\n \tint ewake = 0;\n \n+\tread_lock_irqsave(&ep->lock, flags);\n+\n \tep_set_busy_poll_napi_id(epi);\n \n \t/*\n@@ -1227,7 +1353,7 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * until the next EPOLL_CTL_MOD will be issued.\n \t */\n \tif (!(epi->event.events & ~EP_PRIVATE_BITS))\n-\t\tgoto out;\n+\t\tgoto out_unlock;\n \n \t/*\n \t * Check the events coming with the callback. At this stage, not\n@@ -1236,10 +1362,22 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t * test for \"key\" != NULL before the event match test.\n \t */\n \tif (pollflags && !(pollflags & epi->event.events))\n-\t\tgoto out;\n+\t\tgoto out_unlock;\n \n-\tep_pm_stay_awake_rcu(epi);\n-\tepitem_ready(epi);\n+\t/*\n+\t * If we are transferring events to userspace, we can hold no locks\n+\t * (because we're accessing user memory, and because of linux f_op->poll()\n+\t * semantics). All the events that happen during that period of time are\n+\t * chained in ep->ovflist and requeued later on.\n+\t */\n+\tif (READ_ONCE(ep->ovflist) != EP_UNACTIVE_PTR) {\n+\t\tif (chain_epi_lockless(epi))\n+\t\t\tep_pm_stay_awake_rcu(epi);\n+\t} else if (!ep_is_linked(epi)) {\n+\t\t/* In the usual case, add event to ready list. */\n+\t\tif (list_add_tail_lockless(&epi->rdllink, &ep->rdllist))\n+\t\t\tep_pm_stay_awake_rcu(epi);\n+\t}\n \n \t/*\n \t * Wake up ( if active ) both the eventpoll wait list and the ->poll()\n@@ -1268,9 +1406,15 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v\n \t\t\twake_up(&ep->wq);\n \t}\n \tif (waitqueue_active(&ep->poll_wait))\n+\t\tpwake++;\n+\n+out_unlock:\n+\tread_unlock_irqrestore(&ep->lock, flags);\n+\n+\t/* We have to call this outside the lock */\n+\tif (pwake)\n \t\tep_poll_safewake(ep, epi, pollflags & EPOLL_URING_WAKE);\n \n-out:\n \tif (!(epi->event.events & EPOLLEXCLUSIVE))\n \t\tewake = 1;\n \n@@ -1515,6 +1659,8 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \tif (is_file_epoll(tfile))\n \t\ttep = tfile->private_data;\n \n+\tlockdep_assert_irqs_enabled();\n+\n \tif (unlikely(percpu_counter_compare(&ep->user->epoll_watches,\n \t\t\t\t\t    max_user_watches) >= 0))\n \t\treturn -ENOSPC;\n@@ -1526,10 +1672,11 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t}\n \n \t/* Item initialization follow here ... */\n-\tinit_llist_node(&epi->rdllink);\n+\tINIT_LIST_HEAD(&epi->rdllink);\n \tepi->ep = ep;\n \tep_set_ffd(&epi->ffd, tfile, fd);\n \tepi->event = *event;\n+\tepi->next = EP_UNACTIVE_PTR;\n \n \tif (tep)\n \t\tmutex_lock_nested(&tep->mtx, 1);\n@@ -1596,13 +1743,16 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\treturn -ENOMEM;\n \t}\n \n+\t/* We have to drop the new item inside our item list to keep track of it */\n+\twrite_lock_irq(&ep->lock);\n+\n \t/* record NAPI ID of new item if present */\n \tep_set_busy_poll_napi_id(epi);\n \n \t/* If the file is already \"ready\" we drop it inside the ready list */\n-\tif (revents) {\n+\tif (revents && !ep_is_linked(epi)) {\n+\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n \t\tep_pm_stay_awake(epi);\n-\t\tepitem_ready(epi);\n \n \t\t/* Notify waiting tasks that events are available */\n \t\tif (waitqueue_active(&ep->wq))\n@@ -1611,6 +1761,8 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n \t\t\tpwake++;\n \t}\n \n+\twrite_unlock_irq(&ep->lock);\n+\n \t/* We have to call this outside the lock */\n \tif (pwake)\n \t\tep_poll_safewake(ep, NULL, 0);\n@@ -1625,8 +1777,11 @@ static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t\t     const struct epoll_event *event)\n {\n+\tint pwake = 0;\n \tpoll_table pt;\n \n+\tlockdep_assert_irqs_enabled();\n+\n \tinit_poll_funcptr(&pt, NULL);\n \n \t/*\n@@ -1670,16 +1825,24 @@ static int ep_modify(struct eventpoll *ep, struct epitem *epi,\n \t * list, push it inside.\n \t */\n \tif (ep_item_poll(epi, &pt, 1)) {\n-\t\tep_pm_stay_awake(epi);\n-\t\tepitem_ready(epi);\n+\t\twrite_lock_irq(&ep->lock);\n+\t\tif (!ep_is_linked(epi)) {\n+\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n+\t\t\tep_pm_stay_awake(epi);\n \n-\t\t/* Notify waiting tasks that events are available */\n-\t\tif (waitqueue_active(&ep->wq))\n-\t\t\twake_up(&ep->wq);\n-\t\tif (waitqueue_active(&ep->poll_wait))\n-\t\t\tep_poll_safewake(ep, NULL, 0);\n+\t\t\t/* Notify waiting tasks that events are available */\n+\t\t\tif (waitqueue_active(&ep->wq))\n+\t\t\t\twake_up(&ep->wq);\n+\t\t\tif (waitqueue_active(&ep->poll_wait))\n+\t\t\t\tpwake++;\n+\t\t}\n+\t\twrite_unlock_irq(&ep->lock);\n \t}\n \n+\t/* We have to call this outside the lock */\n+\tif (pwake)\n+\t\tep_poll_safewake(ep, NULL, 0);\n+\n \treturn 0;\n }\n \n@@ -1687,7 +1850,7 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t  struct epoll_event __user *events, int maxevents)\n {\n \tstruct epitem *epi, *tmp;\n-\tLLIST_HEAD(txlist);\n+\tLIST_HEAD(txlist);\n \tpoll_table pt;\n \tint res = 0;\n \n@@ -1702,18 +1865,19 @@ static int ep_send_events(struct eventpoll *ep,\n \tinit_poll_funcptr(&pt, NULL);\n \n \tmutex_lock(&ep->mtx);\n+\tep_start_scan(ep, &txlist);\n \n-\twhile (res < maxevents) {\n+\t/*\n+\t * We can loop without lock because we are passed a task private list.\n+\t * Items cannot vanish during the loop we are holding ep->mtx.\n+\t */\n+\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n \t\tstruct wakeup_source *ws;\n-\t\tstruct llist_node *n;\n \t\t__poll_t revents;\n \n-\t\tn = llist_del_first(&ep->rdllist);\n-\t\tif (!n)\n+\t\tif (res >= maxevents)\n \t\t\tbreak;\n \n-\t\tepi = llist_entry(n, struct epitem, rdllink);\n-\n \t\t/*\n \t\t * Activate ep->ws before deactivating epi->ws to prevent\n \t\t * triggering auto-suspend here (in case we reactive epi->ws\n@@ -1730,30 +1894,21 @@ static int ep_send_events(struct eventpoll *ep,\n \t\t\t__pm_relax(ws);\n \t\t}\n \n+\t\tlist_del_init(&epi->rdllink);\n+\n \t\t/*\n \t\t * If the event mask intersect the caller-requested one,\n \t\t * deliver the event to userspace. Again, we are holding ep->mtx,\n \t\t * so no operations coming from userspace can change the item.\n \t\t */\n \t\trevents = ep_item_poll(epi, &pt, 1);\n-\t\tif (!revents) {\n-\t\t\tinit_llist_node(n);\n-\n-\t\t\t/*\n-\t\t\t * Just in case epi becomes ready after ep_item_poll() above, but before\n-\t\t\t * init_llist_node(). Make sure to add it to the ready list, otherwise an\n-\t\t\t * event may be lost.\n-\t\t\t */\n-\t\t\tif (unlikely(ep_item_poll(epi, &pt, 1))) {\n-\t\t\t\tep_pm_stay_awake(epi);\n-\t\t\t\tepitem_ready(epi);\n-\t\t\t}\n+\t\tif (!revents)\n \t\t\tcontinue;\n-\t\t}\n \n \t\tevents = epoll_put_uevent(revents, epi->event.data, events);\n \t\tif (!events) {\n-\t\t\tllist_add(&epi->rdllink, &ep->rdllist);\n+\t\t\tlist_add(&epi->rdllink, &txlist);\n+\t\t\tep_pm_stay_awake(epi);\n \t\t\tif (!res)\n \t\t\t\tres = -EFAULT;\n \t\t\tbreak;\n@@ -1761,31 +1916,25 @@ static int ep_send_events(struct eventpoll *ep,\n \t\tres++;\n \t\tif (epi->event.events & EPOLLONESHOT)\n \t\t\tepi->event.events &= EP_PRIVATE_BITS;\n-\t\t__llist_add(n, &txlist);\n-\t}\n-\n-\tllist_for_each_entry_safe(epi, tmp, txlist.first, rdllink) {\n-\t\tinit_llist_node(&epi->rdllink);\n-\n-\t\tif (!(epi->event.events & EPOLLET)) {\n+\t\telse if (!(epi->event.events & EPOLLET)) {\n \t\t\t/*\n-\t\t\t * If this file has been added with Level Trigger mode, we need to insert\n-\t\t\t * back inside the ready list, so that the next call to epoll_wait() will\n-\t\t\t * check again the events availability.\n+\t\t\t * If this file has been added with Level\n+\t\t\t * Trigger mode, we need to insert back inside\n+\t\t\t * the ready list, so that the next call to\n+\t\t\t * epoll_wait() will check again the events\n+\t\t\t * availability. At this point, no one can insert\n+\t\t\t * into ep->rdllist besides us. The epoll_ctl()\n+\t\t\t * callers are locked out by\n+\t\t\t * ep_send_events() holding \"mtx\" and the\n+\t\t\t * poll callback will queue them in ep->ovflist.\n \t\t\t */\n+\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n \t\t\tep_pm_stay_awake(epi);\n-\t\t\tepitem_ready(epi);\n \t\t}\n \t}\n-\n-\t__pm_relax(ep->ws);\n+\tep_done_scan(ep, &txlist);\n \tmutex_unlock(&ep->mtx);\n \n-\tif (!llist_empty(&ep->rdllist)) {\n-\t\tif (waitqueue_active(&ep->wq))\n-\t\t\twake_up(&ep->wq);\n-\t}\n-\n \treturn res;\n }\n \n@@ -1878,6 +2027,8 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \twait_queue_entry_t wait;\n \tktime_t expires, *to = NULL;\n \n+\tlockdep_assert_irqs_enabled();\n+\n \tif (timeout && (timeout->tv_sec | timeout->tv_nsec)) {\n \t\tslack = select_estimate_accuracy(timeout);\n \t\tto = &expires;\n@@ -1937,15 +2088,54 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n \t\tinit_wait(&wait);\n \t\twait.func = ep_autoremove_wake_function;\n \n-\t\tprepare_to_wait_exclusive(&ep->wq, &wait, TASK_INTERRUPTIBLE);\n+\t\twrite_lock_irq(&ep->lock);\n+\t\t/*\n+\t\t * Barrierless variant, waitqueue_active() is called under\n+\t\t * the same lock on wakeup ep_poll_callback() side, so it\n+\t\t * is safe to avoid an explicit barrier.\n+\t\t */\n+\t\t__set_current_state(TASK_INTERRUPTIBLE);\n \n-\t\tif (!ep_events_available(ep))\n+\t\t/*\n+\t\t * Do the final check under the lock. ep_start/done_scan()\n+\t\t * plays with two lists (->rdllist and ->ovflist) and there\n+\t\t * is always a race when both lists are empty for short\n+\t\t * period of time although events are pending, so lock is\n+\t\t * important.\n+\t\t */\n+\t\teavail = ep_events_available(ep);\n+\t\tif (!eavail)\n+\t\t\t__add_wait_queue_exclusive(&ep->wq, &wait);\n+\n+\t\twrite_unlock_irq(&ep->lock);\n+\n+\t\tif (!eavail)\n \t\t\ttimed_out = !ep_schedule_timeout(to) ||\n \t\t\t\t!schedule_hrtimeout_range(to, slack,\n \t\t\t\t\t\t\t  HRTIMER_MODE_ABS);\n+\t\t__set_current_state(TASK_RUNNING);\n \n-\t\tfinish_wait(&ep->wq, &wait);\n-\t\teavail = ep_events_available(ep);\n+\t\t/*\n+\t\t * We were woken up, thus go and try to harvest some events.\n+\t\t * If timed out and still on the wait queue, recheck eavail\n+\t\t * carefully under lock, below.\n+\t\t */\n+\t\teavail = 1;\n+\n+\t\tif (!list_empty_careful(&wait.entry)) {\n+\t\t\twrite_lock_irq(&ep->lock);\n+\t\t\t/*\n+\t\t\t * If the thread timed out and is not on the wait queue,\n+\t\t\t * it means that the thread was woken up after its\n+\t\t\t * timeout expired before it could reacquire the lock.\n+\t\t\t * Thus, when wait.entry is empty, it needs to harvest\n+\t\t\t * events.\n+\t\t\t */\n+\t\t\tif (timed_out)\n+\t\t\t\teavail = list_empty(&wait.entry);\n+\t\t\t__remove_wait_queue(&ep->wq, &wait);\n+\t\t\twrite_unlock_irq(&ep->lock);\n+\t\t}\n \t}\n }\n ",
    "stats": {
      "insertions": 324,
      "deletions": 134,
      "files": 1
    }
  }
]
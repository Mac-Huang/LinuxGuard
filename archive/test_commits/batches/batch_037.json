[
  {
    "sha": "3f5f6321f129ad5a30aa03c99c196b4612be68a8",
    "message": "IB/core: Annotate umem_mutex acquisition under fs_reclaim for lockdep\n\nFollowing the fix in the previous commit (\"IB/mlx5: Fix potential\ndeadlock in MR deregistration\"), teach lockdep explicitly about the\nlocking order between fs_reclaim and umem_mutex.\n\nThe previous commit resolved a potential deadlock scenario where\nkzalloc(GFP_KERNEL) was called while holding umem_mutex, which could\nlead to reclaim and eventually invoke the MMU notifier\n(mlx5_ib_invalidate_range()), causing a recursive acquisition of\numem_mutex.\n\nTo prevent such issues from reoccurring unnoticed in future code\nchanges, add a lockdep annotation in ib_init_umem_odp() that simulates\ntaking umem_mutex inside a reclaim context. This makes lockdep aware\nof this locking dependency and ensures that future violations—such as\ncalling kzalloc() or any memory allocator that may enter reclaim while\nholding umem_mutex—will immediately raise a lockdep warning.\n\nSigned-off-by: Or Har-Toov <ohartoov@nvidia.com>\nReviewed-by: Michael Guralnik <michaelgur@nvidia.com>\nLink: https://patch.msgid.link/9d31b9d8fe1db648a9f47cec3df6b8463319dee5.1750061698.git.leon@kernel.org\nSigned-off-by: Leon Romanovsky <leon@kernel.org>",
    "author": "Or Har-Toov",
    "date": "2025-06-25T03:40:16-04:00",
    "files_changed": [
      "drivers/infiniband/core/umem_odp.c"
    ],
    "diff": "diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c\nindex c752ae9fad6c..b1c44ec1a3f3 100644\n--- a/drivers/infiniband/core/umem_odp.c\n+++ b/drivers/infiniband/core/umem_odp.c\n@@ -76,6 +76,17 @@ static int ib_init_umem_odp(struct ib_umem_odp *umem_odp,\n \tend = ALIGN(end, page_size);\n \tif (unlikely(end < page_size))\n \t\treturn -EOVERFLOW;\n+\t/*\n+\t * The mmu notifier can be called within reclaim contexts and takes the\n+\t * umem_mutex. This is rare to trigger in testing, teach lockdep about\n+\t * it.\n+\t */\n+\tif (IS_ENABLED(CONFIG_LOCKDEP)) {\n+\t\tfs_reclaim_acquire(GFP_KERNEL);\n+\t\tmutex_lock(&umem_odp->umem_mutex);\n+\t\tmutex_unlock(&umem_odp->umem_mutex);\n+\t\tfs_reclaim_release(GFP_KERNEL);\n+\t}\n \n \tnr_entries = (end - start) >> PAGE_SHIFT;\n \tif (!(nr_entries * PAGE_SIZE / page_size))",
    "stats": {
      "insertions": 11,
      "deletions": 0,
      "files": 1
    }
  },
  {
    "sha": "2ed25aa7f7711f508b6120e336f05cd9d49943c0",
    "message": "IB/mlx5: Fix potential deadlock in MR deregistration\n\nThe issue arises when kzalloc() is invoked while holding umem_mutex or\nany other lock acquired under umem_mutex. This is problematic because\nkzalloc() can trigger fs_reclaim_aqcuire(), which may, in turn, invoke\nmmu_notifier_invalidate_range_start(). This function can lead to\nmlx5_ib_invalidate_range(), which attempts to acquire umem_mutex again,\nresulting in a deadlock.\n\nThe problematic flow:\n             CPU0                      |              CPU1\n---------------------------------------|------------------------------------------------\nmlx5_ib_dereg_mr()                     |\n → revoke_mr()                         |\n   → mutex_lock(&umem_odp->umem_mutex) |\n                                       | mlx5_mkey_cache_init()\n                                       |  → mutex_lock(&dev->cache.rb_lock)\n                                       |  → mlx5r_cache_create_ent_locked()\n                                       |    → kzalloc(GFP_KERNEL)\n                                       |      → fs_reclaim()\n                                       |        → mmu_notifier_invalidate_range_start()\n                                       |          → mlx5_ib_invalidate_range()\n                                       |            → mutex_lock(&umem_odp->umem_mutex)\n   → cache_ent_find_and_store()        |\n     → mutex_lock(&dev->cache.rb_lock) |\n\nAdditionally, when kzalloc() is called from within\ncache_ent_find_and_store(), we encounter the same deadlock due to\nre-acquisition of umem_mutex.\n\nSolve by releasing umem_mutex in dereg_mr() after umr_revoke_mr()\nand before acquiring rb_lock. This ensures that we don't hold\numem_mutex while performing memory allocations that could trigger\nthe reclaim path.\n\nThis change prevents the deadlock by ensuring proper lock ordering and\navoiding holding locks during memory allocation operations that could\ntrigger the reclaim path.\n\nThe following lockdep warning demonstrates the deadlock:\n\n python3/20557 is trying to acquire lock:\n ffff888387542128 (&umem_odp->umem_mutex){+.+.}-{4:4}, at:\n mlx5_ib_invalidate_range+0x5b/0x550 [mlx5_ib]\n\n but task is already holding lock:\n ffffffff82f6b840 (mmu_notifier_invalidate_range_start){+.+.}-{0:0}, at:\n unmap_vmas+0x7b/0x1a0\n\n which lock already depends on the new lock.\n\n the existing dependency chain (in reverse order) is:\n\n -> #3 (mmu_notifier_invalidate_range_start){+.+.}-{0:0}:\n       fs_reclaim_acquire+0x60/0xd0\n       mem_cgroup_css_alloc+0x6f/0x9b0\n       cgroup_init_subsys+0xa4/0x240\n       cgroup_init+0x1c8/0x510\n       start_kernel+0x747/0x760\n       x86_64_start_reservations+0x25/0x30\n       x86_64_start_kernel+0x73/0x80\n       common_startup_64+0x129/0x138\n\n -> #2 (fs_reclaim){+.+.}-{0:0}:\n       fs_reclaim_acquire+0x91/0xd0\n       __kmalloc_cache_noprof+0x4d/0x4c0\n       mlx5r_cache_create_ent_locked+0x75/0x620 [mlx5_ib]\n       mlx5_mkey_cache_init+0x186/0x360 [mlx5_ib]\n       mlx5_ib_stage_post_ib_reg_umr_init+0x3c/0x60 [mlx5_ib]\n       __mlx5_ib_add+0x4b/0x190 [mlx5_ib]\n       mlx5r_probe+0xd9/0x320 [mlx5_ib]\n       auxiliary_bus_probe+0x42/0x70\n       really_probe+0xdb/0x360\n       __driver_probe_device+0x8f/0x130\n       driver_probe_device+0x1f/0xb0\n       __driver_attach+0xd4/0x1f0\n       bus_for_each_dev+0x79/0xd0\n       bus_add_driver+0xf0/0x200\n       driver_register+0x6e/0xc0\n       __auxiliary_driver_register+0x6a/0xc0\n       do_one_initcall+0x5e/0x390\n       do_init_module+0x88/0x240\n       init_module_from_file+0x85/0xc0\n       idempotent_init_module+0x104/0x300\n       __x64_sys_finit_module+0x68/0xc0\n       do_syscall_64+0x6d/0x140\n       entry_SYSCALL_64_after_hwframe+0x4b/0x53\n\n -> #1 (&dev->cache.rb_lock){+.+.}-{4:4}:\n       __mutex_lock+0x98/0xf10\n       __mlx5_ib_dereg_mr+0x6f2/0x890 [mlx5_ib]\n       mlx5_ib_dereg_mr+0x21/0x110 [mlx5_ib]\n       ib_dereg_mr_user+0x85/0x1f0 [ib_core]\n       uverbs_free_mr+0x19/0x30 [ib_uverbs]\n       destroy_hw_idr_uobject+0x21/0x80 [ib_uverbs]\n       uverbs_destroy_uobject+0x60/0x3d0 [ib_uverbs]\n       uobj_destroy+0x57/0xa0 [ib_uverbs]\n       ib_uverbs_cmd_verbs+0x4d5/0x1210 [ib_uverbs]\n       ib_uverbs_ioctl+0x129/0x230 [ib_uverbs]\n       __x64_sys_ioctl+0x596/0xaa0\n       do_syscall_64+0x6d/0x140\n       entry_SYSCALL_64_after_hwframe+0x4b/0x53\n\n -> #0 (&umem_odp->umem_mutex){+.+.}-{4:4}:\n       __lock_acquire+0x1826/0x2f00\n       lock_acquire+0xd3/0x2e0\n       __mutex_lock+0x98/0xf10\n       mlx5_ib_invalidate_range+0x5b/0x550 [mlx5_ib]\n       __mmu_notifier_invalidate_range_start+0x18e/0x1f0\n       unmap_vmas+0x182/0x1a0\n       exit_mmap+0xf3/0x4a0\n       mmput+0x3a/0x100\n       do_exit+0x2b9/0xa90\n       do_group_exit+0x32/0xa0\n       get_signal+0xc32/0xcb0\n       arch_do_signal_or_restart+0x29/0x1d0\n       syscall_exit_to_user_mode+0x105/0x1d0\n       do_syscall_64+0x79/0x140\n       entry_SYSCALL_64_after_hwframe+0x4b/0x53\n\n Chain exists of:\n &dev->cache.rb_lock --> mmu_notifier_invalidate_range_start -->\n &umem_odp->umem_mutex\n\n Possible unsafe locking scenario:\n\n       CPU0                        CPU1\n       ----                        ----\n   lock(&umem_odp->umem_mutex);\n                                lock(mmu_notifier_invalidate_range_start);\n                                lock(&umem_odp->umem_mutex);\n   lock(&dev->cache.rb_lock);\n\n *** DEADLOCK ***\n\nFixes: abb604a1a9c8 (\"RDMA/mlx5: Fix a race for an ODP MR which leads to CQE with error\")\nSigned-off-by: Or Har-Toov <ohartoov@nvidia.com>\nReviewed-by: Michael Guralnik <michaelgur@nvidia.com>\nLink: https://patch.msgid.link/3c8f225a8a9fade647d19b014df1172544643e4a.1750061612.git.leon@kernel.org\nSigned-off-by: Leon Romanovsky <leon@kernel.org>",
    "author": "Or Har-Toov",
    "date": "2025-06-25T03:39:36-04:00",
    "files_changed": [
      "drivers/infiniband/hw/mlx5/mr.c"
    ],
    "diff": "diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c\nindex 57f9bc2a4a3a..bd35e75d9ce5 100644\n--- a/drivers/infiniband/hw/mlx5/mr.c\n+++ b/drivers/infiniband/hw/mlx5/mr.c\n@@ -2027,23 +2027,50 @@ void mlx5_ib_revoke_data_direct_mrs(struct mlx5_ib_dev *dev)\n \t}\n }\n \n-static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n+static int mlx5_umr_revoke_mr_with_lock(struct mlx5_ib_mr *mr)\n {\n-\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n-\tstruct mlx5_cache_ent *ent = mr->mmkey.cache_ent;\n-\tbool is_odp = is_odp_mr(mr);\n \tbool is_odp_dma_buf = is_dmabuf_mr(mr) &&\n-\t\t\t!to_ib_umem_dmabuf(mr->umem)->pinned;\n-\tbool from_cache = !!ent;\n-\tint ret = 0;\n+\t\t\t      !to_ib_umem_dmabuf(mr->umem)->pinned;\n+\tbool is_odp = is_odp_mr(mr);\n+\tint ret;\n \n \tif (is_odp)\n \t\tmutex_lock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n \n \tif (is_odp_dma_buf)\n-\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv, NULL);\n+\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv,\n+\t\t\t      NULL);\n+\n+\tret = mlx5r_umr_revoke_mr(mr);\n+\n+\tif (is_odp) {\n+\t\tif (!ret)\n+\t\t\tto_ib_umem_odp(mr->umem)->private = NULL;\n+\t\tmutex_unlock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n+\t}\n+\n+\tif (is_odp_dma_buf) {\n+\t\tif (!ret)\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->private = NULL;\n+\t\tdma_resv_unlock(\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n+\t}\n \n-\tif (mr->mmkey.cacheable && !mlx5r_umr_revoke_mr(mr) && !cache_ent_find_and_store(dev, mr)) {\n+\treturn ret;\n+}\n+\n+static int mlx5r_handle_mkey_cleanup(struct mlx5_ib_mr *mr)\n+{\n+\tbool is_odp_dma_buf = is_dmabuf_mr(mr) &&\n+\t\t\t      !to_ib_umem_dmabuf(mr->umem)->pinned;\n+\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n+\tstruct mlx5_cache_ent *ent = mr->mmkey.cache_ent;\n+\tbool is_odp = is_odp_mr(mr);\n+\tbool from_cache = !!ent;\n+\tint ret;\n+\n+\tif (mr->mmkey.cacheable && !mlx5_umr_revoke_mr_with_lock(mr) &&\n+\t    !cache_ent_find_and_store(dev, mr)) {\n \t\tent = mr->mmkey.cache_ent;\n \t\t/* upon storing to a clean temp entry - schedule its cleanup */\n \t\tspin_lock_irq(&ent->mkeys_queue.lock);\n@@ -2055,7 +2082,7 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \t\t\tent->tmp_cleanup_scheduled = true;\n \t\t}\n \t\tspin_unlock_irq(&ent->mkeys_queue.lock);\n-\t\tgoto out;\n+\t\treturn 0;\n \t}\n \n \tif (ent) {\n@@ -2064,8 +2091,14 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \t\tmr->mmkey.cache_ent = NULL;\n \t\tspin_unlock_irq(&ent->mkeys_queue.lock);\n \t}\n+\n+\tif (is_odp)\n+\t\tmutex_lock(&to_ib_umem_odp(mr->umem)->umem_mutex);\n+\n+\tif (is_odp_dma_buf)\n+\t\tdma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv,\n+\t\t\t      NULL);\n \tret = destroy_mkey(dev, mr);\n-out:\n \tif (is_odp) {\n \t\tif (!ret)\n \t\t\tto_ib_umem_odp(mr->umem)->private = NULL;\n@@ -2075,9 +2108,9 @@ static int mlx5_revoke_mr(struct mlx5_ib_mr *mr)\n \tif (is_odp_dma_buf) {\n \t\tif (!ret)\n \t\t\tto_ib_umem_dmabuf(mr->umem)->private = NULL;\n-\t\tdma_resv_unlock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n+\t\tdma_resv_unlock(\n+\t\t\tto_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);\n \t}\n-\n \treturn ret;\n }\n \n@@ -2126,7 +2159,7 @@ static int __mlx5_ib_dereg_mr(struct ib_mr *ibmr)\n \t}\n \n \t/* Stop DMA */\n-\trc = mlx5_revoke_mr(mr);\n+\trc = mlx5r_handle_mkey_cleanup(mr);\n \tif (rc)\n \t\treturn rc;\n ",
    "stats": {
      "insertions": 47,
      "deletions": 14,
      "files": 1
    }
  },
  {
    "sha": "d02b2103a08b6d6908f1d3d8e8783d3f342555ac",
    "message": "drm/i915: fix build error some more\n\nAn earlier patch fixed a build failure with clang, but I still see the\nsame problem with some configurations using gcc:\n\ndrivers/gpu/drm/i915/i915_pmu.c: In function 'config_mask':\ninclude/linux/compiler_types.h:568:38: error: call to '__compiletime_assert_462' declared with attribute error: BUILD_BUG_ON failed: bit > BITS_PER_TYPE(typeof_member(struct i915_pmu, enable)) - 1\ndrivers/gpu/drm/i915/i915_pmu.c:116:3: note: in expansion of macro 'BUILD_BUG_ON'\n  116 |   BUILD_BUG_ON(bit >\n\nAs I understand it, the problem is that the function is not always fully\ninlined, but the __builtin_constant_p() can still evaluate the argument\nas being constant.\n\nMarking it as __always_inline so far works for me in all configurations.\n\nFixes: a7137b1825b5 (\"drm/i915/pmu: Fix build error with GCOV and AutoFDO enabled\")\nFixes: a644fde77ff7 (\"drm/i915/pmu: Change bitmask of enabled events to u32\")\nReviewed-by: Rodrigo Vivi <rodrigo.vivi@intel.com>\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nLink: https://lore.kernel.org/r/20250620111824.3395007-1-arnd@kernel.org\nSigned-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>\n(cherry picked from commit ef69f9dd1cd7301cdf04ba326ed28152a3affcf6)\nSigned-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>",
    "author": "Arnd Bergmann",
    "date": "2025-06-25T10:23:16+03:00",
    "files_changed": [
      "drivers/gpu/drm/i915/i915_pmu.c"
    ],
    "diff": "diff --git a/drivers/gpu/drm/i915/i915_pmu.c b/drivers/gpu/drm/i915/i915_pmu.c\nindex 990bfaba3ce4..5bc696bfbb0f 100644\n--- a/drivers/gpu/drm/i915/i915_pmu.c\n+++ b/drivers/gpu/drm/i915/i915_pmu.c\n@@ -108,7 +108,7 @@ static unsigned int config_bit(const u64 config)\n \t\treturn other_bit(config);\n }\n \n-static u32 config_mask(const u64 config)\n+static __always_inline u32 config_mask(const u64 config)\n {\n \tunsigned int bit = config_bit(config);\n ",
    "stats": {
      "insertions": 1,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "5e9571750c4e53d16727a04159455c693d7b31cb",
    "message": "ALSA: usb: qcom: fix NULL pointer dereference in qmi_stop_session\n\nThe find_substream() call may return NULL, but the error path\ndereferenced 'subs' unconditionally via dev_err(&subs->dev->dev, ...),\ncausing a NULL pointer dereference when subs is NULL.\n\nFix by switching to &uadev[idx].udev->dev which is always valid\nin this context.\n\nSigned-off-by: Pei Xiao <xiaopei01@kylinos.cn>\nLink: https://patch.msgid.link/86ac2939273ac853535049e60391c09d7688714e.1750755508.git.xiaopei01@kylinos.cn\nSigned-off-by: Takashi Iwai <tiwai@suse.de>",
    "author": "Pei Xiao",
    "date": "2025-06-25T08:28:16+02:00",
    "files_changed": [
      "sound/usb/qcom/qc_audio_offload.c"
    ],
    "diff": "diff --git a/sound/usb/qcom/qc_audio_offload.c b/sound/usb/qcom/qc_audio_offload.c\nindex 797afd4561bd..3543b5a53592 100644\n--- a/sound/usb/qcom/qc_audio_offload.c\n+++ b/sound/usb/qcom/qc_audio_offload.c\n@@ -759,7 +759,7 @@ static void qmi_stop_session(void)\n \t\t\tsubs = find_substream(pcm_card_num, info->pcm_dev_num,\n \t\t\t\t\t      info->direction);\n \t\t\tif (!subs || !chip || atomic_read(&chip->shutdown)) {\n-\t\t\t\tdev_err(&subs->dev->dev,\n+\t\t\t\tdev_err(&uadev[idx].udev->dev,\n \t\t\t\t\t\"no sub for c#%u dev#%u dir%u\\n\",\n \t\t\t\t\tinfo->pcm_card_num,\n \t\t\t\t\tinfo->pcm_dev_num,",
    "stats": {
      "insertions": 1,
      "deletions": 1,
      "files": 1
    }
  },
  {
    "sha": "524346e9d79f63a6e5aaa645140da3d1ec7a8a0f",
    "message": "ublk: build batch from IOs in same io_ring_ctx and io task\n\nublk_queue_cmd_list() dispatches the whole batch list by scheduling task\nwork via the tail request's io_uring_cmd, this way is fine even though\nmore than one io_ring_ctx are involved for this batch since it is just\none running context.\n\nHowever, the task work handler ublk_cmd_list_tw_cb() takes `issue_flags`\nof tail uring_cmd's io_ring_ctx for completing all commands. This way is\nwrong if any uring_cmd is issued from different io_ring_ctx.\n\nFixes it by always building batch IOs from same io_ring_ctx and io task\nbecause ublk_dispatch_req() does validate task context, and IO needs to\nbe aborted in case of running from fallback task work context.\n\nFor typical per-queue or per-io daemon implementation, this way shouldn't\nmake difference from performance viewpoint, because single io_ring_ctx is\ntaken in each daemon for normal use case.\n\nFixes: d796cea7b9f3 (\"ublk: implement ->queue_rqs()\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20250625022554.883571-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
    "author": "Ming Lei",
    "date": "2025-06-24T20:44:52-06:00",
    "files_changed": [
      "drivers/block/ublk_drv.c"
    ],
    "diff": "diff --git a/drivers/block/ublk_drv.c b/drivers/block/ublk_drv.c\nindex d36f44f5ee80..90c02ced392a 100644\n--- a/drivers/block/ublk_drv.c\n+++ b/drivers/block/ublk_drv.c\n@@ -1416,6 +1416,14 @@ static blk_status_t ublk_queue_rq(struct blk_mq_hw_ctx *hctx,\n \treturn BLK_STS_OK;\n }\n \n+static inline bool ublk_belong_to_same_batch(const struct ublk_io *io,\n+\t\t\t\t\t     const struct ublk_io *io2)\n+{\n+\treturn (io_uring_cmd_ctx_handle(io->cmd) ==\n+\t\tio_uring_cmd_ctx_handle(io2->cmd)) &&\n+\t\t(io->task == io2->task);\n+}\n+\n static void ublk_queue_rqs(struct rq_list *rqlist)\n {\n \tstruct rq_list requeue_list = { };\n@@ -1427,7 +1435,8 @@ static void ublk_queue_rqs(struct rq_list *rqlist)\n \t\tstruct ublk_queue *this_q = req->mq_hctx->driver_data;\n \t\tstruct ublk_io *this_io = &this_q->ios[req->tag];\n \n-\t\tif (io && io->task != this_io->task && !rq_list_empty(&submit_list))\n+\t\tif (io && !ublk_belong_to_same_batch(io, this_io) &&\n+\t\t\t\t!rq_list_empty(&submit_list))\n \t\t\tublk_queue_cmd_list(io, &submit_list);\n \t\tio = this_io;\n ",
    "stats": {
      "insertions": 10,
      "deletions": 1,
      "files": 1
    }
  }
]